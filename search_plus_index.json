{"./":{"url":"./","title":"前言","keywords":"","body":" 1、关于《k8s源码分析》 本书将系统讲解kubernetes的核心组建源码 2、版本说明 本书基于：v1.13版本源码讲解 3、协议 本书使用Apache License 2.0协议，但是保留出版图书的权利。 4、贡献 欢迎参与本书编写 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"prepare/":{"url":"prepare/","title":"k8s源码分析准备工作","keywords":"","body":"k8s源码分析准备工作 1. 概述 准备工作分为2部分： 源码准备 调试环境搭建 源码准备阶段主要介绍k8s源码的获取与本地golang编译环境配置等；调试环境搭建是介绍如何准备一个k8s环境，用于后续组件的代码调试。调试环境可以在大体学习完一个组件源码后进行，用于验证自己在看源码过程中的一些想法和疑惑。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:17:11 k8s源码分析准备工作1. 概述"},"prepare/get-code.html":{"url":"prepare/get-code.html","title":"源码准备","keywords":"","body":"源码准备 环境准备 源码下载 源码编译 IDE 1. 环境准备 操作系统：我们使用Linux作为k8s源码分析和调试环境，fedora、centos、ubuntu都行，我这里使用fedora； golang相关： GOROOT=/usr/local/lib/golang GOPATH=/root/go go version go1.10.3 linux/amd64 2. 源码下载 mkdir -p /root/go/src/k8s.io cd /root/go/src/k8s.io/ git clone https://github.com/kubernetes/kubernetes.git 下载后本地目录： 3. 源码编译 我们先看一下几个主要的目录： 目录名 用途 cmd 每个组件代码入口（main函数） pkg 各个组件的具体功能实现 staging 已经分库的项目 vendor 依赖 考虑到国内网络环境等因素，我们不使用容器化方式构建。我们尝试在kubernetes项目cmd目录下构建一个组件（执行路径：/root/go/src/k8s.io/kubernetes/cmd/kube-scheduler）： 这里需要注意一下，如果报依赖错误，找不到k8s.io下的某些项目，就到vendor下看一下软链接是不是都还在，如下： 注意到k8s是使用这种方式解决k8s.io下的依赖问题的，如果我们在windows下下载的代码，然后copy到linux下，就很容易遇到这些软链接丢失的情况，导致go找不到依赖，编译失败。 4. IDE 我们使用Goland看代码： 最后，别忘了在正式研读源码前切换到release-1.13分支～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-18 16:27:26 源码准备1. 环境准备2. 源码下载3. 源码编译4. IDE"},"prepare/debug-environment.html":{"url":"prepare/debug-environment.html","title":"调试环境搭建","keywords":"","body":"调试环境搭建 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/":{"url":"core/","title":"概述","keywords":"","body":"核心组件源码分析 1. 概述 核心组件的源码分析主要包括： scheduler apiserver proxy kubelet controller-manager 在分析第一个组件的时候会穿插一些整体性的介绍，比如源码组织啊、使用的一些三方库啊……；后面有些组件比较依赖其他较大的项目的，比如一个核心组件依赖于对client-go项目的理解，那就会先介绍client-go，当然client-go的介绍不会混在核心组件分析的章节中，我会单独分一个大类“周边项目源码分析”中。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:28:56 核心组件源码分析1. 概述"},"core/scheduler/":{"url":"core/scheduler/","title":"scheduler","keywords":"","body":"scheduler Scheduler部分我们先从设计原理上介绍，然后分析源码，再准备环境调试，最后修改一下源码，实现一个自己的调度器。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-21 17:10:40 "},"core/scheduler/desigh.html":{"url":"core/scheduler/desigh.html","title":"调度器设计","keywords":"","body":"调度器设计 1. 概述 我们先整体了解一下Scheduler的设计原理，然后再看这些过程是如何用代码实现的。关于调度器的设计在官网有介绍，我下面结合官网给的说明，简化掉不影响理解的复杂部分，和大家介绍一下Scheduler的工作过程。 英文还可以的小伙伴们可以看一下官网的介绍先：scheduler.md 官网有一段描述如下： The Kubernetes scheduler runs as a process alongside the other master components such as the API server. Its interface to the API server is to watch for Pods with an empty PodSpec.NodeName, and for each Pod, it posts a binding indicating where the Pod should be scheduled. 简单翻译一下，也就是说Scheduler是一个跑在其他组件边上的独立程序，对接Apiserver寻找PodSpec.NodeName为空的Pod，然后用post的方式发送一个api调用，指定这些pod应该跑在哪个node上。 通俗地说，就是scheduler是相对独立的一个组件，主动访问api server，寻找等待调度的pod，然后通过一系列调度算法寻找哪个node适合跑这个pod，然后将这个pod和node的绑定关系发给api server，从而完成了调度的过程。 2. 源码层级 从高level看，scheduler的源码可以分为3层： cmd/kube-scheduler/scheduler.go: main() 函数入口位置，在scheduler过程开始被调用前的一系列初始化工作。 pkg/scheduler/scheduler.go: 调度框架的整体逻辑，在具体的调度算法之上的框架性的代码。 pkg/scheduler/core/generic_scheduler.go: 具体的计算哪些node适合跑哪些pod的算法。 3. 调度算法 调度过程整体如下图所示（官文里这个图没对齐，逼疯强迫症了！！！当然由于中文显示的问题，下图有中文的行也没法完全对齐，这个地方让我很抓狂。。。）： 对于一个给定的pod +---------------------------------------------+ | 可用于调度的nodes如下： | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 初步过滤: node 3 资源不足 +----------------------+----------------------+ | v +----------------------+----------------------+ | 剩下的nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 优先级算法计算结果: node 1: 分数=2 node 2: 分数=5 +----------------------+----------------------+ | v 选择分值最高的节点 = node 2 Scheduler为每个pod寻找一个适合其运行的node，大体分成三步： 通过一系列的“predicates”过滤掉不能运行pod的node，比如一个pod需要500M的内存，有些节点剩余内存只有100M了，就会被剔除； 通过一系列的“priority functions”给剩下的node排一个等级，分出三六九等，寻找能够运行pod的若干node中最合适的一个node； 得分最高的一个node，也就是被“priority functions”选中的node胜出了，获得了跑对应pod的资格。 4. Predicates 和 priorities 策略 Predicates是一些用于过滤不合适node的策略 . Priorities是一些用于区分node排名（分数）的策略（作用在通过predicates过滤的node上）. K8s默认内建了一些predicates 和 priorities 策略，官方文档介绍地址： scheduler_algorithm.md. Predicates 和 priorities 的代码分别在： pkg/scheduler/algorithm/predicates/predicates.go pkg/scheduler/algorithm/priorities. 5. Scheduler 的拓展性 我们可以选择哪些预置策略生效，也可以添加自己的策略。几个月前我司有个奇葩调度需求，当时我就是通过增加一个priorities策略，然后重新编译了一个Scheduler来实现的需求。 6. 调度策略的修改 默认调度策略是通过defaultPredicates() 和 defaultPriorities()函数定义的，源码在 pkg/scheduler/algorithmprovider/defaults/defaults.go，我们可以通过命令行flag --policy-config-file来覆盖默认行为。所以我们可以通过配置文件的方式或者修改pkg/scheduler/algorithm/predicates/predicates.go /pkg/scheduler/algorithm/priorities，然后注册到defaultPredicates()/defaultPriorities()来实现。配置文件类似下面这个样子： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\"}, {\"name\" : \"PodFitsResources\"}, {\"name\" : \"NoDiskConflict\"}, {\"name\" : \"NoVolumeZoneConflict\"}, {\"name\" : \"MatchNodeSelector\"}, {\"name\" : \"HostName\"} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1}, {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10, \"alwaysCheckAllPredicates\" : false } ok，看到这里大伙应该在流程上对Scheduler的原理有个感性的认识了，下一节我们就开始看一下Scheduler源码是怎么写的。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-21 17:17:43 调度器设计1. 概述2. 源码层级3. 调度算法4. Predicates 和 priorities 策略5. Scheduler 的拓展性6. 调度策略的修改"},"core/apiserver/":{"url":"core/apiserver/","title":"apiserver","keywords":"","body":"apiserver Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/proxy/":{"url":"core/proxy/","title":"proxy","keywords":"","body":"proxy Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/kubelet/":{"url":"core/kubelet/","title":"kubelet","keywords":"","body":"kubelet Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/controller-manager/":{"url":"core/controller-manager/","title":"controller-manager","keywords":"","body":"controller-manager Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"around/":{"url":"around/","title":"概述","keywords":"","body":"周边项目源码分析 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:30:13 "},"around/client-go/":{"url":"around/client-go/","title":"client-go","keywords":"","body":"client-go Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:30:13 "}}