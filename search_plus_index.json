{"./":{"url":"./","title":"前言","keywords":"","body":" 1、关于本书 本书将系统讲解kubernetes的核心组件源码，附带介绍相关的周边项目，比如client-go等。 建议通过公众号CloudGeek（微信上直接搜索可以找到）接收更新消息，通过github pages阅读本书。 寻找组织可以加我微信（ITNT01），一句话证明自己是源码学习者，然后我会拉你进群。 2、内容更新 本项目会不定期更新，一般会在每周五更新一节；更新内容将同步发到公众号CloudGeek、博客园CloudGeek等。细微的更新，比如错别字修改等不会同步到其他所有平台。 每次新章节会选择性提前发到微信群内，比如本周五要发出来的新内容周一可能就内部发到群里了，然后接受读者反馈，定稿后上传到github，然后同步到微信公众号等平台。 3、本教程适合谁读 任何对k8s源码感兴趣的人都可以看本教程，但是我建议你至少有golang项目开发经验，简单的golang开源项目的源码分析经验，k8s应用经验，对golang的基础特性和k8s的基础特性有一定的了解；不然直接上手看k8s源码会郁闷的。 4、版本说明 本书基于：v1.13版本源码讲解。 5、协议 本书使用Apache License 2.0协议，但是保留出版图书的权利。 6、贡献 欢迎参与本书编写！如果你对开源项目源码分析感兴趣，可以和我联系，如果你擅长某个项目，可以主导一个章节的编写。 如果想要提交pull request，建议先开一个issue，说明你要做什么修改，一般1天内我会回复issue，告诉你是否接受修改。但是得提前告诉你我是一个有洁癖的人，包括代码洁癖，文字洁癖等，所以请不要提交太随意的风格的内容哦～ 另外注意：一定先更新你的仓库到最新状态，然后编辑新内容，不然冲突很尴尬～ 7、FAQ 暂时我没有考虑增加评论功能，因为不可避免要增加三方插件，三方插件意味着用户需要注册登录等，体验不会太好。万一哪天这个插件倒闭了，就白忙活了。所以在每章开头我增加了一个FAQ部分，会把这一章中各个小节收到的部分问题汇总在开头的FAQ里。 大家在微信里问我的问题一般我都会耐心解答，但是和go语言本身语法相关的初级问题还是不希望经常遇到，因为我认为语言本身问题是很容易通过网络资料学习掌握的。另外有问题尽量抛到群里，私信多的话有时候我要1个问题很好几个人讲，工作量比较大。 8、支持本书 微信扫一扫，鼓励作者快快更新，写出更多优质的文章～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-19 15:35:51 "},"prepare/":{"url":"prepare/","title":"k8s源码分析准备工作","keywords":"","body":"k8s源码分析准备工作 1. 本章规划 源码准备 测试环境搭建-单节点 测试环境搭建-三节点 源码调试 2. 概述 源码准备阶段主要介绍k8s源码的获取与本地golang编译环境配置等；测试环境搭建是介绍如何准备一个k8s环境，用于后续组件的代码测试。测试环境可以在大体学习完一个组件源码后进行，用于验证自己在看源码过程中的一些想法和疑惑。单节点还是多节点大家可以自己灵活决定。 3. 一些经验 看k8s源码的这些日子里遇到过不少困难，前前后后掉过不少坑，有些经验分享给大家： 语言基本功不扎实不要开始刷k8s，脚踏实地，从小项目开始练手，至少一两千行的小型开源项目能够完全看懂之后再尝试看k8s，不然会受打击的，k8s不是一个入门级项目，也不是用来入门编程语言的项目。 广度优先，大项目一般有很好的层级结构，从高层理解项目的逻辑，再一层一层深入。说个简单的例子，找到main函数后发现main只有10行代码，这时候看完十行代码你得认为自己看完了。确实已经看完了整个流程呀，只是没有深入main里面调用到的几个函数嘛，但是看函数名就知道这个函数的作用了，不知道实现而已。ok，这就是第一遍；然后深入下一层；再举个例子，看调度器的时候第一遍过源码不跟到具体的调度算法，深度优先就陷进去了。第一次应该看整体框架逻辑，找到调用预选、优选代码的入口，知道这个函数完成整个预选过程，就往下继续走。这样一遍过完后心里就有调度器的架子了。然后深入预选逻辑，再看优选逻辑。接着可以看具体的实现算法，可以看调度算法是怎么注册的，怎么初始化的。如果你一开始纠结于调度器如何初始化，可能会很痛苦。广度优先，好好体会。 遇到问题先思考，尤其语言层面的问题要能够自己通过网上资料解决。不愿意静下心多思考是肯定看不完k8s的。假如k8s换成C语言实现，涉及到的语言层面的技巧就变了，但是k8s要解决的问题还是一样的，还是一样的思想，一样的流程，这些语言无关的技巧才是k8s特有的，我觉得这才是重点关注的地方。而类似无缓存channel和有缓存channel区别这类问题，配置文件怎么生效这类问题，可以说都和k8s本身无关。 把学到知识，提升技能作为目的；把看完源码作为结果。一味追求速度很快就会发现看不下去了，身心疲惫。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-21 19:47:58 k8s源码分析准备工作1. 本章规划2. 概述3. 一些经验"},"prepare/get-code.html":{"url":"prepare/get-code.html","title":"源码准备","keywords":"","body":"源码准备 环境准备 源码下载 源码编译 IDE 1. 环境准备 操作系统：我们使用Linux作为k8s源码分析和调试环境，fedora、centos、ubuntu都行，我这里使用fedora； golang相关： GOROOT=/usr/local/lib/golang GOPATH=/root/go go version go1.10.3 linux/amd64 2. 源码下载 mkdir -p /root/go/src/k8s.io cd /root/go/src/k8s.io/ git clone https://github.com/kubernetes/kubernetes.git 下载后本地目录： 3. 源码编译 我们先看一下几个主要的目录： 目录名 用途 cmd 每个组件代码入口（main函数） pkg 各个组件的具体功能实现 staging 已经分库的项目 vendor 依赖 考虑到国内网络环境等因素，我们不使用容器化方式构建。我们尝试在kubernetes项目cmd目录下构建一个组件（执行路径：/root/go/src/k8s.io/kubernetes/cmd/kube-scheduler）： 这里需要注意一下，如果报依赖错误，找不到k8s.io下的某些项目，就到vendor下看一下软链接是不是都还在，如下： 注意到k8s是使用这种方式解决k8s.io下的依赖问题的，如果我们在windows下下载的代码，然后copy到linux下，就很容易遇到这些软链接丢失的情况，导致go找不到依赖，编译失败。 4. IDE 我们使用Goland看代码： 最后，别忘了在正式研读源码前切换到release-1.13分支～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 11:08:05 源码准备1. 环境准备2. 源码下载3. 源码编译4. IDE"},"prepare/debug-environment.html":{"url":"prepare/debug-environment.html","title":"测试环境搭建-单节点","keywords":"","body":"测试环境搭建 （k8s-1.13版本单节点环境搭建） 概述 kubeadm简介 操作系统准备 系统信息 配置selinux和firewalld 系统参数与内核模块 配置yum源 禁用swap 安装docker 安装kubeadm、kubelet和kubectl 镜像准备 阿里云镜像加速器配置（可选） 镜像下载 安装k8s master 环境验证 1. 概述 大家注意哦，不一定要先搭建好环境再看源码，大可以先看一个组件，感觉差不多理解了，想要run一把，想要改几行试试的时候回过头来搭建k8s环境。 当然，大家开始看源码的时候，我相信各位都是搭建过不少次k8s集群，敲过N多次kubectl命令了，所以下面我不会解释太基础的命令是做什么的。 今天我们要做的是搭建一个单机版的k8s环境，用于后面的学习。虽然k8s的环境搭建没有openstack来的复杂，但是由于网络等乱七八糟的问题，在国内手动搭建一个五脏俱全的k8s也不算太容易，一个不小心就会被乱七八遭的障碍磨灭兴趣。今天看看我能不能让大家觉得这个过程无痛吧～ 2. kubeadm简介 选择一个好的工具很重要！大家在刚开始学习k8s的时候应该都用二进制文件一步一步搭建过集群吧，那个过程是不是很酸爽？手动一步一步搭建环境对于初学者来说确实大有裨益，可以了解各个组件的细节。我已经懒掉了，我决定从众多k8s自动化安装方案中选择一个来搭建这次的k8s环境。 kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，这不是一个单独的项目哦，我们在kubernetes源码里可以看到这个组件（kubernetes/cmd/kubeadm/）： kubeadm这个工具可以通过简单的kubeadm init和kubeadm join命令来创建一个kubernetes集群，kubeadm提供的其他命令都比较通俗易懂： kubeadm init 启动一个master节点； kubeadm join 启动一个node节点，加入master； kubeadm upgrade 更新集群版本； kubeadm config 从1.8.0版本开始已经用处不大，可以用来view一下配置； kubeadm token 管理kubeadm join的token； kubeadm reset 把kubeadm init或kubeadm join做的更改恢复原状； kubeadm version打印版本信息； kubeadm alpha预览一些alpha特性的命令。 关于kubeadm的成熟度官方有一个表格： Area Maturity Level Command line UX GA Implementation GA Config file API beta CoreDNS GA kubeadm alpha subcommands alpha High availability alpha DynamicKubeletConfig alpha Self-hosting alpha 主要特性其实都已经GA了，虽然还有一些小特性仍处于活跃开发中，但是整体已经接近准生产级别了。对于我们的场景来说用起来已经绰绰有余！ 3. 操作系统准备 我们先使用一个机子来装，后面需要拓展可以增加节点，使用kubeadm join可以很轻松扩展集群。 3.1. 系统信息 内存：2G CPU：2 磁盘：20G 系统版本和内核版本如下所示，大家不需要严格和我保持一致，不要使用太旧的就行了。 # cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) # uname -r 3.10.0-862.9.1.el7.x86_64 3.2. 配置selinux和firewalld # Set SELinux in permissive mode setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config # Stop and disable firewalld systemctl disable firewalld --now 3.3. 系统参数与内核模块 # 修改内核参数 cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 加载内核模块 modprobe br_netfilter lsmod | grep br_netfilter 3.4. 配置yum源 # base repo cd /etc/yum.repos.d mv CentOS-Base.repo CentOS-Base.repo.bak curl -o CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/gpgcheck=1/gpgcheck=0/g' /etc/yum.repos.d/CentOS-Base.repo # docker repo curl -o docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # k8s repo cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # update cache yum clean all yum makecache yum repolist 最终我们可以看到这些repo： 3.5. 禁用swap swapoff -a echo \"vm.swappiness = 0\">> /etc/sysctl.conf sysctl -p 如上配置，重启后swap又会被挂上，我们还需要注释掉/etc/fstab中的一行配置： 最终看到的结果是这样的（这个截图是下一节搭建集群的时候补到这里的，内存大小和单机的不一样）： 4. 安装docker 先看一下有哪些可用版本：yum list docker-ce --showduplicates | sort -r 我们选择一个版本安装： yum install docker-ce- 这里我选择18.06.3，所以我用的命令是： yum install docker-ce-18.06.3.ce 可以用rpm命令看一下docker-ce这个rpm包带来了哪些文件： 启动docker： systemctl enable docker --now 查看服务状态： systemctl status docker 5. 安装kubeadm、kubelet和kubectl kubeadm不管kubelet和kubectl，所以我们需要手动安装kubelet和kubectl： yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 如果你看到这个教程的时候yum源里已经有了1.14版本的kubeadm，那么要么你装新版本，要么通过上面装docker同样的方式指定1.13版本安装，我这里使用的是1.13.3。 最后启动kubelet： systemctl enable --now kubelet 6. 镜像准备 6.1. 阿里云镜像加速器配置（可选） 如果访问dockerhub速度不理想，可以选择配置阿里容器镜像加速器。 访问阿里云官网：https://www.aliyun.com 注册登录（可以使用支付宝直接登录） 找到镜像加速器配置页面（如果一时找不到，可以在页面上使用搜索功能）：https://cr.console.aliyun.com/cn-hangzhou/mirrors 页面上有详细的配置指南，对着操作就可以了 如果懒得登录阿里云折腾，也可以直接使用我的配置： mkdir -p /etc/docker tee /etc/docker/daemon.json 6.2. 镜像下载 然后我们就可以下载image，下载完记得打个tag： docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.13.3 k8s.gcr.io/kube-apiserver:v1.13.3 docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.13.3 k8s.gcr.io/kube-controller-manager:v1.13.3 docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.13.3 k8s.gcr.io/kube-scheduler:v1.13.3 docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.13.3 k8s.gcr.io/kube-proxy:v1.13.3 docker pull mirrorgooglecontainers/pause-amd64:3.1 docker tag mirrorgooglecontainers/pause-amd64:3.1 k8s.gcr.io/pause:3.1 docker pull mirrorgooglecontainers/etcd-amd64:3.2.24 docker tag mirrorgooglecontainers/etcd-amd64:3.2.24 k8s.gcr.io/etcd:3.2.24 docker pull coredns/coredns:1.2.6 docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6 7. 安装k8s master tip：下面的ip地址(192.168.19.100)大家需要替换成自己机器上的！ kubeadm init --pod-network-cidr=10.100.0.0/16 --service-cidr=10.101.0.0/16 --kubernetes-version=v1.13.3 --apiserver-advertise-address 192.168.19.100 --kubernetes-version: 用于指定k8s版本； --apiserver-advertise-address：用于指定kube-apiserver监听的ip地址； --pod-network-cidr：用于指定Pod的网络范围； --service-cidr：用于指定SVC的网络范围； 如上，跑kubeadm init命令后等几分钟。 如果遇到报错，对着错误信息修正一下。比如没有关闭swap会遇到error，系统cpu不够会遇到error，网络不通等等都会出错，仔细看一下错误信息一般都好解决～ 跑完上面的命令后，会看到类似如下的输出： Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.19.100:6443 --token i472cq.tr9a81qxnyqc5zj2 --discovery-token-ca-cert-hash sha256:acba957db29e0efbffe2cf4e484521b3b7e0f9d5c2ab7f9db68a5e31565d0d66 上面输出告诉我们还需要做一些工作： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # flannel (如果上面自定义了pod ip范围，这里需要修改flannel的configmap) kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml flannel的Network修改：如下图，默认是10.244.0.0/16，我这边pod的cidr是10.100.0.0/16，所以下面也改成10.100.0.0/16； 防止kube-flannel.yml下载地址变化留一份本地链接：点击查看> 创建完后可以看到flannel的configmap如下： 稍等一会，应该可以看到node状态变成ready： # kubectl get node NAME STATUS ROLES AGE VERSION kube-master Ready master 23m v1.13.3 如果你的环境迟迟都是NotReady状态，可以kubectl get pod -n kube-system看一下pod状态，一般可以发现问题，比如flannel的镜像下载失败啦～ 当node Ready的时候，我们可以看到pod也全部ready了： 再看一下核心组件状态： 最后注意到kube-master这个node上有一个Taint： # kubectl describe node kube-master | grep Taints Taints: node-role.kubernetes.io/master:NoSchedule 默认master节点是不跑业务pod的，我们暂时只有一个node，所以先去掉这个Taint： # kubectl taint node kube-master node-role.kubernetes.io/master- node/kube-master untainted # kubectl describe node kube-master | grep Taints Taints: 8. 环境验证 我们来跑一个pod，证明环境正常可用了： 写一个yaml： apiVersion: apps/v1 kind: Deployment metadata: name: mytomcat spec: replicas: 1 selector: matchLabels: app: mytomcat template: metadata: name: mytomcat labels: app: mytomcat spec: containers: - name: mytomcat image: tomcat:8 ports: - containerPort: 8080 如上内容保存为tomcat-deploy.yaml，执行kubectl create -f tomcat-deploy.yaml，然后看pod状态： 确认了，是熟悉的Running，哈哈，基本算大功告成了！最后我们看一下tomcat服务能不能访问到： 很完美，如果加个svc配置，就能够通过浏览器访问到汤姆猫界面了！ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-22 12:34:28 测试环境搭建1. 概述2. kubeadm简介3. 操作系统准备3.1. 系统信息3.2. 配置selinux和firewalld3.3. 系统参数与内核模块3.4. 配置yum源3.5. 禁用swap4. 安装docker5. 安装kubeadm、kubelet和kubectl6. 镜像准备6.1. 阿里云镜像加速器配置（可选）6.2. 镜像下载7. 安装k8s master8. 环境验证"},"prepare/debug-environment-3node.html":{"url":"prepare/debug-environment-3node.html","title":"测试环境搭建-三节点","keywords":"","body":"测试环境搭建-三节点 （k8s-1.13版本三节点环境搭建） 概述 系统准备 镜像和rpms 安装master 添加node节点 安装flannel 环境验证 1. 概述 写在前面：本节不建议在未阅读上一节（单机版环境搭建）的情况下阅读。下面内容稍稍随意，上一节提过的就不重复了。另外都是使用kubeadm实现，没有本质区别，所以下文从简。 前面一节讲了单节点的环境搭建，在调试调度策略等场景的时候单节点不好说明问题，所以今天补一个3节点的集群搭建过程。1个笔记本搭建3节点确实有点压力，外加要源码编译，调试，着实卡到不行。大伙不需要一开始先折腾好环境。在看源码遇到困惑，需要上环境调试或者验证特性单机不够用时再倒腾吧。下附我当前渣渣笔记本配置： 2. 系统准备 和上一节同样的配置方式，这里不再赘述。我这里3个节点基本信息如下： ip hostname 用途 29.123.161.240 kube-master master 节点 29.123.161.207 kube-node1 node 节点 29.123.161.208 kube-node2 node 节点 每个节点的/etc/hosts配置： 3. 镜像和rpms 镜像和rpm包的获取方式和上一节一样，node节点并不需要安装和master一样的rpm包，也不需要全部的镜像，不过我贪方便，直接在3个节点放了一样的“包”；我用的是离线的虚拟机，所以是一下子拷贝了rpm包和镜像tar包这些进去，去区分还不如直接全部装，大家按需自己灵活决定～. 4. 安装master 运行init命令： kubeadm init --pod-network-cidr=10.100.0.0/16 --kubernetes-version=v1.13.3 --apiserver-advertise-address 29.123.161.240 --service-cidr=10.101.0.0/16 运行结束后我们看到如下输出： 对着输出信息初始化： 5. 添加node节点 在2个node节点执行同样的kube join命令（具体命令master安装完输出的信息里可以找到）： 6. 安装flannel 和上一节一样下载yaml文件，镜像可以提前下载好（如果网络不给力）。 yml文件本地链接：点击查看> 执行kubectl create -f kube-flannel.yml（同样因为这里自定义了pod的cidr，所以这里需要修改flannel的yaml配置；如果已经创建了资源，同样可以通过修改configmap实现）； 对应的configmap资源： 稍等一会查看pod状态 查看node状态： 7. 环境验证 同样我们用tomcat镜像来测试： apiVersion: apps/v1 kind: Deployment metadata: name: tomcat spec: replicas: 2 selector: matchLabels: app: tomcat template: metadata: name: tomcat labels: app: tomcat spec: containers: - name: tomcat image: tomcat:8 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: tomcat-svc spec: selector: app: tomcat ports: - name: http port: 8080 targetPort: 8080 protocol: TCP 创建资源： 查看pod和svc： 通过svc访问tomcat服务： ok，3节点的环境验证各种特性基本都够用～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-22 12:33:05 测试环境搭建-三节点1. 概述2. 系统准备3. 镜像和rpms4. 安装master5. 添加node节点6. 安装flannel7. 环境验证"},"prepare/debug.html":{"url":"prepare/debug.html","title":"源码调试","keywords":"","body":"源码调试 概述 k8s环境 配置goland 写在后面 1. 概述 本节内容并不是在准备好k8s环境后就写的，大家也不需要在开始看源码前研究怎么调试代码。今天之前我已经发了《调度器优选过程》也就是差不多准备好k8s环境后一个月了。所以大家也可以先跳过本节，先开始看后面的源码分析，灵活把握开始调试源码的时间吧。 Debug应该是所有“攻城狮”的基本技能。除非是“Hello World”之类的小程序，不然都会出错，俗称bug，有bug就要debug. 今天不赘述调试本身的技术，我们只以调度器为例聊聊怎么把k8s的代码单步执行跑起来，看看内存里是个啥～ 2. k8s环境 我们前面已经搭建了3节点的集群，相关组件主要是static pod的方式在运行，所以调试调度器的时候我们应该把kube-scheduler容器停掉： 1、找到static pod 的yaml文件： 这时候scheduler是运行状态： 2、挪开yaml文件，让scheduler停止： 这时候再看scheduler可以发现pod已经没了 3. 配置goland 在main函数前面点一下这个绿色的三角形，当然这样运行肯定会失败，但是点一下会为我们生成一些配置，可以简化很多事情。点完之后开始配置： 点击上面的Edit，可以看到下面窗口： 这里的Program arguments默认是空的，我们怎么知道这里配置啥呢？ 从前面挪动的yaml中可以看到如下配置： 很明显，拷贝这个scheduler.conf到goland所在的机子，加上--kubeconfig这个flag之后就可以运行了。从前面的截图中可以看到我是将其放在了/etc/kubernetes/scheduler.conf. 开启调试： 如上，进入了熟悉的界面。 当然到这里还没有和api server交互，要进入调度逻辑需要有待调度的pod才行。我们使用前面验证环境的使用的tomcat： 创建这个Deployment之后可以看到pod是pending的： 我们把断点打在scheduleOne()里面： 非常熟悉的界面来了： 这样就能跟到调度器里的各种逻辑了。 当调度器跑完后，pod也就起来了： 最后说下我的goland是跑在哪里的： k8s master：29.123.161.240 k8s node：29.123.161.207 29.123.161.208 goland：29.123.161.241 所以goland不在k8s集群内。当然这个没有啥讲究，也可以跑在一起，灵活决定。 4. 写在后面 本来计划讲一下scheduler里的一些主要流程的调试，但是debug这一节放在调度器那章里感觉又不合适，最后决定放在环境准备里面。让新接触本书的小伙伴可以早点看到，灵活决定开始调试的时间。放在这里的话讲太多调度器的知识也不合适。所以这次就不多说具体代码的调试了，在各个章节里如果哪个模块讲解时我觉得需要插一个调试过程，就直接插在对应的地方吧～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-21 19:47:58 源码调试1. 概述2. k8s环境3. 配置goland4. 写在后面"},"core/":{"url":"core/","title":"概述","keywords":"","body":"核心组件源码分析 1. 概述 核心组件的源码分析主要包括： kube-scheduler kube-controller-manager apiserver proxy kubelet 在分析第一个组件的时候会穿插一些整体性的介绍，比如源码组织啊、使用的一些三方库啊……；后面有些组件比较依赖其他较大的项目的，比如一个核心组件依赖于对client-go项目的理解，那就会先介绍client-go，当然client-go的介绍不会混在核心组件分析的章节中，我会单独分一个大类“周边项目源码分析”中。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-18 09:42:48 核心组件源码分析1. 概述"},"core/scheduler/":{"url":"core/scheduler/","title":"scheduler","keywords":"","body":"scheduler Scheduler部分我们先从设计原理上介绍，然后分析源码，最后针对一些主要算法做专题分析。 1. 本章规划 调度器设计 调度程序启动前逻辑 调度器框架 一般调度过程 预选过程 优选过程 抢占调度 调度器初始化 专题-亲和性调度2. FAQ 读者A提问：如果一个pod的资源占用只有100M，能够运行在一个node上，但是配置成了1000M，这个时候node上其实没有1000M，那么predicate过程还能不能过滤通过？ 回答：如果一个人需要100块钱，卡里有1000块钱，这时候找银行要10000块，银行会给吗？银行不会知道你实际需要多少，你告诉他10000，他就看你卡里有没有10000；同样对于k8s来说你配置了需要1000M，k8s就看node上有没有1000M，没有就调度失败。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-18 09:41:05 scheduler1. 本章规划2. FAQ"},"core/scheduler/design.html":{"url":"core/scheduler/design.html","title":"调度器设计","keywords":"","body":"调度器设计 概述 源码层级 调度算法 Predicates 和 priorities 策略 Scheduler 的拓展性 调度策略的修改 1. 概述 我们先整体了解一下Scheduler的设计原理，然后再看这些过程是如何用代码实现的。关于调度器的设计在官网有介绍，我下面结合官网给的说明，简化掉不影响理解的复杂部分，和大家介绍一下Scheduler的工作过程。 英文还可以的小伙伴们可以看一下官网的介绍先：scheduler.md 官网有一段描述如下： The Kubernetes scheduler runs as a process alongside the other master components such as the API server. Its interface to the API server is to watch for Pods with an empty PodSpec.NodeName, and for each Pod, it posts a binding indicating where the Pod should be scheduled. 简单翻译一下，也就是说Scheduler是一个跑在其他组件边上的独立程序，对接Apiserver寻找PodSpec.NodeName为空的Pod，然后用post的方式发送一个api调用，指定这些pod应该跑在哪个node上。 通俗地说，就是scheduler是相对独立的一个组件，主动访问api server，寻找等待调度的pod，然后通过一系列调度算法寻找哪个node适合跑这个pod，然后将这个pod和node的绑定关系发给api server，从而完成了调度的过程。 2. 源码层级 从高level看，scheduler的源码可以分为3层： cmd/kube-scheduler/scheduler.go: main() 函数入口位置，在scheduler过程开始被调用前的一系列初始化工作。 pkg/scheduler/scheduler.go: 调度框架的整体逻辑，在具体的调度算法之上的框架性的代码。 pkg/scheduler/core/generic_scheduler.go: 具体的计算哪些node适合跑哪些pod的算法。 3. 调度算法 调度过程整体如下图所示（官文里这个图没对齐，逼疯强迫症了！！！当然由于中文显示的问题，下图有中文的行也没法完全对齐，这个地方让我很抓狂。。。）： 对于一个给定的pod +---------------------------------------------+ | 可用于调度的nodes如下： | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 初步过滤: node 3 资源不足 +----------------------+----------------------+ | v +----------------------+----------------------+ | 剩下的nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 优先级算法计算结果: node 1: 分数=2 node 2: 分数=5 +----------------------+----------------------+ | v 选择分值最高的节点 = node 2 Scheduler为每个pod寻找一个适合其运行的node，大体分成三步： 通过一系列的“predicates”过滤掉不能运行pod的node，比如一个pod需要500M的内存，有些节点剩余内存只有100M了，就会被剔除； 通过一系列的“priority functions”给剩下的node排一个等级，分出三六九等，寻找能够运行pod的若干node中最合适的一个node； 得分最高的一个node，也就是被“priority functions”选中的node胜出了，获得了跑对应pod的资格。 4. Predicates 和 priorities 策略 Predicates是一些用于过滤不合适node的策略 . Priorities是一些用于区分node排名（分数）的策略（作用在通过predicates过滤的node上）. K8s默认内建了一些predicates 和 priorities 策略，官方文档介绍地址： scheduler_algorithm.md. Predicates 和 priorities 的代码分别在： pkg/scheduler/algorithm/predicates/predicates.go pkg/scheduler/algorithm/priorities. 5. Scheduler 的拓展性 我们可以选择哪些预置策略生效，也可以添加自己的策略。几个月前我司有个奇葩调度需求，当时我就是通过增加一个priorities策略，然后重新编译了一个Scheduler来实现的需求。 6. 调度策略的修改 默认调度策略是通过defaultPredicates() 和 defaultPriorities()函数定义的，源码在 pkg/scheduler/algorithmprovider/defaults/defaults.go，我们可以通过命令行flag --policy-config-file来覆盖默认行为。所以我们可以通过配置文件的方式或者修改pkg/scheduler/algorithm/predicates/predicates.go /pkg/scheduler/algorithm/priorities，然后注册到defaultPredicates()/defaultPriorities()来实现。配置文件类似下面这个样子： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\"}, {\"name\" : \"PodFitsResources\"}, {\"name\" : \"NoDiskConflict\"}, {\"name\" : \"NoVolumeZoneConflict\"}, {\"name\" : \"MatchNodeSelector\"}, {\"name\" : \"HostName\"} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1}, {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10, \"alwaysCheckAllPredicates\" : false } ok，看到这里大伙应该在流程上对Scheduler的原理有个感性的认识了，下一节我们就开始看一下Scheduler源码是怎么写的。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-27 16:24:20 调度器设计1. 概述2. 源码层级3. 调度算法4. Predicates 和 priorities 策略5. Scheduler 的拓展性6. 调度策略的修改"},"core/scheduler/before-scheduler-run.html":{"url":"core/scheduler/before-scheduler-run.html","title":"调度程序启动前逻辑","keywords":"","body":"调度程序启动前逻辑 概述 cobra和main cobra是啥 使用cobra Scheduler的main 1. 概述 前面提到过scheduler程序可以分为三层，第一层是调度器启动前的逻辑，包括命令行参数解析、参数校验、调度器初始化等一系列逻辑。这个部分我不会太详细地介绍，因为这些代码位于调度框架之前，相对比较枯燥无趣，讲多了磨灭大伙对源码的兴趣～ 2. cobra和main 剧透一下先，如果你之前没有用过cobra，那么在第一次见到cobra之后，很可能以后你自己写的程序，开发的小工具会全部变成cobra风格。我最近半年写的命令行程序就全部是基于cobra+pflag的。cobra有多优雅呢，且听我慢慢道来～ 2.1. cobra是啥 从github上我们可以找到这个项目，截至今天已经有上万个star，一百多个contributors，可见来头不小！Cobra官方描述是： Cobra is both a library for creating powerful modern CLI applications as well as a program to generate applications and command files. 也就是这个意思：Cobra既是一个创建强大的现代化命令行程序的库，又是一个用于生成应用和命令行文件的程序。有很多流行的Go项目用了Cobra，其中当然包括我们最最熟知的k8s和docker，大致列出来有这些： Kubernetes Hugo rkt etcd Moby (former Docker) Docker (distribution) OpenShift Delve GopherJS CockroachDB Bleve ProjectAtomic (enterprise) Giant Swarm's gsctl Nanobox/Nanopack rclone nehm Pouch 如果你是云计算方向的攻城狮，上面半数项目应该都耳熟能详～ 2.2. 使用cobra 下面我们实践一下cobra，先下载这个项目编译一下： # 如果你的网络很给力，那么下面这个命令就够了； go get -u github.com/spf13/cobra/cobra # 如果你的网络不给力，那就下载cobra的zip包，丢到GOPATH下对应目录，然后解决依赖，再build 于是我们得到了这样一个可执行文件及项目源码： 我们试一下这个命令：cobra init ${project-name} [root@farmer-hutao src]# cobra init myapp Your Cobra application is ready at /root/go/src/myapp Give it a try by going there and running `go run main.go`. Add commands to it by running `cobra add [cmdname]`. [root@farmer-hutao src]# ls myapp/ cmd LICENSE main.go [root@farmer-hutao src]# pwd /root/go/src 如上，本地可以看到一个main.go和一个cmd目录，这个cmd和k8s源码里的cmd是不是很像～ main.go里面的代码很精简，如下： main.go package main import \"myapp/cmd\" func main() { cmd.Execute() } 这里注意到调用了一个cmd的Execute()方法，我们继续看cmd是什么： 如上图，在main.go里面import了myapp/cmd，也就是这个root.go文件。所以Execute()函数就很好找了。在Execute里面调用了rootCmd.Execute()方法，这个rootCmd是*cobra.Command类型的。我们关注一下这个类型。 下面我们继续使用cobra命令给myapp添加一个子命令： 如上，我们的程序可以使用version子命令了！我们看一下源码发生了什么变化： 多了一个version.go，在这个源文件的init()函数里面调用了一个rootCmd.AddCommand(versionCmd)，这里可以猜到是根命令下添加一个子命令的意思，根命令表示的就是我们直接执行这个可执行文件，子命令就是version，放在一起的感觉就类似大家使用kubectl version的感觉。 另外注意到这里的Run属性是一个匿名函数，这个函数中输出了“version called”字样，也就是说我们执行version子命令的时候其实是调用到了这里的Run. 最后我们实践一下多级子命令： 套路也就这样，通过serverCmd.AddCommand(createCmd)调用后就能够把*cobra.Command类型的createCmd变成serverCmd的子命令了，这个时候我们玩起来就像kubectl get pods. 行，看到这里我们回头看一下scheduler的源码就能找到main的逻辑了。 3. Scheduler的main 我们打开文件：cmd/kube-scheduler/scheduler.go可以找到scheduler的main()函数，很简短，去掉枝干后如下： cmd/kube-scheduler/scheduler.go:34 func main() { command := app.NewSchedulerCommand() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } 看到这里猜都能猜到kube-scheduler这个二进制文件在运行的时候是调用了command.Execute()函数背后的那个Run，那个Run躲在command := app.NewSchedulerCommand()这行代码调用的NewSchedulerCommand()方法里，这个方法一定返回了一个*cobra.Command类型的对象。我们跟进去这个函数，看一下是不是这个样子： cmd/kube-scheduler/app/server.go:70 / NewSchedulerCommand creates a *cobra.Command object with default parameters func NewSchedulerCommand() *cobra.Command { cmd := &cobra.Command{ Use: \"kube-scheduler\", Long: `The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity. The scheduler needs to take into account individual and collective resource requirements, quality of service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, deadlines, and so on. Workload-specific requirements will be exposed through the API as necessary.`, Run: func(cmd *cobra.Command, args []string) { if err := runCommand(cmd, args, opts); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } return cmd } 如上，同样我先删掉了一些枝干代码，剩下的可以很清楚地看到，schduler启动时调用了runCommand(cmd, args, opts)，这个函数在哪里呢，继续跟一下： cmd/kube-scheduler/app/server.go:117 // runCommand runs the scheduler. func runCommand(cmd *cobra.Command, args []string, opts *options.Options) error { c, err := opts.Config() stopCh := make(chan struct{}) // Get the completed config cc := c.Complete() return Run(cc, stopCh) } 如上，可以看到这里是处理配置问题后调用了一个Run()函数，Run()的作用是基于给定的配置启动scheduler，它只会在出错时或者channel stopCh被关闭时才退出，代码主要部分如下： cmd/kube-scheduler/app/server.go:167 // Run executes the scheduler based on the given configuration. It only return on error or when stopCh is closed. func Run(cc schedulerserverconfig.CompletedConfig, stopCh 可以看到这里最终是要跑sched.Run()这个方法来启动scheduler，sched.Run()方法已经在pkg下，具体位置是pkg/scheduler/scheduler.go:276，也就是scheduler框架真正运行的逻辑了。于是我们已经从main出发，找到了scheduler主框架的入口，具体的scheduler逻辑我们下一讲再来仔细分析。 最后我们来看一下sched的定义，在linux里我们经常会看到一些软件叫做什么什么d，d也就是daemon，守护进程的意思，也就是一直跑在后台的一个程序。这里的sched也就是“scheduler daemon”的意思。sched的其实是*Scheduler类型，定义在： pkg/scheduler/scheduler.go:58 // Scheduler watches for new unscheduled pods. It attempts to find // nodes that they fit on and writes bindings back to the api server. type Scheduler struct { config *factory.Config } 如上，注释也很清晰，说Scheduler watch新创建的未被调度的pods，然后尝试寻找合适的node，回写一个绑定关系到api server.这里也可以体会到daemon的感觉，我们平时搭建的k8s集群中运行着一个daemon进程叫做kube-scheduler，这个一直跑着的进程做的就是上面注释里说的事情，在程序里面也就对应这样一个对象：Scheduler. Scheduler结构体中的Config对象我们再简单看一下： pkg/scheduler/factory/factory.go:96 // Config is an implementation of the Scheduler's configured input data. type Config struct { // It is expected that changes made via SchedulerCache will be observed // by NodeLister and Algorithm. SchedulerCache schedulerinternalcache.Cache // Ecache is used for optimistically invalid affected cache items after // successfully binding a pod Ecache *equivalence.Cache NodeLister algorithm.NodeLister Algorithm algorithm.ScheduleAlgorithm GetBinder func(pod *v1.Pod) Binder // PodConditionUpdater is used only in case of scheduling errors. If we succeed // with scheduling, PodScheduled condition will be updated in apiserver in /bind // handler so that binding and setting PodCondition it is atomic. PodConditionUpdater PodConditionUpdater // PodPreemptor is used to evict pods and update pod annotations. PodPreemptor PodPreemptor // NextPod should be a function that blocks until the next pod // is available. We don't use a channel for this, because scheduling // a pod may take some amount of time and we don't want pods to get // stale while they sit in a channel. NextPod func() *v1.Pod // SchedulingQueue holds pods to be scheduled SchedulingQueue internalqueue.SchedulingQueue } 如上，同样我只保留了一些好理解的字段，我们随便扫一下可以看到譬如：SchedulingQueue、NextPod、NodeLister这些很容易从字面上理解的字段，也就是Scheduler对象在工作（完成调度这件事）中需要用到的一些对象。 ok，下一讲我们开始聊Scheduler的工作过程！ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 11:13:36 调度程序启动前逻辑1. 概述2. cobra和main2.1. cobra是啥2.2. 使用cobra3. Scheduler的main"},"core/scheduler/scheduler-framework.html":{"url":"core/scheduler/scheduler-framework.html","title":"调度器框架","keywords":"","body":"调度器框架 写在前面 调度器启动运行 一个pod的调度流程 潜入第三层前的一点逻辑 1. 写在前面 今天我们从pkg/scheduler/scheduler.go出发，分析Scheduler的整体框架。前面讲Scheduler设计的时候有提到过源码的3层结构，pkg/scheduler/scheduler.go也就是中间这一层，负责Scheduler除了具体node过滤算法外的工作逻辑～ 这一层我们先尽可能找主线，顺着主线走通一遍，就像走一个迷宫，一条通路走出去后心里就有地了，但是迷宫中的很多角落是未曾涉足的。我们尽快走通主流程后，再就一些主要知识点专题攻破，比如k8s里面的List-Watch，Informer等好玩的东西。 2. 调度器启动运行 从goland的Structure中可以看到这个源文件(pkg/scheduler/scheduler.go)主要有这些对象： 大概浏览一下可以很快找到我们的第一个关注点应该是Scheduler这个struct和Scheduler的Run()方法： pkg/scheduler/scheduler.go:58 // Scheduler watches for new unscheduled pods. It attempts to find // nodes that they fit on and writes bindings back to the api server. type Scheduler struct { config *factory.Config } 这个struct在上一讲有跟到过，代码注释说的是： Scheduler watch新创建的未被调度的pods，然后尝试寻找合适的node，回写一个绑定关系到api server. 这个注释有个小问题就是用了复数形式，其实最后过滤出来的只有一个node；当然这种小问题知道就好，提到github上人家会觉得你在刷commit.接着往下看，Scheduler绑定了一个Run()方法，如下： pkg/scheduler/scheduler.go:276 // Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } 注释说这个函数开始watching and scheduling，也就是调度器主要逻辑了！注释后半段说到Run()方法起了一个goroutine后马上返回了，这个怎么理解呢？我们先看一下调用Run的地方： cmd/kube-scheduler/app/server.go:240 // Prepare a reusable runCommand function. run := func(ctx context.Context) { sched.Run() 可以发现调用了sched.Run()之后就在等待ctx.Done()了，所以Run中启动的goroutine自己不退出就ok. wait.Until这个函数做的事情是：每隔n时间调用f一次，除非channel c被关闭。这里的n就是0，也就是一直调用，前一次调用返回下一次调用就开始了。这里的f当然就是sched.scheduleOne，c就是sched.config.StopEverything. 3. 一个pod的调度流程 于是我们的关注点就转到了sched.scheduleOne这个方法上，看一下： scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. 注释里说scheduleOne实现1个pod的完整调度工作流，这个过程是顺序执行的，也就是非并发的。结合前面的wait.Until逻辑，也就是说前一个pod的scheduleOne一完成，一个return，下一个pod的scheduleOne立马接着执行！ 这里的串行逻辑也好理解，如果是同时调度N个pod，计算的时候觉得一个node很空闲，实际调度过去启动的时候发现别人的一群pod先起来了，端口啊，内存啊，全给你抢走了！所以这里的调度算法执行过程用串行逻辑很好理解。注意哦，调度过程跑完不是说要等pod起来，最后一步是写一个binding到apiserver，所以不会太慢。下面我们看一下scheduleOne的主要逻辑： pkg/scheduler/scheduler.go:513 func (sched *Scheduler) scheduleOne() { pod := sched.config.NextPod() suggestedHost, err := sched.schedule(pod) if err != nil { if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) } return } assumedPod := pod.DeepCopy() allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) err = sched.assume(assumedPod, suggestedHost) go func() { err := sched.bind(assumedPod, &v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) }() } 上面几行代码只保留了主干，对于我们理解scheduleOne的过程足够了，这里来个流程图吧： 不考虑scheduleOne的所有细节和各种异常情况，基本是上图的流程了，主流程的核心步骤当然是suggestedHost, err := sched.schedule(pod)这一行，这里完成了不需要抢占的场景下node的计算，我们耳熟能详的预选过程，优选过程等就是在这里面。 4. 潜入第三层前的一点逻辑 ok，这时候重点就转移到了suggestedHost, err := sched.schedule(pod)这个过程，强调一下这个过程是“同步”执行的。 pkg/scheduler/scheduler.go:290 // schedule implements the scheduling algorithm and returns the suggested host. func (sched *Scheduler) schedule(pod *v1.Pod) (string, error) { host, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) if err != nil { pod = pod.DeepCopy() sched.config.Error(pod, err) sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"%v\", err) sched.config.PodConditionUpdater.Update(pod, &v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, LastProbeTime: metav1.Now(), Reason: v1.PodReasonUnschedulable, Message: err.Error(), }) return \"\", err } return host, err } schedule方法很简短，我们关注一下第一行，调用sched.config.Algorithm.Schedule()方法，入参是pod和nodes，返回一个host，继续看一下这个Schedule方法： pkg/scheduler/algorithm/scheduler_interface.go:78 type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) Predicates() map[string]FitPredicate Prioritizers() []PriorityConfig } 发现是个接口，这个接口有4个方法，实现ScheduleAlgorithm接口的对象意味着知道如何调度pods到nodes上。默认的实现是pkg/scheduler/core/generic_scheduler.go:98 genericScheduler这个struct.我们先继续看一下ScheduleAlgorithm接口定义的4个方法： Schedule() //给定pod和nodes，计算出一个适合跑pod的node并返回； Preempt() //抢占 Predicates() //预选 Prioritizers() //优选 前面流程里讲到的sched.config.Algorithm.Schedule()也就是genericScheduler.Schedule()方法了，这个方法位于：pkg/scheduler/core/generic_scheduler.go:139一句话概括这个方法就是：尝试将指定的pod调度到给定的node列表中的一个，如果成功就返回这个node的名字。最后看一眼签名： func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) 从如参和返回值其实可以猜到很多东西，行，今天就到这里，具体的逻辑下回我们再分析～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 14:00:16 调度器框架1. 写在前面2. 调度器启动运行3. 一个pod的调度流程4. 潜入第三层前的一点逻辑"},"core/scheduler/generic-scheduler.html":{"url":"core/scheduler/generic-scheduler.html","title":"一般调度过程","keywords":"","body":"一般调度过程 进入Scheduler的第三层逻辑 Computing predicates Prioritizing Selecting host 1. 进入Scheduler的第三层逻辑 今天分析的代码，就已经算kube-scheduler的第三层逻辑了，我们要找到预选和优选的入口，讲完太长，干脆后面单独分2节讲预选和优选过程。所以本小节会比较简短哦～ 今天我们从pkg/scheduler/core/generic_scheduler.go:139开始，也就是从这个generic scheduler的Schedule()方法下手！ 我们依旧关心主干先，这个方法主要涉及的是预选过程+优选过程，看下主要代码： pkg/scheduler/core/generic_scheduler.go:139 func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { nodes, err := nodeLister.List() trace.Step(\"Computing predicates\") filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) trace.Step(\"Prioritizing\") priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) trace.Step(\"Selecting host\") return g.selectHost(priorityList) } 如上，我手一抖就删的只剩下这几行了，大伙应该从这不到十行的代码里找到3个步骤： \"Computing predicates\"：调用findNodesThatFit()方法； \"Prioritizing\"：调用PrioritizeNodes()方法； \"Selecting host\"：调用selectHost()方法。 接着当然是先浏览一下这3步分别完成了哪些工作咯～ 1.1. Computing predicates 这个过程的入口是： filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) 从变量命名上其实就可以猜到一大半，filteredNodes肯定就是过滤出来的nodes，也就是经受住了预选算法考验的node集合，我们从findNodesThatFit方法的函数签名中可以得到准确一些的信息： pkg/scheduler/core/generic_scheduler.go:389 func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) 入参是1个pod + 一堆node，返回值是一堆node（这个堆堆当然 1.2. Prioritizing Prioritizing的入口看着复杂一点： priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 注意到这里的返回值叫做priorityList，什么什么List也就是不止一个了，优选过程不是选出1个最佳节点吗？我们继续看： pkg/scheduler/core/generic_scheduler.go:624 func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) 首选关注返回值是什么意思： pkg/scheduler/api/types.go:305 type HostPriority struct { // Name of the host Host string // Score associated with the host Score int } // HostPriorityList declares a []HostPriority type. type HostPriorityList []HostPriority 看到这里就清晰了，原来有个HostPriority类型记录一个Host的名字和分值，HostPriorityList类型也就是HostPriority类型的集合，意味着记录了多个Host的名字和分值，于是我们可以判断PrioritizeNodes()方法的作用是计算前面的predicates过程筛选出来的nodes各自的Score.所以肯定还有一个根据Score决定哪个node胜出的逻辑咯～，继续往下看吧～ 1.3. Selecting host 这个过程比较明显了，我们直接看代码： pkg/scheduler/core/generic_scheduler.go:227 func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) 这个selectHost()方法大家应该都已经猜到了，就是从上一步的优选过程的结果集中选出一个Score最高的Host，并且返回这个Host的name. genericScheduler的Schedule()方法主要就是这3个过程，下一讲我们开始分析predicates过程。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-04 12:45:17 一般调度过程1. 进入Scheduler的第三层逻辑1.1. Computing predicates1.2. Prioritizing1.3. Selecting host"},"core/scheduler/predicate.html":{"url":"core/scheduler/predicate.html","title":"预选过程","keywords":"","body":"预选过程 预选流程 predicate的并发 一个node的predicate predicates的顺序 单个predicate执行过程 具体的predicate函数 1. 预选流程 predicate过程从pkg/scheduler/core/generic_scheduler.go:389 findNodesThatFit()方法就算正式开始了，这个方法根据给定的predicate functions过滤所有的nodes来寻找一堆可以跑pod的node集。老规矩，我们来看主干代码： pkg/scheduler/core/generic_scheduler.go:389 func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) { checkNode := func(i int) { fits, failedPredicates, err := podFitsOnNode( //…… ) if fits { length := atomic.AddInt32(&filteredLen, 1) filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) if len(filtered) > 0 && len(g.extenders) != 0 { for _, extender := range g.extenders { // Logic of extenders } } return filtered, failedPredicateMap, nil } 如上，删的有点多，大家也可以看一下原函数然后对比一下，看看我为什么只保留这一点。从上面代码中我们可以发现，最重要的是一个子函数调用过程fits, failedPredicates, err := podFitsOnNode()，这个函数的参数我没有贴出来，下面会详细讲；下半部分是一个extender过程，extender不影响对predicate过程的理解，我们后面专门当作一个主题讲。所以这里的关注点是podFitsOnNode()函数。 2. predicate的并发 进入podFitsOnNode()函数逻辑之前，我们先看一下调用到podFitsOnNode()函数的匿名函数变量checkNode是怎么被调用的： pkg/scheduler/core/generic_scheduler.go:458 workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) ParallelizeUntil()函数是用于并行执行N个独立的工作过程的，这个逻辑写的挺有意思，我们看一下完整的代码(这段的分析思路写到注释里哦)： vendor/k8s.io/client-go/util/workqueue/parallelizer.go:38 func ParallelizeUntil(ctx context.Context, workers, pieces int, doWorkPiece DoWorkPieceFunc) { // 从形参列表看，需要关注的有workers和pieces两个数字类型的参数，doworkPiece这个函数类型的参数 // DoWorkPieceFunc类型也就是func(piece int)类型 // 注意到上面调用的时候workers的实参是16，pieces是allNodes，也就是node数量 var stop 回想一下前面的checkNode := func(i int){……}，上面的doWorkPiece(piece)也就是调用到了这里的这个匿名函数func(i int){……}；到这里就清楚如何实现并发执行多个node的predicate过程了。 3. 一个node的predicate checkNode的主要逻辑就是上面介绍的并发加上下面这个podFitsOnNode()函数逻辑： pkg/scheduler/core/generic_scheduler.go:425 fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) 我们从podFitsOnNode()的函数定义入手： pkg/scheduler/core/generic_scheduler.go:537 func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, nodeCache *equivalence.NodeCache, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) 关于这个函数的逻辑，注释里的描述翻译过来大概是这个意思： podFitsOnNode()函数检查一个通过NodeInfo形式给定的node是否满足指定的predicate functions. 对于给定的一个Pod，podFitsOnNode()函数会检查是否有某个“等价的pod”存在，然后重用那个等价pod缓存的predicate结果。 这个函数的调用入口有2处: Schedule and Preempt. 当从Schedule进入时：这个函数想要测试node上所有已经存在的pod外加被指定将要调度到这个node上的其他所有高优先级（优先级不比自己低，也就是>=）的pod后，当前pod是否可以被调度到这个node上。 当从Preempt进入时：后面讲preempt时再详细分析。 podFitsOnNode()函数的参数有点多，每个跟进去就是一堆知识点。这里建议大家从字面先过一边，然后跟进去看一下类型定义，类型的注释等，了解一下功能，先不深究。整体看完一边调度器代码后回过头深入细节。 我们一起看一下其中这个参数：predicateFuncs map[string]algorithm.FitPredicate；这里的predicateFuncs是一个map，表示所有的predicate函数。这个map的key是个字符串，也就是某种形式的name了；value类型跟进去看一下： pkg/scheduler/algorithm/types.go:36 // FitPredicate is a function that indicates if a pod fits into an existing node. // The failure information is given by the error. type FitPredicate func(pod *v1.Pod, meta PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []PredicateFailureReason, error) FitPredicate是一个函数类型，3个参数，pod和node都很好理解，meta跟进去简单看一下可以发现定义的是一些和predicate相关的一些元数据，这些数据是根据pod和node信息获取到的，类似pod的端口有哪些，pod亲和的pod列表等。返回值是一个表示是否fit的bool值，predicate失败的原因列表，一个错误类型。 也就是说，FitPredicate这个函数类型也就是前面一直说的predicate functions的真面目了。下面看podFitsOnNode()函数的具体逻辑吧： pkg/scheduler/core/generic_scheduler.go:537 func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, nodeCache *equivalence.NodeCache, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) { podsAdded := false for i := 0; i 这里的逻辑是从一个for循环开始的，关于这个2次循环的含义代码里有很长的一段注释，我们先看一下注释里怎么说的（这里可以多看几遍体会一下）： 出于某些原因考虑我们需要运行两次predicate. 如果node上有更高或者相同优先级的“指定pods”（这里的“指定pods”指的是通过schedule计算后指定要跑在一个node上但是还未真正运行到那个node上的pods），我们将这些pods加入到meta和nodeInfo后执行一次计算过程。 如果这个过程所有的predicates都成功了，我们再假设这些“指定pods”不会跑到node上再运行一次。第二次计算是必须的，因为有一些predicates比如pod亲和性，也许在“指定pods”没有成功跑到node的情况下会不满足。 如果没有“指定pods”或者第一次计算过程失败了，那么第二次计算不会进行。 我们在第一次调度的时候只考虑相等或者更高优先级的pods，因为这些pod是当前pod必须“臣服”的，也就是说不能够从这些pod中抢到资源，这些pod不会被当前pod“抢占”；这样当前pod也就能够安心从低优先级的pod手里抢资源了。 新pod在上述2种情况下都可调度基于一个保守的假设：资源和pod反亲和性等的predicate在“指定pods”被处理为Running时更容易失败；pod亲和性在“指定pods”被处理为Not Running时更加容易失败。 我们不能假设“指定pods”是Running的因为它们当前还没有运行，而且事实上，它们确实有可能最终又被调度到其他node上了。 看了这个注释后，上面代码里的前几行就很好理解了，在第一次进入循环体和第二次进入时做了不同的处理，具体怎么做的处理我们暂时不关注。下面看省略的这个for循环做了啥： pkg/scheduler/core/generic_scheduler.go:583 // predicates.Ordering()得到的是一个[]string，predicate名字集合 for predicateID, predicateKey := range predicates.Ordering() { var ( fit bool reasons []algorithm.PredicateFailureReason err error ) // 如果predicateFuncs有这个key，则调用这个predicate；也就是说predicateFuncs如果定义了一堆乱七八遭的名字，会被忽略调，因为predicateKey是内置的。 if predicate, exist := predicateFuncs[predicateKey]; exist { // 降低难度，先不看缓存情况。 if eCacheAvailable { fit, reasons, err = nodeCache.RunPredicate(predicate, predicateKey, predicateID, pod, metaToUse, nodeInfoToUse, equivClass) } else { // 真正调用predicate函数了！！！！！！！！！ fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) } if err != nil { return false, []algorithm.PredicateFailureReason{}, err } if !fit { // …… } } } 如上，我们看一下2个地方： predicates.Ordering() fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) 分两个小节吧～ 3.1. predicates的顺序 pkg/scheduler/algorithm/predicates/predicates.go:130 var ( predicatesOrdering = []string{ CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred} ) 如上，这里定义了一个次序，前面的for循环遍历的是这个[]string，这样也就实现了不管predicateFuncs里定义了怎样的顺序，影响不了predicate的实际调用顺序。官网对于这个顺序有这样一个表格解释： Position Predicate comments (note, justification...) 1 CheckNodeConditionPredicate we really don’t want to check predicates against unschedulable nodes. 2 PodFitsHost we check the pod.spec.nodeName. 3 PodFitsHostPorts we check ports asked on the spec. 4 PodMatchNodeSelector check node label after narrowing search. 5 PodFitsResources this one comes here since it’s not restrictive enough as we do not try to match values but ranges. 6 NoDiskConflict Following the resource predicate, we check disk 7 PodToleratesNodeTaints check toleration here, as node might have toleration 8 PodToleratesNodeNoExecuteTaints check toleration here, as node might have toleration 9 CheckNodeLabelPresence labels are easy to check, so this one goes before 10 checkServiceAffinity - 11 MaxPDVolumeCountPredicate - 12 VolumeNodePredicate - 13 VolumeZonePredicate - 14 CheckNodeMemoryPressurePredicate doesn’t happen often 15 CheckNodeDiskPressurePredicate doesn’t happen often 16 InterPodAffinityMatches Most expensive predicate to compute 这个表格大家对着字面意思体会一下吧，基本还是可以联想到意义的。 当然这个顺序是可以被配置文件覆盖的，用户可以使用类似这样的配置： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\", \"order\": 2}, {\"name\" : \"PodFitsResources\", \"order\": 3}, {\"name\" : \"NoDiskConflict\", \"order\": 5}, {\"name\" : \"PodToleratesNodeTaints\", \"order\": 4}, {\"name\" : \"MatchNodeSelector\", \"order\": 6}, {\"name\" : \"PodFitsHost\", \"order\": 1} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1}, {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10 } 整体过完源码后我们再实际尝试一下这些特性，这一边先知道有这回事吧，ok，继续～ 3.2. 单个predicate执行过程 fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) 这行代码其实没有啥复杂逻辑，不过我们还是重复讲一下，清晰理解这一行很有必要。这里的predicate()来自前几行的if语句predicate, exist := predicateFuncs[predicateKey]，往前跟也就是FitPredicate类型，我们前面提过，类型定义在pkg/scheduler/algorithm/types.go:36，这个类型表示的是一个具体的predicate函数，这里使用predicate()也就是一个函数调用的语法，很和谐了。 3.3. 具体的predicate函数 一直在讲predicate，那么predicate函数到底长什么样子呢，我们从具体的实现函数找一个看一下。开始讲design的时候提到过predicate的实现在pkg/scheduler/algorithm/predicates/predicates.go文件中，先看一眼Structure吧： 这个文件中predicate函数有点多，这样看眼花，我们具体点开一个观察一下： pkg/scheduler/algorithm/predicates/predicates.go:277 func NoDiskConflict(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { for _, v := range pod.Spec.Volumes { for _, ev := range nodeInfo.Pods() { if isVolumeConflict(v, ev) { return false, []algorithm.PredicateFailureReason{ErrDiskConflict}, nil } } } return true, nil, nil } 我们知道predicate函数的特点，这样就很好在这个一千六百多行go文件中寻找predicate函数了。像上面这个NoDiskConflict()函数，参数是pod、meta和nodeinfo，很明显是FitPredicate类型的，标准的predicate函数。 这个函数的实现也特别简单，遍历pod的Volumes，然后对于pod的每一个Volume，遍历node上的每个pod，看是否和当前podVolume冲突。如果不fit就返回false加原因；如果fit就返回true，很清晰。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-06 18:50:29 预选过程1. 预选流程2. predicate的并发3. 一个node的predicate3.1. predicates的顺序3.2. 单个predicate执行过程3.3. 具体的predicate函数"},"core/scheduler/priority.html":{"url":"core/scheduler/priority.html","title":"优选过程","keywords":"","body":"优选过程 走近priority过程 PrioritizeNodes整体流程 Results Old Priority Function Map-Reduce Combine Scores Fun和Map-Reduce实例分析 InterPodAffinityPriority(Function) CalculateNodeAffinityPriorityMap(Map) CalculateNodeAffinityPriorityReduce(Reduce) 小结 1. 走近priority过程 pkg/scheduler/core/generic_scheduler.go:186 priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 今天的分析从这行代码开始。 PrioritizeNodes要做的事情是给已经通过predicate的nodes赋上一个分值，从而抉出一个最优node用于运行当前pod. 第一次看priority可能会一脸蒙，和predicate中的逻辑不太一样；大伙得耐下性子多思考，实在有障碍也可以先不求甚解，整体过完后再二刷代码，再不行三刷，总会大彻大悟的！ 从注释中可以找到关于PrioritizeNodes的原理(pkg/scheduler/core/generic_scheduler.go:624)： PrioritizeNodes通过并发调用一个个priority函数来给node排优先级。每一个priority函数会给一个1-10之间的分值，0最低10最高。 每一个priority函数可以有自己的权重，单个函数返回的分值*权重后得到一个加权分值，最终所有的加权分值加在一起就是这个node的最终分值。 然后我们先函数签名入手： pkg/scheduler/core/generic_scheduler.go:624 func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) 形参定义和返回值： pod *v1.Pod* // pod就不用说了； *nodeNameToInfo map[string]*schedulercache.NodeInfo // 这个也不需要讲，字面意思代表一切； meta interface{} // 和predicate里的meta不太一样，下面会贴个debug的图先，具体后面再看； priorityConfigs []algorithm.PriorityConfig // 包含优选算法各种信息，比较重要； nodes []*v1.Node // node集合，不需要解释了； extenders []algorithm.SchedulerExtender // extender逻辑放到后面单独讲。 meta实参长这个样子： 返回值只需要看一下schedulerapi.HostPriorityList类型的含义了，这个类型之前也提过，后面频繁涉及到操作这个结构，所以这里再贴一次，大伙得烂熟于心才行！ pkg/scheduler/api/types.go:305 type HostPriority struct { Host string Score int } type HostPriorityList []HostPriority 着重分析一下这2个type，虽然很简单，还是有必要啰嗦一下，必须记在心里。HostPriority这个struct的属性是Host和Score，一个是string一个是int，所以很明显HostPriority所能够保存的信息是一个节点的名字和分值，再仔细一点说就是这个结构保存的是一个node在一个priority算法计算后所得到的结果；然后看HostPriorityList类型，这个类型是上一个类型的“集合”，集合表达的是一个node多个算法还是多个node一个算法呢？稍微思考一下可以知道HostPriorityList中存的是多个Host和Score的组合，所以HostPriorityList这个结构是要保存一个算法作用于所有node之后，得到的所有node的Score信息的。（这里我们先理解成一个算法的结果，作为函数返回值这里肯定是要保留所有算法作用后的最终node的Score，所以函数后半部分肯定有combine分值的步骤。） 2. PrioritizeNodes整体流程 前面说到PrioritizeNodes()函数也就是node优选的具体逻辑，这个函数略长，我们分段讲解。 2.1. Results PrioritizeNodes()函数开头的逻辑很简单，我们先从第一行看到results定义的这一行。 pkg/scheduler/core/generic_scheduler.go:634 if len(priorityConfigs) == 0 && len(extenders) == 0 { // 这个if很明显是处理特殊场景的，就是优选算法一个都没有配置（extenders同样没有配置）的时候怎么做； // 这个result是要当作返回值的，HostPriorityList类型前面唠叨了很多了，大家得心里有数； result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { // 这一行代码是唯一的“逻辑了”，下面直到for结束都是简单代码；所以我们看一下EqualPriorityMap // 函数的作用就行了。这里我不贴代码，这个函数很短，作用就是设置每个node的Score相同（都为1） // hostPriority的类型也就是schedulerapi.HostPriority类型，再次强调这个类型是要烂熟于心的； hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } // 最终的result也就是设置了每个node的Score为1的schedulerapi.HostPriorityList类型数据； result = append(result, hostPriority) } return result, nil } // 这里只是简单定义3个变量，一把锁，一个并发等待相关的wg，一个错误集合errs； var ( mu = sync.Mutex{} wg = sync.WaitGroup{} errs []error ) // 这里定义了一个appendError小函数，逻辑很简单，并发场景下将错误信息收集到errs中； appendError := func(err error) { mu.Lock() defer mu.Unlock() errs = append(errs, err) } // 最后一个变量results也不难理解，类型是[]schedulerapi.HostPriorityList，这里需要注意这个类型 // 的作用，它保存的是所有算法作用所有node之后得到的结果集，相当于一个二维数组，每个格子是1个算法 // 作用于1个节点的结果，一行也就是1个算法作用于所有节点的结果；一行展成一个二维就是所有算法作用于所有节点； results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs)) 到这里要求大家心中能够想象上面提到的results是什么样的，可以借助纸笔画一画。下面的代码会往这个二维结构里面存储数据。 2.2. Old Priority Function 我们既然讲到“老式”，后面肯定有对应的“新式”。虽然这种函数已经DEPRECATED了，不过对于我们学习掌握优选流程还是很有帮助的。默认的优选算法里其实也只有1个是这在old形式的了： 贴这块代码之前我们先关注一下多次出现的priorityConfigs这个变量的类型： 函数形参中有写到：priorityConfigs []algorithm.PriorityConfig，所以我们直接看PriorityConfig是什么类型： pkg/scheduler/algorithm/types.go:62 // PriorityConfig is a config used for a priority function. type PriorityConfig struct { Name string Map PriorityMapFunction Reduce PriorityReduceFunction // TODO: Remove it after migrating all functions to // Map-Reduce pattern. Function PriorityFunction Weight int } PriorityConfig中有一个Name，一个Weight，很好猜到意思，名字和权重嘛。剩下的Map、Reduce和Function目测代表的就是优选函数的新旧两种表达方式了。我们先看旧的Function属性的类型PriorityFunction是什么： pkg/scheduler/algorithm/types.go:59 type PriorityFunction func(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) 很明显这个类型代表了一个priority函数，入参是pod、nodeNameToInfo和nodes，返回值是HostPriorityList，也就是我们前面提到的1个priority函数作用于每个node后得到了Score信息，存结果的结构就是这个HostPriorityList；看起来很和谐～ 然后讲回PrioritizeNodes过程： pkg/scheduler/core/generic_scheduler.go:661 for i := range priorityConfigs { // 如果第i个优选配置(priorityConfig)定义了老函数，则调用之； if priorityConfigs[i].Function != nil { wg.Add(1) // 注意这里的参数index，这里传入的实参是上面的i； go func(index int) { defer wg.Done() var err error // 所以这里的results[index]就好理解了；后面priorityConfigs[index]的索引也是index， // 这里表达的是第N个优选配置里有Function，那么这个Function的计算结果保存在 // results的第N个格子里； results[index], err = priorityConfigs[index].Function(pod, nodeNameToInfo, nodes) if err != nil { appendError(err) } }(i) } else { // 如果没有定义Function，其实也就是使用了Map-Reduce方式的，这里先存个空的结构占位； results[i] = make(schedulerapi.HostPriorityList, len(nodes)) } } 上面这段代码逻辑还算好理解，唯一有点小绕的还是前面强调的HostPriorityList相关类型的操作上。 2.3. Map-Reduce 关于map-reduce思想我就不在这里赘述了，大数据行业很流行的一个词汇，百度一下（如果你能够google，自然更好咯）可以找到一大堆介绍的文章。 简单说map-reduce就是：Map是映射，Reduce是归约；map是统计一本书中的一页出现了多少次k8s这个词，reduce是将这些map结果汇总在一起得到最终结果。（map一般都是将一个算法作用于一堆数据集的每一个元素，得到一个结果集，reduce有各种形式，可以是累加这些结果，或者是对这个结果集做其他复杂的f(x)操作。 看看在Scheduler里面是怎么用Map-Reduce的吧： // 这个并发逻辑之前介绍过了，我们直接看ParallelizeUntil的最后一个参数就行，这里直接写了一个匿名函数； workqueue.ParallelizeUntil(context.TODO(), 16, len(nodes), func(index int) { // 这里的index是[0，len(nodes)-1]，相当于遍历所有的nodes； nodeInfo := nodeNameToInfo[nodes[index].Name] // 这个for循环遍历的是所有的优选配置，如果有老Fun就跳过，新逻辑就继续； for i := range priorityConfigs { if priorityConfigs[i].Function != nil { // 因为前面old已经运行过了 continue } var err error // 这里的i和前面老Fun的互补，老Fun中没有赋值的results中的元素就在这里赋值了； // 注意到这里调用了一个Map函数就直接赋值给了results[i][index]，这里的index是第一行这个 // 匿名函数的形参，通过ParallelizeUntil这个并发实现所有node对应一个优选算法的分值计算； results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) results[i][index].Host = nodes[index].Name } } }) for i := range priorityConfigs { // 没有定义Reduce函数就不处理； if priorityConfigs[i].Reduce == nil { continue } wg.Add(1) go func(index int) { defer wg.Done() // 调用Reduce函数 if err := priorityConfigs[index].Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if klog.V(10) { for _, hostPriority := range results[index] { klog.Infof(\"%v -> %v: %v, Score: (%d)\", util.GetPodFullName(pod), hostPriority.Host, priorityConfigs[index].Name, hostPriority.Score) } } }(i) } // Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } 看到这里我们可以发现老Fun和Map的区别不大，都是优选函数的执行过程。那为什么会存在两种形式呢？我们看完PrioritizeNodes整体流程后通过具体的Fun和Map-Reduce实现来看二者的区别。 2.4. Combine Scores 这块的代码很简单，我们先抛开extender的逻辑，剩下的代码如下： // Summarize all scores. // 这个result和前面的results类似，result用于存储每个node的Score，到这里已经没有必要区分算法了； result := make(schedulerapi.HostPriorityList, 0, len(nodes)) // 循环执行len(nodes)次 for i := range nodes { // 先在result中塞满所有node的Name，Score初始化为0； result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0}) // 执行了多少个priorityConfig就有多少个Score，所以这里遍历len(priorityConfigs)次； for j := range priorityConfigs { // 每个算法对应第i个node的结果分值加权后累加； result[i].Score += results[j][i].Score * priorityConfigs[j].Weight } } return result, nil 这块逻辑很清晰，要将前面得到的二维结果results压缩成一维的加权分值集合result，最终返回这个result. 从这里我们还可以得到一个结论，不管是Fun还是Map-Reduce，处理的结果都是填充results这个二维结构，所以Map-Reduce也没有什么神秘的，下面通过具体的算法来看二者有何异同。 3. Fun和Map-Reduce实例分析 3.1. InterPodAffinityPriority(Function) 这个算法做的是Pod间亲和性优选，也就是亲和pod越多的节点分值越高，反亲和pod越多分值越低。 我们撇开具体的亲和性计算规则，从优选函数的形式上看一下这段代码的逻辑： pkg/scheduler/algorithm/priorities/interpod_affinity.go:119 func (ipa *InterPodAffinity) CalculateInterPodAffinityPriority(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) { affinity := pod.Spec.Affinity // 是否有亲和性约束； hasAffinityConstraints := affinity != nil && affinity.PodAffinity != nil // 是否有反亲和性约束； hasAntiAffinityConstraints := affinity != nil && affinity.PodAntiAffinity != nil // 这里有一段根据亲和性和反亲和性来计算一个node上匹配的pod数量的逻辑，我们先跳过这些逻辑，从优选算法实现的角度看这个算法的架子； // 当遍历完所有的node之后，可以得到1个最高分和1个最低分，分别记为maxCount和minCount； for _, node := range nodes { if pm.counts[node.Name] > maxCount { maxCount = pm.counts[node.Name] } if pm.counts[node.Name] 0 { // MaxPriority定义的是优选最高分10，第二个因数是当前node的count-最小count， // 然后除以(maxCount - minCount)；举个例子，当前node的计算结果是5，最大count是20，最小 // count是-3，那么这里就是10*[5-(-3)/20-(-3)] // 这个计算的结果显然会在[0-10]之间； fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) } // 如果分差不大于0，这时候int(fScore)也就是0，对于各个node的结果都是0； result = append(result, schedulerapi.HostPriority{Host: node.Name, Score: int(fScore)}) } return result, nil } 如上，我们可以发现最终这个函数计算出了每个node的分值，这个分值在[0-10]之间。所以说到底Function做的事情就是根据一定的规则给每个node赋一个分值，这个分值要求在[0-10]之间，然后把这个HostPriorityList返回就行。 3.2. CalculateNodeAffinityPriorityMap(Map) 这个算法和上一个类似，上一个是Pod的Affinity，这个是Node的Affinity；我们来看代码： pkg/scheduler/algorithm/priorities/node_affinity.go:34 func CalculateNodeAffinityPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } // default is the podspec. affinity := pod.Spec.Affinity if priorityMeta, ok := meta.(*priorityMetadata); ok { // We were able to parse metadata, use affinity from there. affinity = priorityMeta.affinity } var count int32 if affinity != nil && affinity.NodeAffinity != nil && affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution != nil { // Match PreferredDuringSchedulingIgnoredDuringExecution term by term. for i := range affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution { preferredSchedulingTerm := &affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution[i] if preferredSchedulingTerm.Weight == 0 { continue } nodeSelector, err := v1helper.NodeSelectorRequirementsAsSelector(preferredSchedulingTerm.Preference.MatchExpressions) if err != nil { return schedulerapi.HostPriority{}, err } if nodeSelector.Matches(labels.Set(node.Labels)) { count += preferredSchedulingTerm.Weight } } } return schedulerapi.HostPriority{ Host: node.Name, Score: int(count), }, nil } 撇开具体的亲和性计算细节，我们可以发现这个的count没有特定的规则，可能会加到10以上；另外这里的返回值是HostPriority类型，前面的Function返回了HostPriorityList类型。 map函数 pkg/scheduler/algorithm/priorities/selector_spreading.go:221 func (s *ServiceAntiAffinity) CalculateAntiAffinityPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { var firstServiceSelector labels.Selector node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } priorityMeta, ok := meta.(*priorityMetadata) if ok { firstServiceSelector = priorityMeta.podFirstServiceSelector } else { firstServiceSelector = getFirstServiceSelector(pod, s.serviceLister) } // 查找给定node在给定namespace下符合selector的pod，返回值是[]*v1.Pod matchedPodsOfNode := filteredPod(pod.Namespace, firstServiceSelector, nodeInfo) return schedulerapi.HostPriority{ Host: node.Name, // 返回值中Score设置成上面找到的pod的数量 Score: int(len(matchedPodsOfNode)), }, nil } 这个函数比较短，可以看到在指定node上查询到匹配selector的pod越多，分值就越高。假设找到了20个，那么这里的分值就是20；假设找到的是2，那这里的分值就是2. 3.3. CalculateNodeAffinityPriorityReduce(Reduce) 和上面这个Map对应的Reduce函数其实没有单独实现，通过NormalizeReduce函数做了一个通用的Reduce处理： pkg/scheduler/algorithm/priorities/node_affinity.go:77 var CalculateNodeAffinityPriorityReduce = NormalizeReduce(schedulerapi.MaxPriority, false) pkg/scheduler/algorithm/priorities/reduce.go:29 func NormalizeReduce(maxPriority int, reverse bool) algorithm.PriorityReduceFunction { return func( _ *v1.Pod, _ interface{}, _ map[string]*schedulercache.NodeInfo, // 注意到这个result是HostPriorityList，对应1个算法N个node的结果集 result schedulerapi.HostPriorityList) error { var maxCount int // 遍历result将最高的Score赋值给maxCount； for i := range result { if result[i].Score > maxCount { maxCount = result[i].Score } } if maxCount == 0 { if reverse { for i := range result { result[i].Score = maxPriority } } return nil } for i := range result { score := result[i].Score // 举个例子：10*(5/20) score = maxPriority * score / maxCount if reverse { // 如果score是3，得到7；如果score是4，得到6，结果反转； score = maxPriority - score } result[i].Score = score } return nil } } 3.4. 小结 Function：一个算法一次性计算出所有node的Score，这个Score的范围是规定的[0-10]； Map-Reduce：一个Map算法计算1个node的Score，这个Score可以灵活处理，可能是20，可能是-3；Map过程并发进行；最终得到的结果result通过Reduce归约，将这个算法对应的所有node的分值归约为[0-10]； 本节有几张图是goland debug的截图，我们目前还没有提到如何debug；不过本节内容的阅读基本是不影响的。下一节源码分析内容发出来前我会在“环境准备”这一章中增加如何开始debug的内容，大家可以选择开始debug的时机。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-19 20:21:31 优选过程1. 走近priority过程2. PrioritizeNodes整体流程2.1. Results2.2. Old Priority Function2.3. Map-Reduce2.4. Combine Scores3. Fun和Map-Reduce实例分析3.1. InterPodAffinityPriority(Function)3.2. CalculateNodeAffinityPriorityMap(Map)3.3. CalculateNodeAffinityPriorityReduce(Reduce)3.4. 小结"},"core/scheduler/preempt.html":{"url":"core/scheduler/preempt.html","title":"抢占调度","keywords":"","body":"抢占调度 Pod priority preempt 入口 preempt 实现 SchedulingQueue FIFO PriorityQueue PodPreemptor xx.Algorithm.Preempt 接口定义 整体流程 podEligibleToPreemptOthers nodesWherePreemptionMightHelp selectNodesForPreemption pickOneNodeForPreemption 小结 1. Pod priority Pod 有了 priority(优先级) 后才有优先级调度、抢占调度的说法，高优先级的 pod 可以在调度队列中排到前面，优先选择 node；另外当高优先级的 pod 找不到合适的 node 时，就会看 node 上低优先级的 pod 驱逐之后是否能够 run 起来，如果可以，那么 node 上的一个或多个低优先级的 pod 会被驱逐，然后高优先级的 pod 得以成功运行1个 node 上。 今天我们分析 pod 抢占相关的代码。开始之前我们看一下和 priority 相关的2个示例配置文件： PriorityClass 例子 apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false description: \"This priority class should be used for XYZ service pods only.\" 使用上述 PriorityClass apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent priorityClassName: high-priority 这两个文件的内容这里不解释，Pod priority 相关知识点不熟悉的小伙伴请先查阅官方文档，我们下面看调度器中和 preempt 相关的代码逻辑。 2. preempt 入口 在pkg/scheduler/scheduler.go:513 scheduleOne()方法中我们上一次关注的是suggestedHost, err := sched.schedule(pod)这行代码，也就是关注通常情况下调度器如何给一个 pod 匹配一个最合适的 node. 今天我们来看如果这一行代码返回的 err != nil 情况下，如何开始 preempt 过程。 pkg/scheduler/scheduler.go:529 suggestedHost, err := sched.schedule(pod) if err != nil { if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() } else { klog.Errorf(\"error selecting node for pod: %v\", err) metrics.PodScheduleErrors.Inc() } return } 当schedule()函数没有返回 host，也就是没有找到合适的 node 的时候，就会出发 preempt 过程。这时候代码逻辑进入sched.preempt(pod, fitError)这一行。我们先看一下这个函数的整体逻辑，然后深入其中涉及的子过程： pkg/scheduler/scheduler.go:311 func (sched *Scheduler) preempt(preemptor *v1.Pod, scheduleErr error) (string, error) { // 特性没有开启就返回 \"\" if !util.PodPriorityEnabled() || sched.config.DisablePreemption { return \"\", nil } // 更新 pod 信息；入参和返回值都是 *v1.Pod 类型 preemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) // preempt 过程，下文分析 node, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) var nodeName = \"\" if node != nil { nodeName = node.Name // 更新队列中“任命pod”队列 sched.config.SchedulingQueue.UpdateNominatedPodForNode(preemptor, nodeName) // 设置pod的Status.NominatedNodeName err = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) if err != nil { // 如果出错就从 queue 中移除 sched.config.SchedulingQueue.DeleteNominatedPodIfExists(preemptor) return \"\", err } for _, victim := range victims { // 将要驱逐的 pod 驱逐 if err := sched.config.PodPreemptor.DeletePod(victim); err != nil { return \"\", err } sched.config.Recorder.Eventf(victim, v1.EventTypeNormal, \"Preempted\", \"by %v/%v on node %v\", preemptor.Namespace, preemptor.Name, nodeName) } } // Clearing nominated pods should happen outside of \"if node != nil\". // 这个清理过程在上面的if外部，我们回头从 Preempt() 的实现去理解 for _, p := range nominatedPodsToClear { rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil { klog.Errorf(\"Cannot remove nominated node annotation of pod: %v\", rErr) // We do not return as this error is not critical. } } return nodeName, err } 3. preempt 实现 上面 preempt() 函数中涉及到了一些值得深入看看的对象，下面我们逐个看一下这些对象的实现。 3.1. SchedulingQueue SchedulingQueue 表示的是一个存储待调度 pod 的队列 pkg/scheduler/internal/queue/scheduling_queue.go:60 type SchedulingQueue interface { Add(pod *v1.Pod) error AddIfNotPresent(pod *v1.Pod) error AddUnschedulableIfNotPresent(pod *v1.Pod) error Pop() (*v1.Pod, error) Update(oldPod, newPod *v1.Pod) error Delete(pod *v1.Pod) error MoveAllToActiveQueue() AssignedPodAdded(pod *v1.Pod) AssignedPodUpdated(pod *v1.Pod) NominatedPodsForNode(nodeName string) []*v1.Pod WaitingPods() []*v1.Pod Close() UpdateNominatedPodForNode(pod *v1.Pod, nodeName string) DeleteNominatedPodIfExists(pod *v1.Pod) NumUnschedulablePods() int } 在 Scheduler 中 SchedulingQueue 接口对应两种实现： FIFO 先进先出队列 PriorityQueue 优先级队列 3.1.1. FIFO FIFO 结构是对 cache.FIFO 的简单包装，然后实现了 SchedulingQueue 接口。 pkg/scheduler/internal/queue/scheduling_queue.go:97 type FIFO struct { *cache.FIFO } cache.FIFO定义在vendor/k8s.io/client-go/tools/cache/fifo.go:93，这个先进先出队列的细节先不讨论。 3.1.2. PriorityQueue PriorityQueue 同样实现了 SchedulingQueue 接口，PriorityQueue 的顶是最高优先级的 pending pod. 这里的PriorityQueue 有2个子 queue，activeQ 放的是等待调度的 pod，unschedulableQ 放的是已经尝试过调度，然后失败了，被标记为 unschedulable 的 pod. 我们看一下 PriorityQueue 结构的定义： pkg/scheduler/internal/queue/scheduling_queue.go:201 type PriorityQueue struct { stop PriorityQueue 的方法比较好理解，我们看几个吧： 1、func (p *PriorityQueue) Add(pod *v1.Pod) error //在 active queue 中添加1个pod pkg/scheduler/internal/queue/scheduling_queue.go:276 func (p *PriorityQueue) Add(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() // 直接在 activeQ 中添加 pod err := p.activeQ.Add(pod) if err != nil { klog.Errorf(\"Error adding pod %v/%v to the scheduling queue: %v\", pod.Namespace, pod.Name, err) } else { // 如果在 unschedulableQ 中找到这个 pod，抛错误日志后移除队列中该 pod if p.unschedulableQ.get(pod) != nil { klog.Errorf(\"Error: pod %v/%v is already in the unschedulable queue.\", pod.Namespace, pod.Name) p.unschedulableQ.delete(pod) } // 队列的 nominatedPods 属性中标记该 pod 不指定到任何 node p.nominatedPods.add(pod, \"\") p.cond.Broadcast() } return err } 2、func (p *PriorityQueue) AddIfNotPresent(pod *v1.Pod) error//如果2个队列中都不存在该 pod，那么就添加到 active queue 中 pkg/scheduler/internal/queue/scheduling_queue.go:295 func (p *PriorityQueue) AddIfNotPresent(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() //如果队列 unschedulableQ 中有 pod，啥也不做 if p.unschedulableQ.get(pod) != nil { return nil } //如果队列 activeQ 中有 pod，啥也不做 if _, exists, _ := p.activeQ.Get(pod); exists { return nil } // 添加 pod 到 activeQ err := p.activeQ.Add(pod) if err != nil { klog.Errorf(\"Error adding pod %v/%v to the scheduling queue: %v\", pod.Namespace, pod.Name, err) } else { p.nominatedPods.add(pod, \"\") p.cond.Broadcast() } return err } 3、func (p *PriorityQueue) flushUnschedulableQLeftover()//刷新 unschedulableQ 中的 pod，如果一个 pod 的呆的时间超过了 durationStayUnschedulableQ，就移动到 activeQ 中 pkg/scheduler/internal/queue/scheduling_queue.go:346 func (p *PriorityQueue) flushUnschedulableQLeftover() { p.lock.Lock() defer p.lock.Unlock() var podsToMove []*v1.Pod currentTime := p.clock.Now() // 遍历 unschedulableQ 中的 pod for _, pod := range p.unschedulableQ.pods { lastScheduleTime := podTimestamp(pod) // 这里的默认值是 60s，所以超过 60s 的 pod 将得到进入 activeQ 的机会 if !lastScheduleTime.IsZero() && currentTime.Sub(lastScheduleTime.Time) > unschedulableQTimeInterval { podsToMove = append(podsToMove, pod) } } if len(podsToMove) > 0 { // 全部移到 activeQ 中，又有机会被调度了 p.movePodsToActiveQueue(podsToMove) } } 4、func (p *PriorityQueue) Pop() (*v1.Pod, error)//从 activeQ 中 pop 一个 pod pkg/scheduler/internal/queue/scheduling_queue.go:367 func (p *PriorityQueue) Pop() (*v1.Pod, error) { p.lock.Lock() defer p.lock.Unlock() for len(p.activeQ.data.queue) == 0 { // 当队列为空的时候会阻塞 if p.closed { return nil, fmt.Errorf(queueClosed) } p.cond.Wait() } obj, err := p.activeQ.Pop() if err != nil { return nil, err } pod := obj.(*v1.Pod) // 标记 receivedMoveRequest 为 false，表示新的一次调度开始了 p.receivedMoveRequest = false return pod, err } 再看个别 PriorityQueue.nominatedPods 属性相关操作的方法，也就是 preempt() 函数中多次调用到的方法： 5、`func (p PriorityQueue) UpdateNominatedPodForNode(pod v1.Pod, nodeName string)`//pod 抢占的时候，确定一个 node 可以用于跑这个 pod 时，通过调用这个方法将 pod nominated 到 指定的 node 上。 pkg/scheduler/internal/queue/scheduling_queue.go:567 func (p *PriorityQueue) UpdateNominatedPodForNode(pod *v1.Pod, nodeName string) { p.lock.Lock() //逻辑在这里面 p.nominatedPods.add(pod, nodeName) p.lock.Unlock() } 先看 nominatedPods 属性的类型，这个类型用于存储 pods 被 nominate 到 nodes 的信息： pkg/scheduler/internal/queue/scheduling_queue.go:822 type nominatedPodMap struct { // key 是 node name，value 是 nominated 到这个 node 上的 pods nominatedPods map[string][]*v1.Pod // 和上面结构相反，key 是 pod 信息，值是 node 信息 nominatedPodToNode map[ktypes.UID]string } 在看一下add()方法的实现： pkg/scheduler/internal/queue/scheduling_queue.go:832 func (npm *nominatedPodMap) add(p *v1.Pod, nodeName string) { // 不管有没有，先删一下，防止重了 npm.delete(p) nnn := nodeName // 如果传入的 nodeName 是 “” if len(nnn) == 0 { // 查询 pod 的 pod.Status.NominatedNodeName nnn = NominatedNodeName(p) // 如果 pod.Status.NominatedNodeName 也是 “”,return if len(nnn) == 0 { return } } // 逻辑到这里说明要么 nodeName 不为空字符串，要么 nodeName 为空字符串但是 pod 的 pod.Status.NominatedNodeName 不为空字符串，这时候开始下面的赋值 npm.nominatedPodToNode[p.UID] = nnn for _, np := range npm.nominatedPods[nnn] { if np.UID == p.UID { klog.V(4).Infof(\"Pod %v/%v already exists in the nominated map!\", p.Namespace, p.Name) return } } npm.nominatedPods[nnn] = append(npm.nominatedPods[nnn], p) } 3.2. PodPreemptor PodPreemptor 用来驱逐 pods 和更新 pod annotations. pkg/scheduler/factory/factory.go:145 type PodPreemptor interface { GetUpdatedPod(pod *v1.Pod) (*v1.Pod, error) DeletePod(pod *v1.Pod) error SetNominatedNodeName(pod *v1.Pod, nominatedNode string) error RemoveNominatedNodeName(pod *v1.Pod) error } 这个 interface 对应的实现类型是： pkg/scheduler/factory/factory.go:1620 type podPreemptor struct { Client clientset.Interface } 这个类型绑定了4个方法： pkg/scheduler/factory/factory.go:1624 // 新获取一次 pod 的信息 func (p *podPreemptor) GetUpdatedPod(pod *v1.Pod) (*v1.Pod, error) { return p.Client.CoreV1().Pods(pod.Namespace).Get(pod.Name, metav1.GetOptions{}) } // 删除一个 pod func (p *podPreemptor) DeletePod(pod *v1.Pod) error { return p.Client.CoreV1().Pods(pod.Namespace).Delete(pod.Name, &metav1.DeleteOptions{}) } // 设置pod.Status.NominatedNodeName 为指定的 node name func (p *podPreemptor) SetNominatedNodeName(pod *v1.Pod, nominatedNodeName string) error { podCopy := pod.DeepCopy() podCopy.Status.NominatedNodeName = nominatedNodeName _, err := p.Client.CoreV1().Pods(pod.Namespace).UpdateStatus(podCopy) return err } // 清空 pod.Status.NominatedNodeName func (p *podPreemptor) RemoveNominatedNodeName(pod *v1.Pod) error { if len(pod.Status.NominatedNodeName) == 0 { return nil } return p.SetNominatedNodeName(pod, \"\") } 3.3. xx.Algorithm.Preempt 3.3.1. 接口定义 我们回到挺久之前讲常规调度过程的时候提过的一个接口： pkg/scheduler/algorithm/scheduler_interface.go:78 type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) // Preempt 在 pod 调度发生失败的时候尝试抢占低优先级的 pod. // 返回发生 preemption 的 node, 被 preempt的 pods 列表, // nominated node name 需要被移除的 pods 列表，一个 error 信息. Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) Predicates() map[string]FitPredicate Prioritizers() []PriorityConfig } 这个接口上次我们讲到的时候关注了Schedule()、Predicates()和Prioritizers()，这次来看Preempt()是怎么实现的。 3.3.2. 整体流程 Preempt()同样由genericScheduler类型(pkg/scheduler/core/generic_scheduler.go:98)实现，方法前的一大串英文注释先来理解一下： Preempt 寻找一个在发生抢占之后能够成功调度“pod”的node. Preempt 选择一个 node 然后抢占上面的 pods 资源，返回： 这个 node 信息 被抢占的 pods 信息 nominated node name 需要被清理的 node 列表 可能有的 error Preempt 过程不涉及快照更新（快照的逻辑以后再讲） 避免出现这种情况：preempt 发现一个不需要驱逐任何 pods 就能够跑“pod”的 node. 当有很多 pending pods 在调度队列中的时候，a nominated pod 会排到队列中相同优先级的 pod 后面. The nominated pod 会阻止其他 pods 使用“指定”的资源，哪怕花费了很多时间来等待其他 pending 的 pod. 我们先过整体流程，然后逐个分析子流程调用： pkg/scheduler/core/generic_scheduler.go:251 func (g *genericScheduler) Preempt(pod *v1.Pod, nodeLister algorithm.NodeLister, scheduleErr error) (*v1.Node, []*v1.Pod, []*v1.Pod, error) { // 省略几行 // 判断执行驱逐操作是否合适 if !podEligibleToPreemptOthers(pod, g.cachedNodeInfoMap) { klog.V(5).Infof(\"Pod %v/%v is not eligible for more preemption.\", pod.Namespace, pod.Name) return nil, nil, nil, nil } // 所有的 nodes allNodes, err := nodeLister.List() if err != nil { return nil, nil, nil, err } if len(allNodes) == 0 { return nil, nil, nil, ErrNoNodesAvailable } // 计算潜在的执行驱逐后能够用于跑 pod 的 nodes potentialNodes := nodesWherePreemptionMightHelp(allNodes, fitError.FailedPredicates) if len(potentialNodes) == 0 { klog.V(3).Infof(\"Preemption will not help schedule pod %v/%v on any node.\", pod.Namespace, pod.Name) // In this case, we should clean-up any existing nominated node name of the pod. return nil, nil, []*v1.Pod{pod}, nil } // 列出 pdb 对象 pdbs, err := g.pdbLister.List(labels.Everything()) if err != nil { return nil, nil, nil, err } // 计算所有 node 需要驱逐的 pods 有哪些等，后面细讲 nodeToVictims, err := selectNodesForPreemption(pod, g.cachedNodeInfoMap, potentialNodes, g.predicates, g.predicateMetaProducer, g.schedulingQueue, pdbs) if err != nil { return nil, nil, nil, err } // 拓展调度的逻辑 nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil { return nil, nil, nil, err } // 选择1个 node 用于 schedule candidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil { return nil, nil, nil, err } // 低优先级的被 nominate 到这个 node 的 pod 很可能已经不再 fit 这个 node 了，所以 // 需要移除这些 pod 的 nomination，更新这些 pod，挪动到 activeQ 中，让调度器 // 得以寻找另外一个 node 给这些 pod nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.cachedNodeInfoMap[candidateNode.Name]; ok { return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, err } return nil, nil, nil, fmt.Errorf( \"preemption failed: the target node %s has been deleted from scheduler cache\", candidateNode.Name) } 上面涉及到一些子过程调用，我们逐个来看～ podEligibleToPreemptOthers() // 如何判断是否适合抢占？ nodesWherePreemptionMightHelp() // 怎么寻找能够用于 preempt 的 nodes？ selectNodesForPreemption() // 这个过程计算的是什么？ pickOneNodeForPreemption() // 怎么从选择最合适被抢占的 node？ 3.3.3. podEligibleToPreemptOthers podEligibleToPreemptOthers 做的事情是判断一个 pod 是否应该去抢占其他 pods. 如果这个 pod 已经抢占过其他 pods，那些 pods 还在 graceful termination period 中，那就不应该再次发生抢占。 如果一个 node 已经被这个 pod nominated，并且这个 node 上有处于 terminating 状态的 pods，那么就不考虑驱逐更多的 pods. 这个函数逻辑很简单，我们直接看源码： pkg/scheduler/core/generic_scheduler.go:1110 func podEligibleToPreemptOthers(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo) bool { nomNodeName := pod.Status.NominatedNodeName // 如果 pod.Status.NominatedNodeName 不是空字符串 if len(nomNodeName) > 0 { // 被 nominate 的 node if nodeInfo, found := nodeNameToInfo[nomNodeName]; found { for _, p := range nodeInfo.Pods() { // 有低优先级的 pod 处于删除中状态，就返回 false if p.DeletionTimestamp != nil && util.GetPodPriority(p) 3.3.4. nodesWherePreemptionMightHelp nodesWherePreemptionMightHelp 要做的事情是寻找 predicates 阶段失败但是通过抢占也许能够调度成功的 nodes. 这个函数也不怎么长，看下代码： pkg/scheduler/core/generic_scheduler.go:1060 func nodesWherePreemptionMightHelp(nodes []*v1.Node, failedPredicatesMap FailedPredicateMap) []*v1.Node { // 潜力 node， 用于存储返回值的 slice potentialNodes := []*v1.Node{} for _, node := range nodes { // 这个为 true 表示一个 node 驱逐 pod 也不一定能适合当前 pod 运行 unresolvableReasonExist := false // 一个 node 对应的所有失败的 predicates failedPredicates, _ := failedPredicatesMap[node.Name] // 遍历，看是不是再下面指定的这些原因中，如果在，就标记 unresolvableReasonExist = true for _, failedPredicate := range failedPredicates { switch failedPredicate { case predicates.ErrNodeSelectorNotMatch, predicates.ErrPodAffinityRulesNotMatch, predicates.ErrPodNotMatchHostName, predicates.ErrTaintsTolerationsNotMatch, predicates.ErrNodeLabelPresenceViolated, predicates.ErrNodeNotReady, predicates.ErrNodeNetworkUnavailable, predicates.ErrNodeUnderDiskPressure, predicates.ErrNodeUnderPIDPressure, predicates.ErrNodeUnderMemoryPressure, predicates.ErrNodeOutOfDisk, predicates.ErrNodeUnschedulable, predicates.ErrNodeUnknownCondition, predicates.ErrVolumeZoneConflict, predicates.ErrVolumeNodeConflict, predicates.ErrVolumeBindConflict: unresolvableReasonExist = true // 如果找到一个上述失败原因，说明这个 node 已经可以排除了，break 后继续下一个 node 的计算 break } } // false 的时候，也就是这个 node 也许驱逐 pods 后有用，那就添加到 potentialNodes 中 if !unresolvableReasonExist { klog.V(3).Infof(\"Node %v is a potential node for preemption.\", node.Name) potentialNodes = append(potentialNodes, node) } } return potentialNodes } 3.3.5. selectNodesForPreemption 这个函数会并发计算所有的 nodes 是否通过驱逐实现 pod 抢占。 看这个函数内容之前我们先看一下返回值的类型： map[*v1.Node]*schedulerapi.Victims 的 key 很好理解，value 是啥呢： type Victims struct { Pods []*v1.Pod NumPDBViolations int } 这里的 Pods 是被选中准备要驱逐的；NumPDBViolations 表示的是要破坏多少个 PDB 限制。这里肯定也就是要尽量符合 PDB 要求，能不和 PDB 冲突就不冲突。 然后看一下这个函数的整体过程： pkg/scheduler/core/generic_scheduler.go:895 func selectNodesForPreemption(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, potentialNodes []*v1.Node, // 上一个函数计算出来的 nodes predicates map[string]algorithm.FitPredicate, metadataProducer algorithm.PredicateMetadataProducer, queue internalqueue.SchedulingQueue, // 这里其实是前面讲的优先级队列 PriorityQueue pdbs []*policy.PodDisruptionBudget, // pdb 列表 ) (map[*v1.Node]*schedulerapi.Victims, error) { nodeToVictims := map[*v1.Node]*schedulerapi.Victims{} var resultLock sync.Mutex // We can use the same metadata producer for all nodes. meta := metadataProducer(pod, nodeNameToInfo) // 这种形式的并发已经不陌生了，前面遇到过几次了 checkNode := func(i int) { nodeName := potentialNodes[i].Name var metaCopy algorithm.PredicateMetadata if meta != nil { metaCopy = meta.ShallowCopy() } // 这里有一个子过程调用，下面单独介绍 pods, numPDBViolations, fits := selectVictimsOnNode(pod, metaCopy, nodeNameToInfo[nodeName], predicates, queue, pdbs) if fits { resultLock.Lock() victims := schedulerapi.Victims{ Pods: pods, NumPDBViolations: numPDBViolations, } // 如果 fit，就添加到 nodeToVictims 中，也就是最后的返回值 nodeToVictims[potentialNodes[i]] = &victims resultLock.Unlock() } } workqueue.ParallelizeUntil(context.TODO(), 16, len(potentialNodes), checkNode) return nodeToVictims, nil } 上面这个函数的核心逻辑在 selectVictimsOnNode 中，这个函数尝试在给定的 node 中寻找最少数量的需要被驱逐的 pods，同时需要保证驱逐了这些 pods 之后，这个 noode 能够满足“pod”运行需求。 这些被驱逐的 pods 计算同时需要满足一个约束，就是能够删除低优先级的 pod 绝不先删高优先级的 pod. 这个算法首选计算当这个 node 上所有的低优先级 pods 被驱逐之后能否调度“pod”. 如果可以，那就按照优先级排序，根据 PDB 是否破坏分成两组，一组是影响 PDB 限制的，另外一组是不影响 PDB. 两组各自按照优先级排序。然后开始逐渐释放影响 PDB 的 group 中的 pod，然后逐渐释放不影响 PDB 的 group 中的 pod，在这个过程中要保持“pod”能够 fit 这个 node. 也就是说一旦放过某一个 pod 导致“pod”不 fit 这个 node 了，那就说明这个 pod 不能放过，也就是意味着已经找到了最少 pods 集。 看一下具体的实现吧： FILENAME pkg/scheduler/core/generic_scheduler.go:983 func selectVictimsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo, fitPredicates map[string]algorithm.FitPredicate, queue internalqueue.SchedulingQueue, pdbs []*policy.PodDisruptionBudget, ) ([]*v1.Pod, int, bool) { if nodeInfo == nil { return nil, 0, false } // 排个序 potentialVictims := util.SortableList{CompFunc: util.HigherPriorityPod} nodeInfoCopy := nodeInfo.Clone() // 定义删除 pod 函数 removePod := func(rp *v1.Pod) { nodeInfoCopy.RemovePod(rp) if meta != nil { meta.RemovePod(rp) } } // 定义添加 pod 函数 addPod := func(ap *v1.Pod) { nodeInfoCopy.AddPod(ap) if meta != nil { meta.AddPod(ap, nodeInfoCopy) } } // 删除所有的低优先级 pod 看是不是能够满足调度需求了 podPriority := util.GetPodPriority(pod) for _, p := range nodeInfoCopy.Pods() { if util.GetPodPriority(p) 3.3.6. pickOneNodeForPreemption pickOneNodeForPreemption 要从给定的 nodes 中选择一个 node，这个函数假设给定的 map 中 value 部分是以 priority 降序排列的。这里选择 node 的标准是： 最少的 PDB violations 最少的高优先级 victim 优先级总数字最小 victim 总数最小 直接返回第一个 pkg/scheduler/core/generic_scheduler.go:788 func pickOneNodeForPreemption(nodesToVictims map[*v1.Node]*schedulerapi.Victims) *v1.Node { if len(nodesToVictims) == 0 { return nil } // 初始化为最大值 minNumPDBViolatingPods := math.MaxInt32 var minNodes1 []*v1.Node lenNodes1 := 0 // 这个循环要找到 PDBViolatingPods 最少的 node，如果有多个，就全部存在 minNodes1 中 for node, victims := range nodesToVictims { if len(victims.Pods) == 0 { // 如果发现一个不需要驱逐 pod 的 node，马上返回 return node } numPDBViolatingPods := victims.NumPDBViolations if numPDBViolatingPods -6，有2个 pod 的 node 反而被认为总优先级更低！ sumPriorities += int64(util.GetPodPriority(pod)) + int64(math.MaxInt32+1) } if sumPriorities 0 { return minNodes2[0] } klog.Errorf(\"Error in logic of node scoring for preemption. We should never reach here!\") return nil } 4. 小结 咋个说呢，此处应该有总结的，抢占过程的逻辑比我想象中的复杂，设计很巧妙，行云流水，大快人心！preemption 可以简单说成再预选->再优选吧；还是不多说了，一天写这么多有点坐不住了，下回再继续聊调度器～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-29 20:52:37 抢占调度1. Pod priority2. preempt 入口3. preempt 实现3.1. SchedulingQueue3.1.1. FIFO3.1.2. PriorityQueue3.2. PodPreemptor3.3. xx.Algorithm.Preempt3.3.1. 接口定义3.3.2. 整体流程3.3.3. podEligibleToPreemptOthers3.3.4. nodesWherePreemptionMightHelp3.3.5. selectNodesForPreemption3.3.6. pickOneNodeForPreemption4. 小结"},"core/scheduler/init.html":{"url":"core/scheduler/init.html","title":"调度器初始化","keywords":"","body":"调度器初始化 概述 从 --config 开始 options.Option 对象 config.Config对象 runCommand ApplyFeatureGates 默认算法注册 特性开关 Scheduler 的创建 调度算法源 policy / provider 如何生效 默认生效的算法 1. 概述 今天我们要做一些琐碎的知识点分析，比如调度器启动的时候默认配置是怎么来的？默认生效了哪些调度算法？自定义的算法是如何注入的？诸如这些问题，我们顺带会看一下调度器相关的一些数据结构的含义。看完前面这些节的分析后再看完本篇文章你可能会有一种醍醐灌顶的感觉哦～ 2. 从 --config 开始 如果我们编译出来一个 kube-scheduler 二进制文件，运行./kube-scheduler -h后会看到很多的帮助信息，这些信息是分组的，比如第一组 Misc，差不多是“大杂烩”的意思，不好分类的几个 flag，其实也是最重要的几个 flag，如下： 很好理解，第一个红框框圈出来的--config用于指定配置文件，老版本的各种参数基本都不建议使用了，所以这个 config flag 指定的 config 文件中基本包含了所有可配置项，我们看一下代码中获取这个 flag 的相关代码： cmd/kube-scheduler/app/options/options.go:143 func (o *Options) Flags() (nfs apiserverflag.NamedFlagSets) { fs := nfs.FlagSet(\"misc\") // 关注 --config fs.StringVar(&o.ConfigFile, \"config\", o.ConfigFile, \"The path to the configuration file. Flags override values in this file.\") fs.StringVar(&o.WriteConfigTo, \"write-config-to\", o.WriteConfigTo, \"If set, write the configuration values to this file and exit.\") fs.StringVar(&o.Master, \"master\", o.Master, \"The address of the Kubernetes API server (overrides any value in kubeconfig)\") o.SecureServing.AddFlags(nfs.FlagSet(\"secure serving\")) o.CombinedInsecureServing.AddFlags(nfs.FlagSet(\"insecure serving\")) o.Authentication.AddFlags(nfs.FlagSet(\"authentication\")) o.Authorization.AddFlags(nfs.FlagSet(\"authorization\")) o.Deprecated.AddFlags(nfs.FlagSet(\"deprecated\"), &o.ComponentConfig) leaderelectionconfig.BindFlags(&o.ComponentConfig.LeaderElection.LeaderElectionConfiguration, nfs.FlagSet(\"leader election\")) utilfeature.DefaultFeatureGate.AddFlag(nfs.FlagSet(\"feature gate\")) return nfs } 上述代码中有几个点可以关注到： FlagSet 的含义，命令行输出的分组和这里的分组是对应的； 除了认证授权、选举等“非关键”配置外，其他配置基本 Deprecated 了，也就意味着建议使用 config file； 上面代码中可以看到o.ConfigFile接收了config配置，我们看看Option类型是什么样子的~ 2.1. options.Option 对象 Options对象包含运行一个 Scheduler 所需要的所有参数 cmd/kube-scheduler/app/options/options.go:55 type Options struct { // 和命令行帮助信息的分组是一致的 ComponentConfig kubeschedulerconfig.KubeSchedulerConfiguration SecureServing *apiserveroptions.SecureServingOptionsWithLoopback CombinedInsecureServing *CombinedInsecureServingOptions Authentication *apiserveroptions.DelegatingAuthenticationOptions Authorization *apiserveroptions.DelegatingAuthorizationOptions Deprecated *DeprecatedOptions // config 文件的路径 ConfigFile string // 如果指定了，会输出 config 的默认配置到这个文件 WriteConfigTo string Master string } 前面的 flag 相关代码中写到配置文件的内容给了o.ConfigFile，也就是Options.ConfigFile，那这个属性怎么使用呢？ 我们来看下面这个 ApplyTo() 函数，这个函数要做的事情是把 options 配置 apply 给 scheduler app configuration(这个对象后面会讲到)： cmd/kube-scheduler/app/options/options.go:162 // 把 Options apply 给 Config func (o *Options) ApplyTo(c *schedulerappconfig.Config) error { // --config 没有使用的情况 if len(o.ConfigFile) == 0 { c.ComponentConfig = o.ComponentConfig // 使用 Deprecated 的配置 if err := o.Deprecated.ApplyTo(&c.ComponentConfig); err != nil { return err } if err := o.CombinedInsecureServing.ApplyTo(c, &c.ComponentConfig); err != nil { return err } } else { // 加载 config 文件中的内容 cfg, err := loadConfigFromFile(o.ConfigFile) if err != nil { return err } // 上面加载到的配置赋值给 Config中的 ComponentConfig c.ComponentConfig = *cfg if err := o.CombinedInsecureServing.ApplyToFromLoadedConfig(c, &c.ComponentConfig); err != nil { return err } } // …… return nil } 这个函数中可以看到用 --config 和不用 --config 两种情况下 options 是如何应用到schedulerappconfig.Config中的。那么这里提到的 Config 对象又是什么呢？ 2.2. config.Config对象 Config 对象包含运行一个 Scheduler 所需要的所有 context cmd/kube-scheduler/app/config/config.go:32 type Config struct { // 调度器配置对象 ComponentConfig kubeschedulerconfig.KubeSchedulerConfiguration LoopbackClientConfig *restclient.Config InsecureServing *apiserver.DeprecatedInsecureServingInfo InsecureMetricsServing *apiserver.DeprecatedInsecureServingInfo Authentication apiserver.AuthenticationInfo Authorization apiserver.AuthorizationInfo SecureServing *apiserver.SecureServingInfo Client clientset.Interface InformerFactory informers.SharedInformerFactory PodInformer coreinformers.PodInformer EventClient v1core.EventsGetter Recorder record.EventRecorder Broadcaster record.EventBroadcaster LeaderElection *leaderelection.LeaderElectionConfig } 所以前面的c.ComponentConfig = o.ComponentConfig这行代码也就是把 Options 中的 ComponentConfig 赋值给了 Config 中的 ComponentConfig；是哪里的逻辑让 Options 和 Config 对象产生了关联呢？(也就是说前面提到的 ApplyTo() 方法是再哪里被调用的？) 继续跟下去可以找到Config()函数，从这个函数的返回值*schedulerappconfig.Config可以看到它的目的，是需要得到一个 schedulerappconfig.Config，代码不长： cmd/kube-scheduler/app/options/options.go:221 func (o *Options) Config() (*schedulerappconfig.Config, error) { // …… c := &schedulerappconfig.Config{} // 前面我们看到的 ApplyTo() 函数 if err := o.ApplyTo(c); err != nil { return nil, err } // Prepare kube clients. // …… // Prepare event clients. eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, corev1.EventSource{Component: c.ComponentConfig.SchedulerName}) // Set up leader election if enabled. // …… c.Client = client c.InformerFactory = informers.NewSharedInformerFactory(client, 0) c.PodInformer = factory.NewPodInformer(client, 0) c.EventClient = eventClient c.Recorder = recorder c.Broadcaster = eventBroadcaster c.LeaderElection = leaderElectionConfig return c, nil } 那调用这个Config()函数的地方又在哪里呢？继续跟就到 runCommand 里面了～ 2.3. runCommand runCommand 这个函数我们不陌生： cmd/kube-scheduler/app/server.go:117 func runCommand(cmd *cobra.Command, args []string, opts *options.Options) error { // …… // 这个地方完成了前面说到的配置文件和命令行参数等读取和应用工作 c, err := opts.Config() if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } stopCh := make(chan struct{}) // Get the completed config cc := c.Complete() // To help debugging, immediately log version klog.Infof(\"Version: %+v\", version.Get()) // 这里有一堆逻辑 algorithmprovider.ApplyFeatureGates() // Configz registration. // …… return Run(cc, stopCh) } runCommand 在最开始的时候我们有见到过，已经到 cobra 入口的 Run 中了： cmd/kube-scheduler/app/server.go:85 Run: func(cmd *cobra.Command, args []string) { if err := runCommand(cmd, args, opts); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, 上面涉及到2个知识点： ApplyFeatureGates Run 中的逻辑 我们下面分别来看看～ 3. ApplyFeatureGates 这个函数跟进去可以看到如下几行简单的代码，这里很自然我们能够想到继续跟defaults.ApplyFeatureGates()，但是不能只看到这个函数哦，具体来看： pkg/scheduler/algorithmprovider/plugins.go:17 package algorithmprovider import ( \"k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults\" ) // ApplyFeatureGates applies algorithm by feature gates. func ApplyFeatureGates() { defaults.ApplyFeatureGates() } 到这里分2条路： import defaults 这个 package 的时候有一个init()函数调用的逻辑 defaults.ApplyFeatureGates() 函数调用本身。 3.1. 默认算法注册 pkg/scheduler/algorithmprovider/defaults/defaults.go:38 func init() { // …… registerAlgorithmProvider(defaultPredicates(), defaultPriorities()) // …… } init()函数中我们先关注 registerAlgorithmProvider() 函数，这里从字面上可以得到不少信息，大胆猜一下：是不是注册了默认的预选算法和优选算法？ pkg/scheduler/algorithmprovider/defaults/defaults.go:222 func registerAlgorithmProvider(predSet, priSet sets.String) { // 注册 algorithm provider. 默认使用 DefaultProvider factory.RegisterAlgorithmProvider(factory.DefaultProvider, predSet, priSet) factory.RegisterAlgorithmProvider(ClusterAutoscalerProvider, predSet, copyAndReplace(priSet, \"LeastRequestedPriority\", \"MostRequestedPriority\")) } 看到这里可以关注到 AlgorithmProvider 这个概念，后面会讲到。 先看一下里面调用的注册函数是怎么实现的： pkg/scheduler/factory/plugins.go:387 func RegisterAlgorithmProvider(name string, predicateKeys, priorityKeys sets.String) string { schedulerFactoryMutex.Lock() defer schedulerFactoryMutex.Unlock() validateAlgorithmNameOrDie(name) // 很明显，关键逻辑在这里 algorithmProviderMap[name] = AlgorithmProviderConfig{ FitPredicateKeys: predicateKeys, PriorityFunctionKeys: priorityKeys, } return name } 首先，algorithmProviderMap 这个变量是一个包级变量，在86行做的定义：algorithmProviderMap = make(map[string]AlgorithmProviderConfig) 这里的 key 有2种情况： \"DefaultProvider\" \"ClusterAutoscalerProvider\" 混合云场景用得到 ClusterAutoscalerProvider，大家感兴趣可以研究一下 ClusterAutoscaler 特性，这块我们先不说。默认的情况是生效的 DefaultProvider，这块逻辑后面还会提到。 然后这个 map 的 value 的类型是一个简单的 struct： pkg/scheduler/factory/plugins.go:99 type AlgorithmProviderConfig struct { FitPredicateKeys sets.String PriorityFunctionKeys sets.String } 接着看一下defaultPredicates()函数 pkg/scheduler/algorithmprovider/defaults/defaults.go:106 func defaultPredicates() sets.String { return sets.NewString( // Fit is determined by volume zone requirements. factory.RegisterFitPredicateFactory( predicates.NoVolumeZoneConflictPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewVolumeZonePredicate(args.PVInfo, args.PVCInfo, args.StorageClassInfo) }, ), // …… factory.RegisterFitPredicate(predicates.NoDiskConflictPred, predicates.NoDiskConflict), // …… ) } 这个函数里面就2中类型的玩法，简化一些可以理解成上面这个样子，我们一个个来看。 先认识一下 sets.NewString()函数要干嘛： vendor/k8s.io/apimachinery/pkg/util/sets/string.go:27 type String map[string]Empty // NewString creates a String from a list of values. func NewString(items ...string) String { ss := String{} ss.Insert(items...) return ss } // …… // Insert adds items to the set. func (s String) Insert(items ...string) { for _, item := range items { s[item] = Empty{} } } 如上，很简单的类型封装。里面的Empty是：type Empty struct{}，所以本质上就是要用map[string]struct{}这个类型罢了。 因此上面defaultPredicates()函数中sets.NewString()内每一个参数本质上就是一个 string 类型了，我们来看这一个个 string 是怎么来的。 pkg/scheduler/factory/plugins.go:195 func RegisterFitPredicateFactory(name string, predicateFactory FitPredicateFactory) string { schedulerFactoryMutex.Lock() defer schedulerFactoryMutex.Unlock() validateAlgorithmNameOrDie(name) // 唯一值的关注的逻辑 fitPredicateMap[name] = predicateFactory // 返回 name return name } 这个函数要返回一个 string 我们已经知道了，里面的逻辑也只有这一行需要我们关注：fitPredicateMap[name] = predicateFactory，这个 map 类型也是一个包级变量：fitPredicateMap = make(map[string]FitPredicateFactory)，所以前面讲的注册本质也就是在填充这个变量而已。理解fitPredicateMap[name] = predicateFactory中 fitPredicateMap 的 key 和 value，也就知道了这里的 Register 要做什么。 defaultPredicates()中的第二种注册方式 RegisterFitPredicate 区别不大，函数体也是调用的 RegisterFitPredicateFactory()： pkg/scheduler/factory/plugins.go:106 func RegisterFitPredicate(name string, predicate algorithm.FitPredicate) string { return RegisterFitPredicateFactory(name, func(PluginFactoryArgs) algorithm.FitPredicate { return predicate }) } 3.2. 特性开关 pkg/scheduler/algorithmprovider/defaults/defaults.go:183 func ApplyFeatureGates() { if utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition) { factory.RemoveFitPredicate(predicates.CheckNodeConditionPred) factory.RemoveFitPredicate(predicates.CheckNodeMemoryPressurePred) factory.RemoveFitPredicate(predicates.CheckNodeDiskPressurePred) factory.RemoveFitPredicate(predicates.CheckNodePIDPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeConditionPred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeMemoryPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeDiskPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodePIDPressurePred) factory.RegisterMandatoryFitPredicate(predicates.PodToleratesNodeTaintsPred, predicates.PodToleratesNodeTaints) factory.RegisterMandatoryFitPredicate(predicates.CheckNodeUnschedulablePred, predicates.CheckNodeUnschedulablePredicate) factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.PodToleratesNodeTaintsPred) factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.CheckNodeUnschedulablePred) } if utilfeature.DefaultFeatureGate.Enabled(features.ResourceLimitsPriorityFunction) { factory.RegisterPriorityFunction2(\"ResourceLimitsPriority\", priorities.ResourceLimitsPriorityMap, nil, 1) factory.InsertPriorityKeyToAlgorithmProviderMap(factory.RegisterPriorityFunction2(\"ResourceLimitsPriority\", priorities.ResourceLimitsPriorityMap, nil, 1)) } } 这个函数看着几十行，实际上只在重复一件事情，增加或删除一些预选和优选算法。我们看一下这里的一些逻辑： utilfeature.DefaultFeatureGate.Enabled() 函数要做的事情是判断一个 feature 是否开启；函数参数本质只是一个字符串： pkg/features/kube_features.go:25 const ( AppArmor utilfeature.Feature = \"AppArmor\" DynamicKubeletConfig utilfeature.Feature = \"DynamicKubeletConfig\" // …… ) 这里定义了很多的 feature，然后定义了哪些 feature 是开启的，处在 alpha 还是 beta 或者 GA 等： pkg/features/kube_features.go:405 var defaultKubernetesFeatureGates = map[utilfeature.Feature]utilfeature.FeatureSpec{ AppArmor: {Default: true, PreRelease: utilfeature.Beta}, DynamicKubeletConfig: {Default: true, PreRelease: utilfeature.Beta}, ExperimentalHostUserNamespaceDefaultingGate: {Default: false, PreRelease: utilfeature.Beta}, ExperimentalCriticalPodAnnotation: {Default: false, PreRelease: utilfeature.Alpha}, DevicePlugins: {Default: true, PreRelease: utilfeature.Beta}, TaintBasedEvictions: {Default: true, PreRelease: utilfeature.Beta}, RotateKubeletServerCertificate: {Default: true, PreRelease: utilfeature.Beta}, // …… } 所以回到前面ApplyFeatureGates()的逻辑，utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition)要判断的是 TaintNodesByCondition 这个特性是否开启了，如果开启了就把 predicates 中 \"CheckNodeCondition\", \"CheckNodeMemoryPressure\", \"CheckNodePIDPressurePred\", \"CheckNodeDiskPressure\" 这几个算法去掉，把 \"PodToleratesNodeTaints\", \"CheckNodeUnschedulable\" 加上。接着对于特性 \"ResourceLimitsPriorityFunction\" 的处理也是同一个逻辑。 4. Scheduler 的创建 我们换一条线，从 Scheduler 对象的创建再来看另外几个知识点。 前面分析到runCommand()函数的时候我们说到了需要关注最后一行return Run(cc, stopCh)的逻辑，在Run()函数中主要的逻辑就是创建 Scheduler 和启动 Scheduler；现在我们来看创建逻辑： cmd/kube-scheduler/app/server.go:174 sched, err := scheduler.New(cc.Client, cc.InformerFactory.Core().V1().Nodes(), cc.PodInformer, cc.InformerFactory.Core().V1().PersistentVolumes(), cc.InformerFactory.Core().V1().PersistentVolumeClaims(), cc.InformerFactory.Core().V1().ReplicationControllers(), cc.InformerFactory.Apps().V1().ReplicaSets(), cc.InformerFactory.Apps().V1().StatefulSets(), cc.InformerFactory.Core().V1().Services(), cc.InformerFactory.Policy().V1beta1().PodDisruptionBudgets(), storageClassInformer, cc.Recorder, cc.ComponentConfig.AlgorithmSource, stopCh, scheduler.WithName(cc.ComponentConfig.SchedulerName), scheduler.WithHardPodAffinitySymmetricWeight(cc.ComponentConfig.HardPodAffinitySymmetricWeight), scheduler.WithEquivalenceClassCacheEnabled(cc.ComponentConfig.EnableContentionProfiling), scheduler.WithPreemptionDisabled(cc.ComponentConfig.DisablePreemption), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithBindTimeoutSeconds(*cc.ComponentConfig.BindTimeoutSeconds)) 这里调用了一个New()函数，传了很多参数进去。New()函数的定义如下： pkg/scheduler/scheduler.go:131 func New(client clientset.Interface, nodeInformer coreinformers.NodeInformer, podInformer coreinformers.PodInformer, pvInformer coreinformers.PersistentVolumeInformer, pvcInformer coreinformers.PersistentVolumeClaimInformer, replicationControllerInformer coreinformers.ReplicationControllerInformer, replicaSetInformer appsinformers.ReplicaSetInformer, statefulSetInformer appsinformers.StatefulSetInformer, serviceInformer coreinformers.ServiceInformer, pdbInformer policyinformers.PodDisruptionBudgetInformer, storageClassInformer storageinformers.StorageClassInformer, recorder record.EventRecorder, schedulerAlgorithmSource kubeschedulerconfig.SchedulerAlgorithmSource, stopCh 这里涉及到的东西有点小多，我们一点点看： options := defaultSchedulerOptions 这行代码的 defaultSchedulerOptions 是一个 schedulerOptions 对象： pkg/scheduler/scheduler.go:121 // LINE 67 type schedulerOptions struct { schedulerName string hardPodAffinitySymmetricWeight int32 enableEquivalenceClassCache bool disablePreemption bool percentageOfNodesToScore int32 bindTimeoutSeconds int64 } // …… // LINE 121 var defaultSchedulerOptions = schedulerOptions{ // \"default-scheduler\" schedulerName: v1.DefaultSchedulerName, // 1 hardPodAffinitySymmetricWeight: v1.DefaultHardPodAffinitySymmetricWeight, enableEquivalenceClassCache: false, disablePreemption: false, // 50 percentageOfNodesToScore: schedulerapi.DefaultPercentageOfNodesToScore, // 100 bindTimeoutSeconds: BindTimeoutSeconds, } 回到New()函数的逻辑： pkg/scheduler/scheduler.go:148 for _, opt := range opts { opt(&options) } 这里的 opts 定义在参数里：opts ...func(o *schedulerOptions)，我们看一个实参来理解一下：scheduler.WithName(cc.ComponentConfig.SchedulerName)： pkg/scheduler/scheduler.go:80 // 这个函数能够把给定的 schedulerName 赋值给 schedulerOptions.schedulerName func WithName(schedulerName string) Option { return func(o *schedulerOptions) { o.schedulerName = schedulerName } } 这种方式设置一个对象的属性还是挺有意思的。 4.1. 调度算法源 我们继续往后面看New()函数的其他逻辑： source := schedulerAlgorithmSource 这行代码里的 schedulerAlgorithmSource 代表了什么？ 形参中有这个变量的定义：schedulerAlgorithmSource kubeschedulerconfig.SchedulerAlgorithmSource，跟进去可以看到： pkg/scheduler/apis/config/types.go:97 // 表示调度算法源，两个属性是互相排斥的，也就是二选一的意思 type SchedulerAlgorithmSource struct { // Policy is a policy based algorithm source. Policy *SchedulerPolicySource // Provider is the name of a scheduling algorithm provider to use. Provider *string } 这两个属性肯定得理解一下了，目测挺重要的样子： Policy pkg/scheduler/apis/config/types.go:106 type SchedulerPolicySource struct { // 文件方式配置生效的调度算法 File *SchedulerPolicyFileSource // ConfigMap 方式配置生效的调度算法 ConfigMap *SchedulerPolicyConfigMapSource } // 下面分别是2个属性的结构定义： // …… type SchedulerPolicyFileSource struct { // Path is the location of a serialized policy. Path string } // …… type SchedulerPolicyConfigMapSource struct { // Namespace is the namespace of the policy config map. Namespace string // Name is the name of hte policy config map. Name string } 大家还记得我们在讲调度器设计的时候提到的 Policy 文件不？大概长这个样子： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\"}, {\"name\" : \"HostName\"} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10, \"alwaysCheckAllPredicates\" : false } 所以啊，这个 Policy原来是通过代码里的 SchedulerPolicySource 去配置的～ 4.2. policy / provider 如何生效 前面讲到调度算法从何而来(源头)，现在我们看一下这些算法配置如何生效的： pkg/scheduler/scheduler.go:173 source := schedulerAlgorithmSource switch { // 如果 Provider 配置了，就不用 policy 了 case source.Provider != nil: // 根据给定的 Provider 创建 scheduler config sc, err := configurator.CreateFromProvider(*source.Provider) if err != nil { return nil, fmt.Errorf(\"couldn't create scheduler using provider %q: %v\", *source.Provider, err) } config = sc // 如果 Policy 提供了，就没有上面的 provider 的事情了 case source.Policy != nil: // 根据给定的 Policy 创建 scheduler config policy := &schedulerapi.Policy{} switch { // 是 File 的情况 case source.Policy.File != nil: if err := initPolicyFromFile(source.Policy.File.Path, policy); err != nil { return nil, err } // 是 ConfigMap 的情况 case source.Policy.ConfigMap != nil: if err := initPolicyFromConfigMap(client, source.Policy.ConfigMap, policy); err != nil { return nil, err } } sc, err := configurator.CreateFromConfig(*policy) if err != nil { return nil, fmt.Errorf(\"couldn't create scheduler from policy: %v\", err) } config = sc default: return nil, fmt.Errorf(\"unsupported algorithm source: %v\", source) } 上面代码涉及到的2个类型我们再来关注一下： schedulerapi.Policy factory.Config 这个 Policy 就是具体用于存放我们配置的 policy 的载体，对照着这个结构我们可以判断自己在配置 policy 的时候应该按照什么格式： pkg/scheduler/api/types.go:47 type Policy struct { metav1.TypeMeta Predicates []PredicatePolicy Priorities []PriorityPolicy ExtenderConfigs []ExtenderConfig HardPodAffinitySymmetricWeight int32 AlwaysCheckAllPredicates bool } 这个结构内部封装的一层层结构我就不继续贴了，大家感兴趣可以点开看一下，跟到底的落点都是基础类型的，string啊，int啊，bool啊这些～ 关于 factory.Config 可能大家有印象，这个结构就是 Scheduler 对象的唯一属性： pkg/scheduler/scheduler.go:58 type Scheduler struct { config *factory.Config } Config 结构体的属性不外乎 Scheduler 在落实调度、抢占等动作时所需要的一系列方法(或对象)；在New()函数的最后有一行sched := NewFromConfig(config)，实现是简单地实例化 Scheduler，然后将 config 赋值给 Scheduler 的 config 属性，然后返回 Scheduler 对象的地址。 5. 默认生效的算法 我们最后还是单独拎出来强调一下生效了哪些算法的具体逻辑吧，前面有提到一些了，我相信肯定有人很关注这个知识点。 前面提到 Scheduler 创建的时候使用的 New()函数，函数中 switch 判断 schedulerAlgorithmSource 是 Provider 还是 Policy，然后做了具体的初始化逻辑，我们具体看其中一个初始化， 串一下这些点： sc, err := configurator.CreateFromProvider(*source.Provider) 如果我们配置的是 Provider，这时候代码逻辑调用的是这样一行，这个函数的实现如下： pkg/scheduler/factory/factory.go:1156 func (c *configFactory) CreateFromProvider(providerName string) (*Config, error) { // 比如说我们配置的 name 是 DefaultProvider，这个函数要获取一个 AlgorithmProviderConfig 类型的对象 provider, err := GetAlgorithmProvider(providerName) if err != nil { return nil, err } // 下面详细看 return c.CreateFromKeys(provider.FitPredicateKeys, provider.PriorityFunctionKeys, []algorithm.SchedulerExtender{}) } 这个函数里有2个点需要关注，第一个是GetAlgorithmProvider()函数返回了什么： pkg/scheduler/factory/plugins.go:99 type AlgorithmProviderConfig struct { FitPredicateKeys sets.String PriorityFunctionKeys sets.String } 看到这个返回值类型，心里就明朗了。 我们继续看比较重要的CreateFromKeys()方法调用的具体逻辑，这个函数的实参中 provider.FitPredicateKeys, provider.PriorityFunctionKeys 很明显和具体的 provider 相关，不同 provider 定义的预置算法不同。继续来看函数实现： pkg/scheduler/factory/factory.go:1255 func (c *configFactory) CreateFromKeys(predicateKeys, priorityKeys sets.String, extenders []algorithm.SchedulerExtender) (*Config, error) { // …… // 根据 predicateKeys 得到 predicateFuncs predicateFuncs, err := c.GetPredicates(predicateKeys) // 根据 priorityKeys 得到 priorityConfigs priorityConfigs, err := c.GetPriorityFunctionConfigs(priorityKeys) // …… // 创建一个 genericScheduler，这个对象我们很熟悉。algo 也就是 Algorithm 的简写； algo := core.NewGenericScheduler( c.schedulerCache, c.equivalencePodCache, c.podQueue, predicateFuncs, // 和 predicateKeys 对应 predicateMetaProducer, priorityConfigs, // 和 priorityKeys 对应 priorityMetaProducer, // …… ) podBackoff := util.CreateDefaultPodBackoff() return &Config{ // …… Algorithm: algo, // 很清晰了 // …… }, nil } 上面的NewGenericScheduler()函数接收了这些参数之后丢给了 genericScheduler 对象，这个对象中 predicates 属性对应参数 predicateFuncs，prioritizers 属性对应参数 priorityConfigs； 从这里的代码可以看出来我们配置的算法源可以影响到 Scheduler 的初始化，最终体现在改变了 Scheduler 对象的 config 属性的 Algorithm 属性的 prioritizers 和 prioritizers 上。我们最后回顾一下这2个属性的类型，就和以前的预选、优选过程分析的时候关注的点对上了： predicates --> map[string]algorithm.FitPredicate prioritizers --> []algorithm.PriorityConfig 是不是很熟悉呢？ 行，今天就讲到这里～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-03 21:11:14 调度器初始化1. 概述2. 从 --config 开始2.1. options.Option 对象2.2. config.Config对象2.3. runCommand3. ApplyFeatureGates3.1. 默认算法注册3.2. 特性开关4. Scheduler 的创建4.1. 调度算法源4.2. policy / provider 如何生效5. 默认生效的算法"},"core/scheduler/affinity.html":{"url":"core/scheduler/affinity.html","title":"专题-亲和性调度","keywords":"","body":"专题-亲和性调度(Author - XiaoYang) 简介 约束调度 Labels.selector标签选择器 Node亲和性 Node亲和性预选策略MatchNodeSelectorPred Node亲和性优选策略NodeAffinityPriority Pod亲和性 Pod亲和性预选策略MatchInterPodAffinityPred Pod亲和性优选策略InterPodAffinityPriority Service亲和性 Serice亲和性预选策略checkServiceAffinity 1. 简介 在未分析和深入理解scheduler源码逻辑之前，本人在操作配置亲和性上，由于官方和第三方文档者说明不清楚等原因，在亲和性理解上有遇到过一些困惑，如： 亲和性的operator的 “In”底层是什么匹配操作？正则匹配吗？“Gt/Lt”底层又是什么操作实现的？ 所有能查到的文档描述pod亲和性的topoloykey有三个： kubernetes.io/hostname failure-domain.beta.kubernetes.io/zone failure-domain.beta.kubernetes.io/region 为什么？真的只支持这三个key？不能自定义？ Pod与Node亲和性两种类型的差异是什么？而Pod亲和性正真要去匹配的是什么，其内在逻辑是？ 不知道你们是否有同样类似的问题或困惑呢？当你清晰的理解了代码逻辑实现后，那么你会觉得一切是那么的 清楚明确了，不再有“隐性知识”问题存在。所以我希望本文所述内容能给大家在kubernetes亲和性的解惑上有所帮助。 1.1. 约束调度 在展开源码分析之前为更好的理解亲和性代码逻辑，补充一些kubernetes调度相关的基础知识： 亲和性目的是为了实现用户可以按需将pod调度到指定Node上，我称之为“约束调度”。 约束调度操作上常用以下三类： NodeSelector / NodeName node标签选择器 和 \"nodeName\"匹配 Affinity (Node/Pod/Service） 亲和性 Taint / Toleration 污点和容忍 本文所述主题是亲和性，亲和性分为三种类型Node、Pod、Service亲和，以下是亲和性预选和优选阶段代码实现的策略对应表（后面有详细分析）： 预选阶段策略 Pod.Spec配置 类别 次序 MatchNodeSelecotorPred NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution Node 6 MatchInterPodAffinityPred PodAffinity.RequiredDuringSchedulingIgnoredDuringExecution**PodAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution Pod 22 CheckServiceAffinityPred Service 12 优选阶段策略 Pod.Spec配置 默认权重 InterPodAffinityPriority PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution 1 NodeAffinityPriority NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution 1 1.2. Labels.selector标签选择器 labels selector是亲和性代码底层使用最基础的代码工具，不论是nodeAffinity还是podAffinity都是需要用到它。在使用yml类型deployment定义一个pod，配置其亲和性时须指定匹配表达式，其根本的匹配都是要对Node或pod的labels标签进行条件匹配。而这些labels标签匹配计算就必须要用到labels.selector工具(公共使用部分)。 所以在将此块最底层的匹配计算分析部分放在最前面，以便于后面源码分析部分更容易理解。 labels.selector接口定义，关键的方法是Matchs() vendor/k8s.io/apimachinery/pkg/labels/selector.go:36 type Selector interface { Matches(Labels) bool Empty() bool String() string Add(r ...Requirement) Selector Requirements() (requirements Requirements, selectable bool) DeepCopySelector() Selector } 看一下调用端，如下面的几个实例的func，调用labels.NewSelector()实例化一个labels.selector对象返回. func LabelSelectorAsSelector(ps *LabelSelector) (labels.Selector, error) { ... selector := labels.NewSelector() ... } func NodeSelectorRequirementsAsSelector(nsm []v1.NodeSelectorRequirement) (labels.Selector, error) { ... selector := labels.NewSelector() ... } func TopologySelectorRequirementsAsSelector(tsm []v1.TopologySelectorLabelRequirement) (labels.Selector, error) { ... selector := labels.NewSelector() ... } NewSelector返回的是一个InternelSelector类型，而InternelSelector类型是一个Requirement（必要条件） 类型的列表。 vendor/k8s.io/apimachinery/pkg/labels/selector.go:79 func NewSelector() Selector { return internalSelector(nil) } type internalSelector []Requirement InternelSelector类的Matches()底层实现是遍历调用requirement.Matches() vendor/k8s.io/apimachinery/pkg/labels/selector.go:340 func (lsel internalSelector) Matches(l Labels) bool { for ix := range lsel { // internalSelector[ix]为Requirement if matches := lsel[ix].Matches(l); !matches { return false } } return true } 再来看下requirment结构定义(key、操作符、值 ) \"这就是配置的亲和匹配条件表达式\" vendor/k8s.io/apimachinery/pkg/labels/selector.go:114 type Requirement struct { key string operator selection.Operator // In huge majority of cases we have at most one value here. // It is generally faster to operate on a single-element slice // than on a single-element map, so we have a slice here. strValues []string } requirment.matchs() 真正的条件表达式操作实现,基于表达式operator,计算key/value,返回匹配与否 vendor/k8s.io/apimachinery/pkg/labels/selector.go:192 func (r *Requirement) Matches(ls Labels) bool { switch r.operator { case selection.In, selection.Equals, selection.DoubleEquals: if !ls.Has(r.key) { //IN return false } return r.hasValue(ls.Get(r.key)) case selection.NotIn, selection.NotEquals: //NotIn if !ls.Has(r.key) { return true } return !r.hasValue(ls.Get(r.key)) case selection.Exists: //Exists return ls.Has(r.key) case selection.DoesNotExist: //NotExists return !ls.Has(r.key) case selection.GreaterThan, selection.LessThan: // GT、LT if !ls.Has(r.key) { return false } lsValue, err := strconv.ParseInt(ls.Get(r.key), 10, 64) //能转化为数值的”字符数值“ if err != nil { klog.V(10).Infof(\"ParseInt failed for value %+v in label %+v, %+v\", ls.Get(r.key), ls, err) return false } // There should be only one strValue in r.strValues, and can be converted to a integer. if len(r.strValues) != 1 { klog.V(10).Infof(\"Invalid values count %+v of requirement %#v, for 'Gt', 'Lt' operators, exactly one value is required\", len(r.strValues), r) return false } var rValue int64 for i := range r.strValues { rValue, err = strconv.ParseInt(r.strValues[i], 10, 64) if err != nil { klog.V(10).Infof(\"ParseInt failed for value %+v in requirement %#v, for 'Gt', 'Lt' operators, the value must be an integer\", r.strValues[i], r) return false } } return (r.operator == selection.GreaterThan && lsValue > rValue) || (r.operator == selection.LessThan && lsValue 注： 除了LabelsSelector外还有NodeSelector 、FieldsSelector、PropertySelector等，但基本都是类似的Selector接口实现，逻辑上都基本一致，后在源码分析过程有相应的说明。 2. Node亲和性 Node亲和性基础描述: yml配置实例sample： --- apiVersion:v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: #pod实例部署在prd-zone-A 或 prd-zone-B requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/prd-zone-name operator: In values: - prd-zone-A - prd-zone-B preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: securityZone operator: In values: - BussinssZone containers: - name: with-node-affinity image: gcr.io/google_containers/pause:2.0 2.1. Node亲和性预选策略MatchNodeSelectorPred 策略说明： 基于NodeSelector和NodeAffinity定义为被调度的pod选择相匹配的Node（Nodes Labels） 适用NodeAffinity配置项： NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution 预选策略源码分析： 策略注册： defaults.init()注册了一条名为“MatchNodeSelectorPred”预选策略项,策略Func是PodMatchNodeSelector() pkg/scheduler/algorithmprovider/defaults/defaults.go:78 func init() { ... factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) ... } 策略Func: PodMatchNodeSelector() 获取目标Node信息,调用podMatchesNodeSelectorAndAffinityTerms()对被调度pod和目标node进行亲和性匹配。 如果符合则返回true,反之false并记录错误信息。 pkg/scheduler/algorithm/predicates/predicates.go:853 func PodMatchNodeSelector(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { // 获取node信息 node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } // 关键子逻辑func // 输入参数:被调度的pod和前面获取的node(被检测的node) if podMatchesNodeSelectorAndAffinityTerms(pod, node) { return true, nil, nil } return false, []algorithm.PredicateFailureReason{ErrNodeSelectorNotMatch}, nil } ​ podMatchesNodeSelectorAndAffinityTerms() ​ NodeSelector和NodeAffinity定义的\"必要条件\"配置匹配检测 pkg/scheduler/algorithm/predicates/predicates.go:807 func podMatchesNodeSelectorAndAffinityTerms(pod *v1.Pod, node *v1.Node) bool { // 如果设置了NodeSelector,则检测Node labels是否满足NodeSelector所定义的所有terms项. if len(pod.Spec.NodeSelector) > 0 { selector := labels.SelectorFromSet(pod.Spec.NodeSelector) if !selector.Matches(labels.Set(node.Labels)) { return false } } //如果设置了NodeAffinity，则进行Node亲和性匹配 nodeMatchesNodeSelectorTerms() *[后面有详细分析]* nodeAffinityMatches := true affinity := pod.Spec.Affinity if affinity != nil && affinity.NodeAffinity != nil { nodeAffinity := affinity.NodeAffinity if nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution == nil { return true } if nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution != nil { nodeSelectorTerms := nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms klog.V(10).Infof(\"Match for RequiredDuringSchedulingIgnoredDuringExecution node selector terms %+v\", nodeSelectorTerms) // 关键处理func: nodeMatchesNodeSelectorTerms() nodeAffinityMatches = nodeAffinityMatches && nodeMatchesNodeSelectorTerms(node, nodeSelectorTerms) } } return nodeAffinityMatches } 注： NodeSelector和NodeAffinity.Require... 都存在配置则都需True； 如果NodeSelector失败则直接false,不处理NodeAffinity; 如果指定了多个 NodeSelectorTerms，那 node只要满足其中一个条件; 如果指定了多个 MatchExpressions，那必须要满足所有条件. nodeMatchesNodeSelectorTerms() 调用v1helper.MatchNodeSelectorTerms()进行NodeSelectorTerm定义的必要条件进行检测是否符合。 关键的配置定义分为两类(matchExpressions/matchFileds)： -“requiredDuringSchedulingIgnoredDuringExecution.matchExpressions”定义检测(匹配key与value) -“requiredDuringSchedulingIgnoredDuringExecution.matchFileds”定义检测(不匹配key，只value) pkg/scheduler/algorithm/predicates/predicates.go:797 func nodeMatchesNodeSelectorTerms(node *v1.Node, nodeSelectorTerms []v1.NodeSelectorTerm) bool { nodeFields := map[string]string{} // 获取检测目标node的Filelds for k, f := range algorithm.NodeFieldSelectorKeys { nodeFields[k] = f(node) } // 调用v1helper.MatchNodeSelectorTerms() // 参数：nodeSelectorTerms 亲和性配置的必要条件Terms // labels 被检测的目标node的label列表 // fields 被检测的目标node filed列表 return v1helper.MatchNodeSelectorTerms(nodeSelectorTerms, labels.Set(node.Labels), fields.Set(nodeFields)) } // pkg/apis/core/v1/helper/helpers.go:302 func MatchNodeSelectorTerms( nodeSelectorTerms []v1.NodeSelectorTerm, nodeLabels labels.Set, nodeFields fields.Set,) bool { for _, req := range nodeSelectorTerms { // nil or empty term selects no objects if len(req.MatchExpressions) == 0 && len(req.MatchFields) == 0 { continue } // MatchExpressions条件表达式匹配 ① if len(req.MatchExpressions) != 0 { labelSelector, err := NodeSelectorRequirementsAsSelector(req.MatchExpressions) if err != nil || !labelSelector.Matches(nodeLabels) { continue } } // MatchFields条件表达式匹配 ② if len(req.MatchFields) != 0 { fieldSelector, err := NodeSelectorRequirementsAsFieldSelector(req.MatchFields) if err != nil || !fieldSelector.Matches(nodeFields) { continue } } return true } return false } ① NodeSelectorRequirementAsSelector() 是对“requiredDuringSchedulingIgnoredDuringExecution.matchExpressions\"所配置的表达式进行Selector表达式进行格式化加工，返回一个labels.Selector实例化对象. [本文开头1.2章节有分析] pkg/apis/core/v1/helper/helpers.go:222 func NodeSelectorRequirementsAsSelector(nsm []v1.NodeSelectorRequirement) (labels.Selector, error) { if len(nsm) == 0 { return labels.Nothing(), nil } selector := labels.NewSelector() for _, expr := range nsm { var op selection.Operator switch expr.Operator { case v1.NodeSelectorOpIn: op = selection.In case v1.NodeSelectorOpNotIn: op = selection.NotIn case v1.NodeSelectorOpExists: op = selection.Exists case v1.NodeSelectorOpDoesNotExist: op = selection.DoesNotExist case v1.NodeSelectorOpGt: op = selection.GreaterThan case v1.NodeSelectorOpLt: op = selection.LessThan default: return nil, fmt.Errorf(\"%q is not a valid node selector operator\", expr.Operator) } // 表达式的三个关键要素： expr.Key, op, expr.Values r, err := labels.NewRequirement(expr.Key, op, expr.Values) if err != nil { return nil, err } selector = selector.Add(*r) } return selector, nil } ② NodeSelectorRequirementAsFieldSelector() 是对“requiredDuringSchedulingIgnoredDuringExecution.matchFields\"所配置的表达式进行Selector表达式进行格式化加工，返回一个Fields.Selector实例化对象. pkg/apis/core/v1/helper/helpers.go:256 func NodeSelectorRequirementsAsFieldSelector(nsm []v1.NodeSelectorRequirement) (fields.Selector, error) { if len(nsm) == 0 { return fields.Nothing(), nil } selectors := []fields.Selector{} for _, expr := range nsm { switch expr.Operator { case v1.NodeSelectorOpIn: if len(expr.Values) != 1 { return nil, fmt.Errorf(\"unexpected number of value (%d) for node field selector operator %q\", len(expr.Values), expr.Operator) } selectors = append(selectors, fields.OneTermEqualSelector(expr.Key, expr.Values[0])) case v1.NodeSelectorOpNotIn: if len(expr.Values) != 1 { return nil, fmt.Errorf(\"unexpected number of value (%d) for node field selector operator %q\", len(expr.Values), expr.Operator) } selectors = append(selectors, fields.OneTermNotEqualSelector(expr.Key, expr.Values[0])) default: return nil, fmt.Errorf(\"%q is not a valid node field selector operator\", expr.Operator) } } return fields.AndSelectors(selectors...), nil } 关键数据结构 NodeSelector相关结构的定义 vendor/k8s.io/api/core/v1/types.go:2436 type NodeSelector struct { NodeSelectorTerms []NodeSelectorTerm `json:\"nodeSelectorTerms\" protobuf:\"bytes,1,rep,name=nodeSelectorTerms\"` } type NodeSelectorTerm struct { MatchExpressions []NodeSelectorRequirement `json:\"matchExpressions,omitempty\" protobuf:\"bytes,1,rep,name=matchExpressions\"` MatchFields []NodeSelectorRequirement `json:\"matchFields,omitempty\" protobuf:\"bytes,2,rep,name=matchFields\"` } type NodeSelectorRequirement struct { Key string `json:\"key\" protobuf:\"bytes,1,opt,name=key\"` Operator NodeSelectorOperator `json:\"operator\" protobuf:\"bytes,2,opt,name=operator,casttype=NodeSelectorOperator\"` Values []string `json:\"values,omitempty\" protobuf:\"bytes,3,rep,name=values\"` } type NodeSelectorOperator string const ( NodeSelectorOpIn NodeSelectorOperator = \"In\" NodeSelectorOpNotIn NodeSelectorOperator = \"NotIn\" NodeSelectorOpExists NodeSelectorOperator = \"Exists\" NodeSelectorOpDoesNotExist NodeSelectorOperator = \"DoesNotExist\" NodeSelectorOpGt NodeSelectorOperator = \"Gt\" NodeSelectorOpLt NodeSelectorOperator = \"Lt\" ) FieldsSelector实现类的结构定义（Match value) vendor/k8s.io/apimachinery/pkg/fields/selector.go:78 type hasTerm struct { field, value string } func (t *hasTerm) Matches(ls Fields) bool { return ls.Get(t.field) == t.value } type notHasTerm struct { field, value string } func (t *notHasTerm) Matches(ls Fields) bool { return ls.Get(t.field) != t.value } 2.2. Node亲和性优选策略NodeAffinityPriority 策略说明： 通过被调度的pod亲和性配置定义条件，对潜在可被调度运行的Nodes进行亲和性匹配并评分. 适用NodeAffinity配置项： NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution 预选策略源码分析： 策略注册：defaultPriorities()注册了一条名为“NodeAffinityPriority”优选策略项.并注册了策略的两个方法Map/Reduce： CalculateNodeAffinityPriorityMap() map计算， 对潜在被调度Node进行亲和匹配，并为其计权重得分. CalculateNodeAffinityPriorityReduce() reduce计算，重新统计得分,取值区间0~10. pkg/scheduler/algorithmprovider/defaults/defaults.go:266 //k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go/algorithmprovider/defaults.go func defaultPriorities() sets.String { ... factory.RegisterPriorityFunction2(\"NodeAffinityPriority\", priorities.CalculateNodeAffinityPriorityMap, priorities.CalculateNodeAffinityPriorityReduce, 1), ... } 策略Func: map计算 CalculateNodeAffinityPriorityMap() 遍历affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution所 定义的Terms解NodeSelector对象(labels.selector)后，对潜在被调度Node的labels进行Match匹配检测，如果匹配则将条件所给定的Weight权重值累计。 最后将返回各潜在的被调度Node最后分值。 pkg/scheduler/algorithm/priorities/node_affinity.go:34 func CalculateNodeAffinityPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { // 获取被检测的Node信息 node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } // 默认为Spec配置的Affinity affinity := pod.Spec.Affinity if priorityMeta, ok := meta.(*priorityMetadata); ok { // We were able to parse metadata, use affinity from there. affinity = priorityMeta.affinity } var count int32 if affinity != nil && affinity.NodeAffinity != nil && affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution != nil { // 遍历PreferredDuringSchedulingIgnoredDuringExecution定义的`必要条件项`(Terms) for i := range affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution { preferredSchedulingTerm := &affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution[i] if preferredSchedulingTerm.Weight == 0 { //注意前端的配置，如果weight为0则不做任何处理 continue } // TODO: Avoid computing it for all nodes if this becomes a performance problem. // 获取node亲和MatchExpression表达式条件，实例化label.Selector对象. nodeSelector, err := v1helper.NodeSelectorRequirementsAsSelector(preferredSchedulingTerm.Preference.MatchExpressions) if err != nil { return schedulerapi.HostPriority{}, err } if nodeSelector.Matches(labels.Set(node.Labels)) { count += preferredSchedulingTerm.Weight } } } // 返回Node得分 return schedulerapi.HostPriority{ Host: node.Name, Score: int(count), }, nil } 再次看到前面(预选策略分析时)分析过的NodeSelectorRequirementAsSelector()返回一个labels.Selector实例对象 使用selector.Matches对node.Labels进行匹配是否符合条件. reduce计算 CalculateNodeAffinityPriorityReduce() 将各个node的最后得分重新计算分布区间在0〜10. 代码内给定一个NormalizeReduce()方法，MaxPriority值为10,reverse取反false关闭 pkg/scheduler/algorithm/priorities/node_affinity.go:77 const MaxPriority = 10 var CalculateNodeAffinityPriorityReduce = NormalizeReduce(schedulerapi.MaxPriority, false) NormalizeReduce() 结果评分取值0〜MaxPriority reverse取反为true时，最终评分=(MaxPriority-其原评分值） pkg/scheduler/algorithm/priorities/reduce.go:29 func NormalizeReduce(maxPriority int, reverse bool) algorithm.PriorityReduceFunction { return func( _ *v1.Pod, _ interface{}, _ map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error { var maxCount int // 取出最大的值 for i := range result { if result[i].Score > maxCount { maxCount = result[i].Score } } // 如果最大的值为0，且取反设为真，则将所有的评分设置为MaxPriority if maxCount == 0 { if reverse { for i := range result { result[i].Score = maxPriority } } return nil } // 计算后得分 = maxPrority * 原分值 / 最大值 // 如果取反为真则 maxPrority - 计算后得分 for i := range result { score := result[i].Score score = maxPriority * score / maxCount if reverse { score = maxPriority - score } result[i].Score = score } return nil } } 3. Pod亲和性 Pod亲和性基础描述: yml配置实例sample： --- apiVersion: apps/v1beta1 kind: Deployment metadata: name: affinity labels: app: affinity spec: replicas: 3 template: metadata: labels: app: affinity role: lab-web spec: containers: - name: nginx image: nginx:1.9.0 ports: - containerPort: 80 name: nginx_web_Lab affinity: #为实现高可用，三个pod应该分布在不同Node上 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - prod-pod topologyKey: kubernetes.io/hostname 3.1. Pod亲和性预选策略MatchInterPodAffinityPred 策略说明： 对需被调度的Pod进行亲和/反亲和配置匹配检测目标Pods，然后获取满足亲和条件的Pods所运行的Nodes ​的 TopologyKey的值(亲和性pod定义topologyKey)与目标 Nodes进行一一匹配是否符合条件. 适用NodeAffinity配置项： PodAffinity.RequiredDuringSchedulingIgnoredDuringExecutionPodAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution 预选策略源码分析： 策略注册：defaultPredicates()注册了一条名为“MatchInterPodAffinity”预选策略项. pkg/scheduler/algorithmprovider/defaults/defaults.go:143 func defaultPredicates() sets.String { ... factory.RegisterFitPredicateFactory( predicates.MatchInterPodAffinityPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewPodAffinityPredicate(args.NodeInfo, args.PodLister) }, ... } 策略Func: checker.InterPodAffinityMatches() Func是通过NewPodAffinityProdicate()实例化PodAffinityChecker类对象后返回。 pkg/scheduler/algorithm/predicates/predicates.go:1138 type PodAffinityChecker struct { info NodeInfo podLister algorithm.PodLister } func NewPodAffinityPredicate(info NodeInfo, podLister algorithm.PodLister) algorithm.FitPredicate { checker := &PodAffinityChecker{ info: info, podLister: podLister, } return checker.InterPodAffinityMatches //返回策略func } InterPodAffinityMatches() 检测一个pod是否满足调度到特定的（符合pod亲和或反亲和配置）Node上。 satisfiesExistingPodsAntiAffinity() 满足存在的Pods反亲和配置. satisfiesPodsAffinityAntiAffinity() 满足Pods亲和与反亲和配置. pkg/scheduler/algorithm/predicates/predicates.go:1155 func (c *PodAffinityChecker) InterPodAffinityMatches(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } //① if failedPredicates, error := c.satisfiesExistingPodsAntiAffinity(pod, meta, nodeInfo); failedPredicates != nil { failedPredicates := append([]algorithm.PredicateFailureReason{ErrPodAffinityNotMatch}, failedPredicates) return false, failedPredicates, error } // Now check if requirements will be satisfied on this node. affinity := pod.Spec.Affinity if affinity == nil || (affinity.PodAffinity == nil && affinity.PodAntiAffinity == nil) { return true, nil, nil } //② if failedPredicates, error := c.satisfiesPodsAffinityAntiAffinity(pod, meta, nodeInfo, affinity); failedPredicates != nil { failedPredicates := append([]algorithm.PredicateFailureReason{ErrPodAffinityNotMatch}, failedPredicates) return false, failedPredicates, error } return true, nil, nil } ① satisfiesExistingPodsAntiAffinity() 检测当pod被调度到目标node上是否触犯了其它pods所定义的反亲和配置. 即：当调度一个pod到目标Node上，而某个或某些Pod定义了反亲和配置与被 调度的Pod相匹配(触犯)，那么就不应该将此Node加入到可选的潜在调度Nodes列表内. pkg/scheduler/algorithm/predicates/predicates.go:1293 func (c *PodAffinityChecker) satisfiesExistingPodsAntiAffinity(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return ErrExistingPodsAntiAffinityRulesNotMatch, fmt.Errorf(\"Node is nil\") } var topologyMaps *topologyPairsMaps //如果存在预处理的MetaData则直接获取topologyPairsAntiAffinityPodsMap if predicateMeta, ok := meta.(*predicateMetadata); ok { topologyMaps = predicateMeta.topologyPairsAntiAffinityPodsMap } else { // 不存在预处理的MetaData处理逻辑. // 过滤掉pod的nodeName等于NodeInfo.Node.Name,且不存在于nodeinfo中. // 即运行在其它Nodes上的Pods filteredPods, err := c.podLister.FilteredList(nodeInfo.Filter, labels.Everything()) if err != nil { errMessage := fmt.Sprintf(\"Failed to get all pods, %+v\", err) klog.Error(errMessage) return ErrExistingPodsAntiAffinityRulesNotMatch, errors.New(errMessage) } // 获取被调度Pod与其它存在反亲和配置的Pods匹配的topologyMaps if topologyMaps, err = c.getMatchingAntiAffinityTopologyPairsOfPods(pod, filteredPods); err != nil { errMessage := fmt.Sprintf(\"Failed to get all terms that pod %+v matches, err: %+v\", podName(pod), err) klog.Error(errMessage) return ErrExistingPodsAntiAffinityRulesNotMatch, errors.New(errMessage) } } // 遍历所有topology pairs(所有反亲和topologyKey/Value)，检测Node是否有影响. for topologyKey, topologyValue := range node.Labels { if topologyMaps.topologyPairToPods[topologyPair{key: topologyKey, value: topologyValue}] != nil { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v\", podName(pod), node.Name) return ErrExistingPodsAntiAffinityRulesNotMatch, nil } } return nil, nil } getMatchingAntiAffinityTopologyPairsOfPods() 获取被调度Pod与其它存在反亲和配置的Pods匹配的topologyMaps pkg/scheduler/algorithm/predicates/predicates.go:1270 func (c *PodAffinityChecker) getMatchingAntiAffinityTopologyPairsOfPods(pod *v1.Pod, existingPods []*v1.Pod) (*topologyPairsMaps, error) { topologyMaps := newTopologyPairsMaps() // 遍历所有存在Pods,获取pod所运行的Node信息 for _, existingPod := range existingPods { existingPodNode, err := c.info.GetNodeInfo(existingPod.Spec.NodeName) if err != nil { if apierrors.IsNotFound(err) { klog.Errorf(\"Node not found, %v\", existingPod.Spec.NodeName) continue } return nil, err } // 依据被调度的pod、目标pod、目标Node信息(上面获取得到)获取TopologyPairs。 // getMatchingAntiAffinityTopologyPairsOfPod()下面详述 existingPodTopologyMaps, err := getMatchingAntiAffinityTopologyPairsOfPod(pod, existingPod, existingPodNode) if err != nil { return nil, err } topologyMaps.appendMaps(existingPodTopologyMaps) } return topologyMaps, nil } //1)是否ExistingPod定义了反亲和配置，如果没有直接返回 //2)如果有定义，是否有任务一个反亲和Term匹配需被调度的pod. // 如果配置则将返回term定义的TopologyKey和Node的topologyValue. func getMatchingAntiAffinityTopologyPairsOfPod(newPod *v1.Pod, existingPod *v1.Pod, node *v1.Node) (*topologyPairsMaps, error) { affinity := existingPod.Spec.Affinity if affinity == nil || affinity.PodAntiAffinity == nil { return nil, nil } topologyMaps := newTopologyPairsMaps() for _, term := range GetPodAntiAffinityTerms(affinity.PodAntiAffinity) { namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(existingPod, &term) selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector) if err != nil { return nil, err } if priorityutil.PodMatchesTermsNamespaceAndSelector(newPod, namespaces, selector) { if topologyValue, ok := node.Labels[term.TopologyKey]; ok { pair := topologyPair{key: term.TopologyKey, value: topologyValue} topologyMaps.addTopologyPair(pair, existingPod) } } } return topologyMaps, nil } ② satisfiesPodsAffinityAntiAffinity() 满足Pods亲和与反亲和配置. 我们先看一下代码结构，我将共分为两个部分if{}部分,else{}部分,依赖于是否指定了预处理的预选metadata. pkg/scheduler/algorithm/predicates/predicates.go:1367 func (c *PodAffinityChecker) satisfiesPodsAffinityAntiAffinity(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo, affinity *v1.Affinity) (algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return ErrPodAffinityRulesNotMatch, fmt.Errorf(\"Node is nil\") } if predicateMeta, ok := meta.(*predicateMetadata); ok { ... //partI } else { ... //partII } return nil, nil } partI if{...} 如果指定了预处理metadata，则使用此逻辑，否则跳至else{...} 获取所有pod亲和性定义AffinityTerms，如果存在亲和性定义，基于指定的metadata判断AffinityTerms所定义的nodeTopoloykey与值是否所有都存在于metadata.topologyPairsPotentialAffinityPods之内（潜在匹配亲和定义的pod list）。 获取所有pod亲和性定义AntiAffinityTerms，如果存在反亲和定义，基于指定的metadata判断AntiAffinityTerms所定义的nodeTopoloykey与值 是否有一个存在于 metadata.topologyPairsPotentialAntiAffinityPods之内的情况（潜在匹配anti反亲和定义的pod list）。 if predicateMeta, ok := meta.(*predicateMetadata); ok { // 检测所有affinity terms. topologyPairsPotentialAffinityPods := predicateMeta.topologyPairsPotentialAffinityPods if affinityTerms := GetPodAffinityTerms(affinity.PodAffinity); len(affinityTerms) > 0 { matchExists := c.nodeMatchesAllTopologyTerms(pod, topologyPairsPotentialAffinityPods, nodeInfo, affinityTerms) if !matchExists { if !(len(topologyPairsPotentialAffinityPods.topologyPairToPods) == 0 && targetPodMatchesAffinityOfPod(pod, pod)) { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAffinity\", podName(pod), node.Name) return ErrPodAffinityRulesNotMatch, nil } } } // 检测所有anti-affinity terms. topologyPairsPotentialAntiAffinityPods := predicateMeta.topologyPairsPotentialAntiAffinityPods if antiAffinityTerms := GetPodAntiAffinityTerms(affinity.PodAntiAffinity); len(antiAffinityTerms) > 0 { matchExists := c.nodeMatchesAnyTopologyTerm(pod, topologyPairsPotentialAntiAffinityPods, nodeInfo, antiAffinityTerms) if matchExists { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAntiAffinity\", podName(pod), node.Name) return ErrPodAntiAffinityRulesNotMatch, nil } } } 以下说明继续if{…}内所用的各个子逻辑函数分析(按代码位置的先后顺序)： GetPodAffinityTerms() 如果存在podAffinity硬件配置，获取所有\"匹配必要条件”Terms pkg/scheduler/algorithm/predicates/predicates.go:1217 func GetPodAffinityTerms(podAffinity *v1.PodAffinity) (terms []v1.PodAffinityTerm) { if podAffinity != nil { if len(podAffinity.RequiredDuringSchedulingIgnoredDuringExecution) != 0 { terms = podAffinity.RequiredDuringSchedulingIgnoredDuringExecution } } return terms } nodeMatchesAllTopologyTerms() 判断目标Node是否匹配所有亲和性配置的定义Terms的topology值. pkg/scheduler/algorithm/predicates/predicates.go:1336 // 目标Node须匹配所有Affinity terms所定义的TopologyKey，且值须与nodes(运行被亲和匹配表达式匹配的Pods) // 的TopologyKey和值相匹配。 // 注：此逻辑内metadata预计算了topologyPairs func (c *PodAffinityChecker) nodeMatchesAllTopologyTerms(pod *v1.Pod, topologyPairs *topologyPairsMaps, nodeInfo *schedulercache.NodeInfo, terms []v1.PodAffinityTerm) bool { node := nodeInfo.Node() for _, term := range terms { // 判断目标node上是否存在亲和配置定义的TopologyKey的key，取出其topologykey值 // 根据key与值创建topologyPair // 基于metadata.topologyPairsPotentialAffinityPods(潜在亲和pods的topologyPairs)判断\\ //目标node上的ToplogyKey与value是否相互匹配. if topologyValue, ok := node.Labels[term.TopologyKey]; ok { pair := topologyPair{key: term.TopologyKey, value: topologyValue} if _, ok := topologyPairs.topologyPairToPods[pair]; !ok { return false // 一项不满足则为false } } else { return false } } return true } // topologyPairsMaps结构定义 type topologyPairsMaps struct { topologyPairToPods map[topologyPair]podSet podToTopologyPairs map[string]topologyPairSet } targetPodMatchesAffinityOfPod() 根据pod的亲和定义检测目标pod的NameSpace是否符合条件以及 Labels.selector条件表达式是否匹配. pkg/scheduler/algorithm/predicates/metadata.go:498 func targetPodMatchesAffinityOfPod(pod, targetPod *v1.Pod) bool { affinity := pod.Spec.Affinity if affinity == nil || affinity.PodAffinity == nil { return false } affinityProperties, err := getAffinityTermProperties(pod, GetPodAffinityTerms(affinity.PodAffinity)) // ① if err != nil { klog.Errorf(\"error in getting affinity properties of Pod %v\", pod.Name) return false } // ② return podMatchesAllAffinityTermProperties(targetPod, affinityProperties) } // ① 获取affinityTerms所定义所有的namespaces 和 selector 列表， // 返回affinityTermProperites数组. 数组的每项定义{namesapces,selector}. func getAffinityTermProperties(pod *v1.Pod, terms []v1.PodAffinityTerm) (properties []*affinityTermProperties, err error) { if terms == nil { return properties, nil } for _, term := range terms { namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(pod, &term) // 基于定义的亲和性term，创建labels.selector selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector) if err != nil { return nil, err } // 返回 namespaces 和 selector properties = append(properties, &affinityTermProperties{namespaces: namespaces, selector: selector}) } return properties, nil } // 返回Namespace列表（如果term未指定Namespace则使用被调度pod的Namespace）. func GetNamespacesFromPodAffinityTerm(pod *v1.Pod, podAffinityTerm *v1.PodAffinityTerm) sets.String { names := sets.String{} if len(podAffinityTerm.Namespaces) == 0 { names.Insert(pod.Namespace) } else { names.Insert(podAffinityTerm.Namespaces...) } return names } // ② 遍历properties所有定义的namespaces 和 selector 列表，调用PodMatchesTermsNamespaceAndSelector()进行一一匹配. func podMatchesAllAffinityTermProperties(pod *v1.Pod, properties []*affinityTermProperties) bool { if len(properties) == 0 { return false } for _, property := range properties { if !priorityutil.PodMatchesTermsNamespaceAndSelector(pod, property.namespaces, property.selector) { return false } } return true } // 检测NameSpaces一致性和Labels.selector是否匹配. // - 如果pod.Namespaces不相等于指定的NameSpace值则返回false，如果true则继续labels match. // - 如果pod.labels不能Match Labels.selector选择器，则返回false,反之true func PodMatchesTermsNamespaceAndSelector(pod *v1.Pod, namespaces sets.String, selector labels.Selector) bool { if !namespaces.Has(pod.Namespace) { return false } if !selector.Matches(labels.Set(pod.Labels)) { return false } return true } GetPodAntiAffinityTerms() 获取pod反亲和配置所有的必要条件Terms pkg/scheduler/algorithm/predicates/predicates.go:1231 func GetPodAntiAffinityTerms(podAntiAffinity *v1.PodAntiAffinity) (terms []v1.PodAffinityTerm) { if podAntiAffinity != nil { if len(podAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution) != 0 { terms = podAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution } } return terms } nodeMatchesAnyTopologyTerm() 判断目标Node是否有匹配了反亲和的定义Terms的topology值*. pkg/scheduler/algorithm/predicates/predicates.go:1353 // Node只须匹配任何一条AnitAffinity terms所定义的TopologyKey则为True // 逻辑等同于nodeMatchesAllTopologyTerms(),只是匹配一条则返回为true. func (c *PodAffinityChecker) nodeMatchesAnyTopologyTerm(pod *v1.Pod, topologyPairs *topologyPairsMaps, nodeInfo *schedulercache.NodeInfo, terms []v1.PodAffinityTerm) bool { node := nodeInfo.Node() for _, term := range terms { if topologyValue, ok := node.Labels[term.TopologyKey]; ok { pair := topologyPair{key: term.TopologyKey, value: topologyValue} if _, ok := topologyPairs.topologyPairToPods[pair]; ok { return true // 一项满足则为true } } } return false } partII else{...} 如果没有预处理的Metadata，则通过指定podFilter过滤器获取满足条件的pod列表 获取所有亲和配置定义，如果存在则，通过获取PodAffinity所定义的所有namespaces和标签条件表达式进行匹配”目标pod\",完全符合则获取此目标pod的运行node的topologykey（此为affinity指定的topologykey）的 值和\"潜在Node\"的topologykey的值比对是否一致。 与上类似，获取所有anti反亲和配置定义，如果存在则，通过获取PodAntiAffinity所定义的所有namespaces和标签条件表达式进行匹配”目标pod\",完全符合则获取此目标pod的运行node的topologykey（此为AntiAffinity指定的topologykey）的值和\"潜在Node\"的topologykey的值比对是否一致。 else { // We don't have precomputed metadata. We have to follow a slow path to check affinity terms. filteredPods, err := c.podLister.FilteredList(nodeInfo.Filter, labels.Everything()) if err != nil { return ErrPodAffinityRulesNotMatch, err } //获取亲和、反亲和配置定义的\"匹配条件\"Terms affinityTerms := GetPodAffinityTerms(affinity.PodAffinity) antiAffinityTerms := GetPodAntiAffinityTerms(affinity.PodAntiAffinity) matchFound, termsSelectorMatchFound := false, false for _, targetPod := range filteredPods { // 遍历所有目标Pod,检测所有亲和性配置\"匹配条件\"Terms if !matchFound && len(affinityTerms) > 0 { // podMatchesPodAffinityTerms()对namespaces和标签条件表达式进行匹配目标pod【详解后述】 affTermsMatch, termsSelectorMatch, err := c.podMatchesPodAffinityTerms(pod, targetPod, nodeInfo, affinityTerms) if err != nil { errMessage := fmt.Sprintf(\"Cannot schedule pod %+v onto node %v, because of PodAffinity, err: %v\", podName(pod), node.Name, err) klog.Error(errMessage) return ErrPodAffinityRulesNotMatch, errors.New(errMessage) } if termsSelectorMatch { termsSelectorMatchFound = true } if affTermsMatch { matchFound = true } } // 同上，遍历所有目标Pod,检测所有Anti反亲和配置\"匹配条件\"Terms. if len(antiAffinityTerms) > 0 { antiAffTermsMatch, _, err := c.podMatchesPodAffinityTerms(pod, targetPod, nodeInfo, antiAffinityTerms) if err != nil || antiAffTermsMatch { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAntiAffinityTerm, err: %v\", podName(pod), node.Name, err) return ErrPodAntiAffinityRulesNotMatch, nil } } } if !matchFound && len(affinityTerms) > 0 { if termsSelectorMatchFound { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAffinity\", podName(pod), node.Name) return ErrPodAffinityRulesNotMatch, nil } // Check if pod matches its own affinity properties (namespace and label selector). if !targetPodMatchesAffinityOfPod(pod, pod) { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAffinity\", podName(pod), node.Name) return ErrPodAffinityRulesNotMatch, nil } } } 以下说明继续else{…}内所用的子逻辑函数分析： podMatchesPodAffinityTerms() 通过获取亲和配置定义的所有namespaces和标签条件表达式进行匹配目标pod,完全符合则获取此目标pod的运行node的topologykey（此为affinity指定的topologykey）的值和潜在Node的topologykey的值比对是否一致. pkg/scheduler/algorithm/predicates/predicates.go:1189 func (c *PodAffinityChecker) podMatchesPodAffinityTerms(pod, targetPod *v1.Pod, nodeInfo *schedulercache.NodeInfo, terms []v1.PodAffinityTerm) (bool, bool, error) { if len(terms) == 0 { return false, false, fmt.Errorf(\"terms array is empty\") } // 获取{namespaces,selector}列表 props, err := getAffinityTermProperties(pod, terms) if err != nil { return false, false, err } // 匹配目标pod是否在affinityTerm定义的{namespaces,selector}列表内所有项，如果不匹配则返回false, // 如果匹配则获取此pod的运行node信息(称为目标Node)， // 通过“目标Node”所定义的topologykey（此为affinity指定的topologykey）的值来匹配“潜在被调度的Node”的topologykey是否一致。 if !podMatchesAllAffinityTermProperties(targetPod, props) { return false, false, nil } // Namespace and selector of the terms have matched. Now we check topology of the terms. targetPodNode, err := c.info.GetNodeInfo(targetPod.Spec.NodeName) if err != nil { return false, false, err } for _, term := range terms { if len(term.TopologyKey) == 0 { return false, false, fmt.Errorf(\"empty topologyKey is not allowed except for PreferredDuringScheduling pod anti-affinity\") } if !priorityutil.NodesHaveSameTopologyKey(nodeInfo.Node(), targetPodNode, term.TopologyKey) { return false, true, nil } } return true, true, nil } priorityutil.NodesHaveSameTopologyKey() 正真的toplogykey比较实现的逻辑代码块。 *从此代码可以看出deployment的yml对topologykey设定的可以支持自定义的 pkg/scheduler/algorithm/priorities/util/topologies.go:53 // 判断两者的topologyKey定义的值是否一致。 func NodesHaveSameTopologyKey(nodeA, nodeB *v1.Node, topologyKey string) bool { if len(topologyKey) == 0 { return false } if nodeA.Labels == nil || nodeB.Labels == nil { return false } nodeALabel, okA := nodeA.Labels[topologyKey] //取Node一个被意义化的“Label”的值value nodeBLabel, okB := nodeB.Labels[topologyKey] // If found label in both nodes, check the label if okB && okA { return nodeALabel == nodeBLabel //比对 } return false } 3.2. Pod亲和性优选策略InterPodAffinityPriority 策略说明： 并发遍历所有潜在的目标Nodes，对Pods与需被调度Pod的亲和和反亲性检测，对亲性匹配则增，对反亲性 匹配则减， 最终对每个Node进行统计分数。 适用NodeAffinity配置项： PodAffinity.PreferredDuringSchedulingIgnoredDuringExecutionPodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution 预选策略源码分析： 策略注册：defaultPriorities()注册了一条名为“InterPodAffinityPriority”优选策略项. pkg/scheduler/algorithmprovider/defaults/defaults.go:145 // k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go func defaultPriorities() sets.String { ... factory.RegisterPriorityConfigFactory( \"InterPodAffinityPriority\", factory.PriorityConfigFactory{ Function: func(args factory.PluginFactoryArgs) algorithm.PriorityFunction { return priorities.NewInterPodAffinityPriority(args.NodeInfo, args.NodeLister, args.PodLister, args.HardPodAffinitySymmetricWeight) }, Weight: 1, }, ), ... } 策略Func: interPodAffinity.CalculateInterPodAffinityPriority() 通过NewPodAffinityPriority()实例化interPodAffinityod类对象及CalculateInterPodAffinityPriority()策略Func返回。 pkg/scheduler/algorithm/priorities/interpod_affinity.go:45 func NewInterPodAffinityPriority( info predicates.NodeInfo, nodeLister algorithm.NodeLister, podLister algorithm.PodLister, hardPodAffinityWeight int32) algorithm.PriorityFunction { interPodAffinity := &InterPodAffinity{ info: info, nodeLister: nodeLister, podLister: podLister, hardPodAffinityWeight: hardPodAffinityWeight, } return interPodAffinity.CalculateInterPodAffinityPriority } CalculateInterPodAffinityPriority() 基于pod亲和性配置匹配\"必要条件项”Terms,并发处理所有目标nodes,为其目标node统计亲和weight得分. 我们先来看一下它的代码结构： processPod := func(existingPod *v1.Pod) error {… pm.processTerms()} processNode := func(i int) {…} workqueue.ParallelizeUntil(context.TODO(), 16, len(allNodeNames), processNode) fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) 此代码逻辑需理解几个定义： pod 一个\"需被调度的Pod\" hasAffinityConstraints \"被调度的pod\"是否有定义亲和配置 hasAntiAffinityConstraints \"被调度的pod\"是否有定义亲和配置 existingPod 一个待处理的\"亲和目标pod\" existingPodNode 运行此“亲和目标pod”的节点--“目标Node” existingHasAffinityConstraints \"亲和目标pod\"是否存在亲和约束existingHasAntiAffinityConstraints \"亲和目标pod\"是否存在反亲和约束 pkg/scheduler/algorithm/priorities/interpod_affinity.go:119 func (ipa *InterPodAffinity) CalculateInterPodAffinityPriority(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) { affinity := pod.Spec.Affinity //\"需被调度Pod\"是否存在亲和、反亲和约束配置 hasAffinityConstraints := affinity != nil && affinity.PodAffinity != nil hasAntiAffinityConstraints := affinity != nil && affinity.PodAntiAffinity != nil allNodeNames := make([]string, 0, len(nodeNameToInfo)) for name := range nodeNameToInfo { allNodeNames = append(allNodeNames, name) } var maxCount float64 var minCount float64 pm := newPodAffinityPriorityMap(nodes) // processPod()主要处理pod亲和和反亲和weight累计的逻辑代码。 ② // 调用了Terms处理方法：processTerms() processPod := func(existingPod *v1.Pod) error { ... // 亲和性检测逻辑代码 ① pm.processTerms(terms, pod, existingPod, existingPodNode, 1) ... } //ProcessNode()通过一个判断是否存在亲和性配置选择调用processPod() ③ processNode := func(i int) { ... if err := processPod(existingPod); err != nil { pm.setError(err) } ... } // 并发多线程处理调用ProcessNode() workqueue.ParallelizeUntil(context.TODO(), 16, len(allNodeNames), processNode) ... for _, node := range nodes { if pm.counts[node.Name] > maxCount { maxCount = pm.counts[node.Name] } if pm.counts[node.Name] 0 { //reduce计算fScore分 ④ fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) } result = append(result, schedulerapi.HostPriority{ Host: node.Name, Score: int(fScore) }) } } return result, nil } ① ProcessTerms() 给定Pod和此Pod的定义的亲和性配置(podAffinityTerm)、被测目标pod、运行被测目标pod的Node信息，对所有潜在可被调度的Nodes列表进行一一检测,并对根据检测结果为node进行weight累计。 流程如下： “被测Pod”的namespaces是否与“给定的pod”的namespaces是否一致； “被测Pod”的labels是否与“给定的pod”的podAffinityTerm定义匹配; 如果前两条件都为True，则对运行“被测的pod”的node的TopologyKey的值与所有潜在可被调度的Node进行遍历检测 TopologyKey的值是否一致，true则累计weight值. 逻辑理解： 1与2实现了找出在同一个namespace下满足被调pod所配置podAffinityTerm的pods; 3则实现获取topologyKey的值与潜在被调度的Node进行匹配检测” . 此处则可清楚的理解pod亲和性配置匹配的内在含义与逻辑。 pkg/scheduler/algorithm/priorities/interpod_affinity.go:107 func (p *podAffinityPriorityMap) processTerms(terms []v1.WeightedPodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, multiplier int) { for i := range terms { term := &terms[i] p.processTerm(&term.PodAffinityTerm, podDefiningAffinityTerm, podToCheck, fixedNode, float64(term.Weight*int32(multiplier))) } } func (p *podAffinityPriorityMap) processTerm(term *v1.PodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, weight float64) { // 获取namesapce信息(affinityTerm.Namespaces或pod.Namesapce) // 根据podAffinityTerm定义生成selector对象（参看本文开头的述labelSelector） namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(podDefiningAffinityTerm, term) selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector) //labeSelector if err != nil { p.setError(err) return } //判断“被检测的Pod”的Namespace和Selector Labels是否匹配 match := priorityutil.PodMatchesTermsNamespaceAndSelector(podToCheck, namespaces, selector) if match { func() { p.Lock() defer p.Unlock() for _, node := range p.nodes { //对\"运行被检测亲和Pod的Node节点\" 与被考虑的所有Nodes进行一一匹配TopologyKey检查,如相等则进行累加权值 if priorityutil.NodesHaveSameTopologyKey(node, fixedNode, term.TopologyKey) { p.counts[node.Name] += weight } } }() } } GetNamespaceFromPodAffinitTerm() 返回Namespaces列表（如果term未指定Namespace则使用被调度pod的Namespace） pkg/scheduler/algorithm/priorities/util/topologies.go:28 func GetNamespacesFromPodAffinityTerm(pod *v1.Pod, podAffinityTerm *v1.PodAffinityTerm) sets.String { names := sets.String{} if len(podAffinityTerm.Namespaces) == 0 { names.Insert(pod.Namespace) } else { names.Insert(podAffinityTerm.Namespaces...) } return names } PodMatchesTermsNamespaceAndSelector() 检测NameSpace一致性和Labels.selector是否匹配. pkg/scheduler/algorithm/priorities/util/topologies.go:40 func PodMatchesTermsNamespaceAndSelector(pod *v1.Pod, namespaces sets.String, selector labels.Selector) bool { if !namespaces.Has(pod.Namespace) { return false } if !selector.Matches(labels.Set(pod.Labels)) { return false } return true } ② processPod() 处理亲和和反亲和逻辑层，调用processTerms()进行检测与统计权重值。 pkg/scheduler/algorithm/priorities/interpod_affinity.go:136 processPod := func(existingPod *v1.Pod) error { existingPodNode, err := ipa.info.GetNodeInfo(existingPod.Spec.NodeName) if err != nil { if apierrors.IsNotFound(err) { klog.Errorf(\"Node not found, %v\", existingPod.Spec.NodeName) return nil } return err } existingPodAffinity := existingPod.Spec.Affinity existingHasAffinityConstraints := existingPodAffinity != nil && existingPodAffinity.PodAffinity != nil existingHasAntiAffinityConstraints := existingPodAffinity != nil && existingPodAffinity.PodAntiAffinity != nil //如果\"需被调度的Pod\"存在亲和约束，则与\"亲和目标Pod\"和\"亲和目标Node\"进行一次ProcessTerms()检测，如果成立则wieght权重值加1倍. if hasAffinityConstraints { terms := affinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, pod, existingPod, existingPodNode, 1) } // 如果\"需被调度的Pod\"存在反亲和约束，则与\"亲和目标Pod\"和\"亲和目标Node\"进行一次ProcessTerms()检测，如果成立则wieght权重值减1倍. if hasAntiAffinityConstraints { terms := affinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, pod, existingPod, existingPodNode, -1) } //如果\"亲和目标Pod\"存在亲和约束，则反过来与\"需被调度的Pod\"和\"亲和目标Node\"进行一次ProcessTerms()检测，如果成立则wieght权重值加1倍. if existingHasAffinityConstraints { if ipa.hardPodAffinityWeight > 0 { terms := existingPodAffinity.PodAffinity.RequiredDuringSchedulingIgnoredDuringExecution for _, term := range terms { pm.processTerm(&term, existingPod, pod, existingPodNode, float64(ipa.hardPodAffinityWeight)) } } terms := existingPodAffinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, existingPod, pod, existingPodNode, 1) } // 如果\"亲和目标Pod\"存在反亲和约束，则反过来与\"需被调度的Pod\"和\"亲和目标Node\"进行一次ProcessTerms()检测，如果成立则wieght权重值减1倍. if existingHasAntiAffinityConstraints { terms := existingPodAffinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, existingPod, pod, existingPodNode, -1) } return nil } ③ processNode 如果\"被调度pod\"未定义亲和配置，则检测潜在Nodes的亲和性定义. pkg/scheduler/algorithm/priorities/interpod_affinity.go:193 processNode := func(i int) { nodeInfo := nodeNameToInfo[allNodeNames[i]] if nodeInfo.Node() != nil { if hasAffinityConstraints || hasAntiAffinityConstraints { // We need to process all the nodes. for _, existingPod := range nodeInfo.Pods() { if err := processPod(existingPod); err != nil { pm.setError(err) } } } else { for _, existingPod := range nodeInfo.PodsWithAffinity() { if err := processPod(existingPod); err != nil { pm.setError(err) } } } } } ④ 最后的得分fscore计算公式： // 10 * (node权重累计值 - 最小权重得分值) / (最大权重得分值 - 最小权重得分值) fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) const ( // MaxPriority defines the max priority value. MaxPriority = 10 ) 4. Service亲和性 在default调度器代码内并未注册此预选策略，仅有代码实现。连google/baidu上都无法查询到相关使用案例，配置用法不予分析，仅看下面源码详细分析。 代码场景应用注释译文： 一个服务的第一个Pod被调度到带有Label “region=foo”的Nodes（资源集群）上， 那么其服务后面的其它Pod都将调度至Label “region=foo”的Nodes。 4.1. Serice亲和性预选策略checkServiceAffinity 通过NewServiceAffinityPredicate()创建一个ServiceAffinity类对象，并返回两个预选策略所必须的处理Func: affinity.checkServiceAffinity 基于预选元数据Meta，对被调度的pod检测Node是否满足服务亲和性. affinity.serverAffinityMetadataProducer 基于预选Meta的pod信息，获取服务信息和在相同NameSpace下的的Pod列表，供亲和检测时使用。 后面将详述处理func pkg/scheduler/algorithm/predicates/predicates.go:955 func NewServiceAffinityPredicate(podLister algorithm.PodLister, serviceLister algorithm.ServiceLister, nodeInfo NodeInfo, labels []string) (algorithm.FitPredicate, PredicateMetadataProducer) { affinity := &ServiceAffinity{ podLister: podLister, serviceLister: serviceLister, nodeInfo: nodeInfo, labels: labels, } return affinity.checkServiceAffinity, affinity.serviceAffinityMetadataProducer } affinity.serverAffinityMetadataProducer() 输入：predicateMateData 返回：services 和 pods 基于预选MetaData的pod信息查询出services 基于预选MetaData的pod Lables获取所有匹配的pods,且过滤掉仅剩在同一个Namespace的pods。 pkg/scheduler/algorithm/predicates/predicates.go:934 func (s *ServiceAffinity) serviceAffinityMetadataProducer(pm *predicateMetadata) { if pm.pod == nil { klog.Errorf(\"Cannot precompute service affinity, a pod is required to calculate service affinity.\") return } pm.serviceAffinityInUse = true var errSvc, errList error // 1.基于预选MetaData的pod信息查询services pm.serviceAffinityMatchingPodServices, errSvc = s.serviceLister.GetPodServices(pm.pod) // 2.基于预选MetaData的pod Lables获取所有匹配的pods selector := CreateSelectorFromLabels(pm.pod.Labels) allMatches, errList := s.podLister.List(selector) // In the future maybe we will return them as part of the function. if errSvc != nil || errList != nil { klog.Errorf(\"Some Error were found while precomputing svc affinity: \\nservices:%v , \\npods:%v\", errSvc, errList) } // 3.过滤掉仅剩在同一个Namespace的pods pm.serviceAffinityMatchingPodList = FilterPodsByNamespace(allMatches, pm.pod.Namespace) } affinity.checkServiceAffinity() 基于预处理的MetaData，对被调度的pod检测Node是否满足服务亲和性。 最终的亲和检测Labels: ​ Final affinityLabels =（A ∩ B）+ （B ∩ C） 与 node.Labels 进行Match计算 //∩交集符号 A: 需被调度pod的NodeSelector配置 B: 需被调度pod定义的服务亲和affinityLabels配置 C: 被选定的亲和目标Node的Lables pkg/scheduler/algorithm/predicates/predicates.go:992 func (s *ServiceAffinity) checkServiceAffinity(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { var services []*v1.Service var pods []*v1.Pod if pm, ok := meta.(*predicateMetadata); ok && (pm.serviceAffinityMatchingPodList != nil || pm.serviceAffinityMatchingPodServices != nil) { services = pm.serviceAffinityMatchingPodServices pods = pm.serviceAffinityMatchingPodList } else { // Make the predicate resilient in case metadata is missing. pm = &predicateMetadata{pod: pod} s.serviceAffinityMetadataProducer(pm) pods, services = pm.serviceAffinityMatchingPodList, pm.serviceAffinityMatchingPodServices } // 筛选掉存在于Node（nodeinfo）上pods，且与之进行podKey比对不相等的pods。 ① filteredPods := nodeInfo.FilterOutPods(pods) node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } // affinityLabes交集 ==（A ∩ B） // A：被调度pod的NodeSelector定义 B：定义的亲和性Labels ② affinityLabels := FindLabelsInSet(s.labels, labels.Set(pod.Spec.NodeSelector)) // Step 1: If we don't have all constraints, introspect nodes to find the missing constraints. if len(s.labels) > len(affinityLabels) { if len(services) > 0 { if len(filteredPods) > 0 { //\"被选定的亲和Node\" //基于第一个filteredPods获取Node信息 nodeWithAffinityLabels, err := s.nodeInfo.GetNodeInfo(filteredPods[0].Spec.NodeName) if err != nil { return false, nil, err } // 输入：交集Labels、服务亲和Labels、被选出的亲和Node Lables // affinityLabels = affinityLabels + 交集（B ∩ C） // B: 服务亲和Labels C:被选出的亲和Node的Lables ③ AddUnsetLabelsToMap(affinityLabels, s.labels, labels.Set(nodeWithAffinityLabels.Labels)) } } } // 进行一次最终的匹配（affinityLabels 与 被检测亲和的node.Labels ） ④ if CreateSelectorFromLabels(affinityLabels).Matches(labels.Set(node.Labels)) { return true, nil, nil } return false, []algorithm.PredicateFailureReason{ErrServiceAffinityViolated}, nil } ① FilterOutPods() 筛选掉存在于Node（nodeinfo）上pods，且与之进行podKey比对不相等的pods filteredPods = 未在Node上的pods + 在node上但podKey相同的pods pkg/scheduler/cache/node_info.go:656 func (n *NodeInfo) FilterOutPods(pods []*v1.Pod) []*v1.Pod { //获取Node的详细信息 node := n.Node() if node == nil { return pods } filtered := make([]*v1.Pod, 0, len(pods)) for _, p := range pods { //如果pod（亲和matched）的NodeName 不等于Spec配置的nodeNmae (即pod不在此Node上)，将pod放入filtered. if p.Spec.NodeName != node.Name { filtered = append(filtered, p) continue } //如果在此Node上，则获取podKey（pod.UID） //遍历此Node上所有的目标Pods，获取每个podKey进行与匹配pod的podkey是否相同， //相同则将pod放入filtered并返回 podKey, _ := GetPodKey(p) for _, np := range n.Pods() { npodkey, _ := GetPodKey(np) if npodkey == podKey { filtered = append(filtered, p) break } } } return filtered } ② FindLabelsInSet() 参数一： (B)定义的亲和性Labels配置 参数二： (A)被调度pod的定义NodeSelector配置Selector 检测存在的于NodeSelector的亲和性Labels配置，则取两者的交集部分. （A ∩ B） pkg/scheduler/algorithm/predicates/utils.go:26 func FindLabelsInSet(labelsToKeep []string, selector labels.Set) map[string]string { aL := make(map[string]string) for _, l := range labelsToKeep { if selector.Has(l) { aL[l] = selector.Get(l) } } return aL } ③ AddUnsetLabelsToMap() 参数一： (N)在FindLabelsInSet()计算出来的交集Labels 参数二： (B)定义的亲和性Labels配置 参数三： (C)\"被选出的亲和Node\"上的Lables 检测存在的于\"被选出的亲和Node\"上的亲和性配置Labels，则取两者的交集部分存放至N. (B ∩ C)=>N pkg/scheduler/algorithm/predicates/utils.go:37 // 输入：交集Labels、服务亲和Labels、被选出的亲和Node Lables // 填充：Labels交集 ==（B ∩ C） B: 服务亲和Labels C:被选出的亲和Node Lables func AddUnsetLabelsToMap(aL map[string]string, labelsToAdd []string, labelSet labels.Set) { for _, l := range labelsToAdd { // 如果存在则不作任何操作 if _, exists := aL[l]; exists { continue } // 反之，计算包含的交集部分 C ∩ B if labelSet.Has(l) { aL[l] = labelSet.Get(l) } } } ④ CreateSelectorFromLabels().Match() 返回labels.Selector对象 pkg/scheduler/algorithm/predicates/utils.go:62 func CreateSelectorFromLabels(aL map[string]string) labels.Selector { if aL == nil || len(aL) == 0 { return labels.Everything() } return labels.Set(aL).AsSelector() } End Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-18 15:55:20 专题-亲和性调度(Author - XiaoYang)1. 简介1.1. 约束调度1.2. Labels.selector标签选择器2. Node亲和性2.1. Node亲和性预选策略MatchNodeSelectorPred2.2. Node亲和性优选策略NodeAffinityPriority3. Pod亲和性3.1. Pod亲和性预选策略MatchInterPodAffinityPred3.2. Pod亲和性优选策略InterPodAffinityPriority4. Service亲和性4.1. Serice亲和性预选策略checkServiceAffinity"},"core/controller-manager/":{"url":"core/controller-manager/","title":"controller-manager","keywords":"","body":"controller-manager Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-18 09:43:03 "},"core/controller-manager/controller.html":{"url":"core/controller-manager/controller.html","title":"控制器概述","keywords":"","body":"控制器概述 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-16 18:05:45 "},"core/apiserver/":{"url":"core/apiserver/","title":"apiserver","keywords":"","body":"apiserver Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/proxy/":{"url":"core/proxy/","title":"proxy","keywords":"","body":"proxy Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/kubelet/":{"url":"core/kubelet/","title":"kubelet","keywords":"","body":"kubelet Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"around/":{"url":"around/","title":"概述","keywords":"","body":"周边项目源码分析 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-17 20:22:48 "},"around/client-go/":{"url":"around/client-go/","title":"client-go","keywords":"","body":"client-go client-go 部分我步打算从头到尾一点点讲。在核心组件源码的分析过程中有用到一些 client-go 中的关键特性时咱以专题形式逐渐添加进来。 1. 本章规划 Informer Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-17 20:21:41 client-go1. 本章规划"},"around/client-go/informer.html":{"url":"around/client-go/informer.html","title":"Informer机制","keywords":"","body":"Informer机制 概述 架构概览 SharedInformerFactory 同质的方法 ForResource()方法 internalinterfaces.SharedInformerFactory sharedInformerFactory SharedIndexInformer indexer reflector ResourceEventHandler controller controller.Run() sharedIndexInformer.HandleDeltas() processor sharedProcessor.run() sharedIndexInformer.Run() processorListener.run() listerwatcher 小结 1. 概述 讲 Informer 还是比较有压力的，client-go 中的逻辑确实有点复杂，我甚至怀疑有“炫技”的成分。Informer 在很多组件的源码中可以看到，尤其是 kube-controller-manager (写这篇文章时我已经基本写完 kube-scheduler 的源码分析，准备着手写 kube-controller-manager 了，鉴于 controlelr 和 client-go 关联太大，跳过来先讲讲 Informer). Informer 是 client-go 中一个比较核心的工具，通过 Informer 我们可以轻松 List/Get 某个资源对象，可以监听资源对象的各种事件(比如创建和删除)然后触发回调函数，让我们能够在各种事件发生的时候能够作出相应的逻辑处理。举个例字，当 pod 数量变化的时候 deployment 是不是需要判断自己名下的 pod 数量是否还和预期的一样？如果少了是不是要考虑创建？ 2. 架构概览 如上图，Informer 可以 watch API Server，监听各种事件，然后回调事件 handler。这些事件 handler 可以做一些简单的过滤，最终要将 item 放到 workequeue 中，这个 workerqueue 也是 client-go 提供的工具。最终用户写的 controller 负责启动 worker 去消费这 workqueue 中的 item. 3. SharedInformerFactory SharedInformerFactory 提供所有 API group 资源的 shared informers，也就是说通过这个 factory 可以使用 DeploymentInformer、ConfigMapInformer 等等各种 Informer，从而能够实现针对各种资源的逻辑处理。 informers/factory.go:185 type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh 这个 interface 我们关注3个点： internalinterfaces.SharedInformerFactory接口 ForResource()方法 其他方法的类型 3.1. 同质的方法 我们先看第三点，找个特例，从这个接口的一个方法往里面看一下类型含义，比如Apps() apps.Interface吧： informers/apps/interface.go:29 type Interface interface { // V1 provides access to shared informers for resources in V1. V1() v1.Interface // V1beta1 provides access to shared informers for resources in V1beta1. V1beta1() v1beta1.Interface // V1beta2 provides access to shared informers for resources in V1beta2. V1beta2() v1beta2.Interface } 很自然我们想到要继续看v1.Interface： informers/apps/v1/interface.go:26 type Interface interface { // ControllerRevisions returns a ControllerRevisionInformer. ControllerRevisions() ControllerRevisionInformer // DaemonSets returns a DaemonSetInformer. DaemonSets() DaemonSetInformer // Deployments returns a DeploymentInformer. Deployments() DeploymentInformer // ReplicaSets returns a ReplicaSetInformer. ReplicaSets() ReplicaSetInformer // StatefulSets returns a StatefulSetInformer. StatefulSets() StatefulSetInformer } DeploymentInformer 又是什么类型呢？ informers/apps/v1/deployment.go:36 type DeploymentInformer interface { Informer() cache.SharedIndexInformer Lister() v1.DeploymentLister } 可以看到这个 interface 的两个方法的特点，这个接口要提供的是针对 Deployments 的 shared informer 和 lister. 我们先不纠结细节，到这里我们先理解SharedInformerFactory 提供所有 API group 资源的 shared informers这句话。 3.2. ForResource()方法 这个方法返回指定类型的 shared informer 的通用访问方式，从实现中可以看到一些端倪： informers/generic.go:80 func (f *sharedInformerFactory) ForResource(resource schema.GroupVersionResource) (GenericInformer, error) { switch resource { // Group=admissionregistration.k8s.io, Version=v1alpha1 case v1alpha1.SchemeGroupVersion.WithResource(\"initializerconfigurations\"): return &genericInformer{resource: resource.GroupResource(), informer: f.Admissionregistration().V1alpha1().InitializerConfigurations().Informer()}, nil // …… } 这里的返回值是 GenericInformer 类型，很简洁： informers/generic.go:58 type GenericInformer interface { Informer() cache.SharedIndexInformer Lister() cache.GenericLister } 3.3. internalinterfaces.SharedInformerFactory informers/internalinterfaces/factory_interfaces.go:34 type SharedInformerFactory interface { Start(stopCh 这里的 InformerFor() 方法和前面的 ForResource() 有点像，这里的返回值是 SharedIndexInformer，GenericInformer 的 Informer() 方法返回值也是 SharedIndexInformer： tools/cache/shared_informer.go:66 type SharedIndexInformer interface { SharedInformer // AddIndexers add indexers to the informer before it starts. AddIndexers(indexers Indexers) error GetIndexer() Indexer } 3.4. sharedInformerFactory sharedInformerFactory 对象是 SharedInformerFactory 接口的具体实现，从这个 struct 的属性中我们可以看到一些有用的信息： informers/factory.go:53 type sharedInformerFactory struct { client kubernetes.Interface namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc lock sync.Mutex defaultResync time.Duration customResync map[reflect.Type]time.Duration informers map[reflect.Type]cache.SharedIndexInformer startedInformers map[reflect.Type]bool } 这里主要注意 client 和 informers，client 先不细说，大家从字面理解，当作一个可以和 api server 交互(CURD)的工具先就行。informers map[reflect.Type]cache.SharedIndexInformer明显是存放了多个不同类型的 informers，这个 map 的 key 表达一种 obj 的类型，value 是 SharedIndexInformer，后面我们会讲。 4. SharedIndexInformer 看 client-go 的过程中我一直在想到底哪个对象最能代表 Informer，后来觉得 SharedIndexInformer 应该可以被认为就是广义的 Informer 了。 我们在前面看到 GenericInformer 的代码，再附加对应 struct 贴一份： type GenericInformer interface { Informer() cache.SharedIndexInformer Lister() cache.GenericLister } type genericInformer struct { informer cache.SharedIndexInformer resource schema.GroupResource } 我们编码的时候直接使用的都是 SharedInformerFactory，往里面跟可以认为 GenericInformer 是第一层，这个接口的方法很清晰表达了意图。这里涉及到 informer+lister，我们一一来看。 SharedIndexInformer 的定义如下： tools/cache/shared_informer.go:66 type SharedIndexInformer interface { SharedInformer AddIndexers(indexers Indexers) error GetIndexer() Indexer } 这里包了一个 Interface： tools/cache/shared_informer.go:43 type SharedInformer interface { // 留意这个方法 AddEventHandler(handler ResourceEventHandler) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) GetStore() Store GetController() Controller Run(stopCh 从函数名得不到太多直观的信息，我们从 SharedIndexInformer 的实现 sharedIndexInformer 入手： tools/cache/shared_informer.go:127 type sharedIndexInformer struct { indexer Indexer controller Controller processor *sharedProcessor cacheMutationDetector CacheMutationDetector listerWatcher ListerWatcher objectType runtime.Object resyncCheckPeriod time.Duration defaultEventHandlerResyncPeriod time.Duration clock clock.Clock started, stopped bool startedLock sync.Mutex blockDeltas sync.Mutex } 从 sharedIndexInformer 的属性中可以看到几个实实在在的对象： indexer controller processor listerWatcher 4.1. indexer Indexer 接口提供了各种 index 函数，让我们在 list 一个对象时可以使用这些索引函数： tools/cache/index.go:27 type Indexer interface { Store Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(indexName string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers AddIndexers(newIndexers Indexers) error } 这个接口的实现是 cache： tools/cache/store.go:112 type cache struct { cacheStorage ThreadSafeStore keyFunc KeyFunc } 另外我们注意到包了一个接口 Store： type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) GetByKey(key string) (item interface{}, exists bool, err error) Replace([]interface{}, string) error Resync() error } Store 是一个一般对象的存储接口，Reflector(后面介绍)知道怎样 watch server 然后更新 store. Reflector 能够将 store 当作一个本地缓存系统，进而以类似队列的方式工作(队列中存的是等待被处理的对象)。 我们来看 Store 接口的一个实现： type DeltaFIFO struct { items map[string]Deltas queue []string //…… } 4.2. reflector 前面说到 Store 要给 Reflector 服务，我们看一下 Reflector 的定义： tools/cache/reflector.go:47 type Reflector struct { name string metrics *reflectorMetrics expectedType reflect.Type // The destination to sync up with the watch source store Store // listerWatcher is used to perform lists and watches. listerWatcher ListerWatcher // …… } Reflector 要做的事情是 watch 一个指定的资源，然后将这个资源的变化反射到给定的store中。很明显这里的两个属性 listerWatcher 和 store 就是这些逻辑的关键。 我们简单看一下往 store 中添加数据的代码： tools/cache/reflector.go:324 switch event.Type { case watch.Added: err := r.store.Add(event.Object) // …… case watch.Modified: err := r.store.Update(event.Object) // …… case watch.Deleted: // …… err := r.store.Delete(event.Object) 这个 store 一般用的是 DeltaFIFO，到这里大概就知道 Refactor 从 API Server watch 资源，然后写入 DeltaFIFO 的过程了，大概长这个样子： 然后我们关注一下 DeltaFIFO 的 knownObjects 属性，在创建一个 DeltaFIFO 实例的时候有这样的逻辑： tools/cache/delta_fifo.go:59 func NewDeltaFIFO(keyFunc KeyFunc, knownObjects KeyListerGetter) *DeltaFIFO { f := &DeltaFIFO{ items: map[string]Deltas{}, queue: []string{}, keyFunc: keyFunc, knownObjects: knownObjects, } f.cond.L = &f.lock return f } 这里接收了 KeyListerGetter 类型的 knownObjects，继续往前跟可以看到我们前面提到的 SharedIndexInformer 的初始化逻辑中将 indexer 对象当作了这里的 knownObjects 的实参： tools/cache/shared_informer.go:192 fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, s.indexer) s.indexer 来自于：NewSharedIndexInformer() 函数的逻辑： func NewSharedIndexInformer(lw ListerWatcher, objType runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer { realClock := &clock.RealClock{} sharedIndexInformer := &sharedIndexInformer{ processor: &sharedProcessor{clock: realClock}, indexer: NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers), listerWatcher: lw, objectType: objType, resyncCheckPeriod: defaultEventHandlerResyncPeriod, defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod, cacheMutationDetector: NewCacheMutationDetector(fmt.Sprintf(\"%T\", objType)), clock: realClock, } return sharedIndexInformer } 这里的 NewIndexer() 函数中就可以看到我们前面提到的 Indexer 接口的实现 cache 对象了： !FILENMAE tools/cache/store.go:239 func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer { return &cache{ cacheStorage: NewThreadSafeStore(indexers, Indices{}), keyFunc: keyFunc, } } Ok，我们可以基于前面的图加一个框框了： 4.3. ResourceEventHandler 在 SharedInformer 接口中有一个方法AddEventHandler(handler ResourceEventHandler)，我们看一下这个方法的一些细节。先来看 ResourceEventHandler 接口的定义： tools/cache/controller.go:177 type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } // adaptor type ResourceEventHandlerFuncs struct { AddFunc func(obj interface{}) UpdateFunc func(oldObj, newObj interface{}) DeleteFunc func(obj interface{}) } ResourceEventHandler 要做的事情是 handle 一个资源对象的事件通知，在这个资源对象发生增加、修改、删除的时候分别对应上面3个方法的逻辑。下面在 processor 部分我们继续看 ResourceEventHandler. 4.4. controller controller 对应这里的 Controller 接口： tools/cache/controller.go:82 type Controller interface { Run(stopCh 这里有个Run()方法比较显眼，我们看一下 sharedIndexInformer 对 Run() 方法的实现： tools/cache/shared_informer.go:189 func (s *sharedIndexInformer) Run(stopCh 关注这里基于 Config 创建了一个 Controller 赋值给 s.controller，然后调用了这个 s.controller.Run() 方法。我们看一下 New 里面是什么： tools/cache/controller.go:89 // New makes a new Controller from the given Config. func New(c *Config) Controller { ctlr := &controller{ config: *c, clock: &clock.RealClock{}, } return ctlr } 这里的 controller 类型是： tools/cache/controller.go:75 type controller struct { config Config reflector *Reflector reflectorMutex sync.RWMutex clock clock.Clock } 4.4.1. controller.Run() 我们接着关注这个 controller 是怎么实现 Run() 方法的： tools/cache/controller.go:100 func (c *controller) Run(stopCh 这个 loop 是用来消费 queue 的： tools/cache/controller.go:148 func (c *controller) processLoop() { for { // 这里的 Pop() 明显是阻塞式的 // type PopProcessFunc func(interface{}) error // PopProcessFunc 用于处理 queue 中 pop 出来的 element obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { if err == FIFOClosedError { return } if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } 这里的 PopProcessFunc 可能会让人一时摸不着头脑，其实这这值是一个函数类型func(interface{}) error，这里PopProcessFunc(c.config.Process)也就是把c.config.Process转为了PopProcessFunc类型而已。 我们在前面有贴 sharedIndexInformer.Run() 这个函数，里面的Process: s.HandleDeltas,这一行其实就交代了这里的 PopProcessFunc 类型实例来源。 4.4.2. sharedIndexInformer.HandleDeltas() tools/cache/shared_informer.go:344 func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest // 循环处理这个对象的一系列状态 for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil && exists { if err := s.indexer.Update(d.Object); err != nil { return err } // distribute s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } // distribute s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } // distribute s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 先关注这里的 distribute 过程，注意到这个 distribute 的参数是 xxxNotification，下面 processor 部分会讲到这些信号被处理的逻辑。 tools/cache/shared_informer.go:400 func (p *sharedProcessor) distribute(obj interface{}, sync bool) { p.listenersLock.RLock() defer p.listenersLock.RUnlock() if sync { for _, listener := range p.syncingListeners { // add listener.add(obj) } } else { for _, listener := range p.listeners { // add listener.add(obj) } } } tools/cache/shared_informer.go:506 func (p *processorListener) add(notification interface{}) { p.addCh 这里的 p.addCh 接收到信号，也就是下面 processor 部分的逻辑processorListener.pop()逻辑的起点。 4.5. processor 在 sharedIndexInformer 对象中有一个属性processor *sharedProcessor，这个 sharedProcessor 类型定义如下： tools/cache/shared_informer.go:375 type sharedProcessor struct { listenersStarted bool listenersLock sync.RWMutex listeners []*processorListener syncingListeners []*processorListener clock clock.Clock wg wait.Group } 这里的重点明显是 listeners 属性了，我们继续看 listeners 的类型中 processorListener 的定义： tools/cache/shared_informer.go:466 type processorListener struct { nextCh chan interface{} addCh chan interface{} handler ResourceEventHandler // …… } 这里有一个我们前面提到的 handler，下面结合在一起跟一下handler 方法调用逻辑。 4.5.1. sharedProcessor.run() 从 processor 的 run() 方法开始看： tools/cache/shared_informer.go:415 func (p *sharedProcessor) run(stopCh 撇开细节，可以看到这里调用了内部所有 listener 的 run() 和 pop() 方法。 4.5.2. sharedIndexInformer.Run() 我们前面写 controller 时提到过这个Run() ，现在只关注一点，sharedIndexInformer 的 run 会调用到s.processor.run，也就是上面写的 sharedProcessor.run(). 4.5.3. processorListener.run() sharedProcessor.run()往里调到了 processorListener.run() 和 processorListener.pop()，先看一下这个 run 做了什么： tools/cache/shared_informer.go:540 func (p *processorListener) run() { stopCh := make(chan struct{}) wait.Until(func() { // 一分钟执行一次这个 func() // 一分钟内的又有几次重试 err := wait.ExponentialBackoff(retry.DefaultRetry, func() (bool, error) { // 等待信号 nextCh for next := range p.nextCh { // notification 是 next 的实际类型 switch notification := next.(type) { // update case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) // add case addNotification: p.handler.OnAdd(notification.newObj) // delete case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fmt.Errorf(\"unrecognized notification: %#v\", next)) } } return true, nil }) if err == nil { close(stopCh) } }, 1*time.Minute, stopCh) } 这个 run 过程不复杂，等待信号然后调用 handler 的增删改方法做对应的处理逻辑。case 里的 Notification 再看一眼： tools/cache/shared_informer.go:176 type updateNotification struct { oldObj interface{} newObj interface{} } type addNotification struct { newObj interface{} } type deleteNotification struct { oldObj interface{} } 另外注意到for next := range p.nextCh是下面的 case 执行的前提，也就是说触发点是 p.nextCh，我们接着看 pop 过程( pod的代码花了我不少时间，这里的逻辑不简单)： tools/cache/shared_informer.go:510 func (p *processorListener) pop() { defer utilruntime.HandleCrash() defer close(p.nextCh) // Tell .run() to stop // 这个 chan 是没有初始化的 var nextCh chan 这里的 pop 逻辑的入口是，我们前面 controller 部分讲到了这个 addCh 的来源。继续看其他逻辑。 4.6. listerwatcher ListerWatcher 的出镜率还是挺高的，大家应该在很多文章里都有看到过这个词。我们先看一下接口定义： tools/cache/listwatch.go:31 type ListerWatcher interface { // List should return a list type object; List(options metav1.ListOptions) (runtime.Object, error) // Watch should begin a watch at the specified version. Watch(options metav1.ListOptions) (watch.Interface, error) } type ListFunc func(options metav1.ListOptions) (runtime.Object, error) type WatchFunc func(options metav1.ListOptions) (watch.Interface, error) type ListWatch struct { ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. DisableChunking bool } 从这些代码中我们能够体会到一些 ListerWatcher 的用意，但心里应该还是纠结的。我们看一下 deployment 的 list-watch. 我们是从 sharedIndexInformer 中看到有个属性 listerWatcher，DeploymentInformer 的创建代码如下： informers/apps/v1beta2/deployment.go:50 // 注意到返回值类型是 SharedIndexInformer，也就是说这里的初始化肯定需要给 listerWatcher 属性赋值 func NewDeploymentInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers) cache.SharedIndexInformer { return NewFilteredDeploymentInformer(client, namespace, resyncPeriod, indexers, nil) } func NewFilteredDeploymentInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer { return cache.NewSharedIndexInformer( // 这里初始化一个 ListWatch 类型实例 &cache.ListWatch{ // ListFunc 和 WatchFunc 的赋值 ListFunc: func(options v1.ListOptions) (runtime.Object, error) { if tweakListOptions != nil { tweakListOptions(&options) } // 逻辑是通过client的 xxx 实现的，这个 client 其实就是 Clientset return client.AppsV1beta2().Deployments(namespace).List(options) }, WatchFunc: func(options v1.ListOptions) (watch.Interface, error) { if tweakListOptions != nil { tweakListOptions(&options) } return client.AppsV1beta2().Deployments(namespace).Watch(options) }, }, &appsv1beta2.Deployment{}, resyncPeriod, indexers, ) } 以 list 为例，client.AppsV1beta2().Deployments(namespace).List(options)其实是 client 提供的逻辑了，我们可以看一下 List() 方法对应的接口： // DeploymentInterface has methods to work with Deployment resources. type DeploymentInterface interface { Create(*v1beta2.Deployment) (*v1beta2.Deployment, error) Update(*v1beta2.Deployment) (*v1beta2.Deployment, error) UpdateStatus(*v1beta2.Deployment) (*v1beta2.Deployment, error) Delete(name string, options *v1.DeleteOptions) error DeleteCollection(options *v1.DeleteOptions, listOptions v1.ListOptions) error Get(name string, options v1.GetOptions) (*v1beta2.Deployment, error) List(opts v1.ListOptions) (*v1beta2.DeploymentList, error) Watch(opts v1.ListOptions) (watch.Interface, error) Patch(name string, pt types.PatchType, data []byte, subresources ...string) (result *v1beta2.Deployment, err error) DeploymentExpansion } 顺着这个接口再往里跟很快就到 http 协议层了，要了然整个 list-watch 的原理还得结合 API Server 的代码，我们今天先不讲。 5. 小结 Informer 的实现还是有点复杂的，啃的过程中很容易一个不小心就被绕晕了。今天我们以开头的那张图结尾。以后讲Operator 的时候会基于这个图增加几个框框。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-17 20:19:11 Informer机制1. 概述2. 架构概览3. SharedInformerFactory3.1. 同质的方法3.2. ForResource()方法3.3. internalinterfaces.SharedInformerFactory3.4. sharedInformerFactory4. SharedIndexInformer4.1. indexer4.2. reflector4.3. ResourceEventHandler4.4. controller4.4.1. controller.Run()4.4.2. sharedIndexInformer.HandleDeltas()4.5. processor4.5.1. sharedProcessor.run()4.5.2. sharedIndexInformer.Run()4.5.3. processorListener.run()4.6. listerwatcher5. 小结"}}