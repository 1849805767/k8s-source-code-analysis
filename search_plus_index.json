{"./":{"url":"./","title":"前言","keywords":"","body":" 1、关于本书 本书将系统讲解kubernetes的核心组件源码，附带介绍相关的周边项目，比如client-go等。 建议通过公众号CloudGeek（微信上直接搜索可以找到）接收更新消息，通过github pages阅读本书。 寻找组织可以加我微信（ITNT01），一句话证明自己是源码学习者，然后我会拉你进群。 2、内容更新 本项目会不定期更新，一般会在每周五更新一节；更新内容将同步发到公众号CloudGeek、博客园CloudGeek等。细微的更新，比如错别字修改等不会同步到其他所有平台。 每次新章节会选择性提前发到微信群内，比如本周五要发出来的新内容周一可能就内部发到群里了，然后接受读者反馈，定稿后上传到github，然后同步到微信公众号等平台。 3、本教程适合谁读 任何对k8s源码感兴趣的人都可以看本教程，但是我建议你至少有golang项目开发经验，简单的golang开源项目的源码分析经验，k8s应用经验，对golang的基础特性和k8s的基础特性有一定的了解；不然直接上手看k8s源码会郁闷的。 4、版本说明 本书基于：v1.13版本源码讲解。 5、协议 本书使用Apache License 2.0协议，但是保留出版图书的权利。 6、贡献 欢迎参与本书编写！如果你对开源项目源码分析感兴趣，可以和我联系，如果你擅长某个项目，可以主导一个章节的编写。 如果想要提交pull request，建议先开一个issue，说明你要做什么修改，一般1天内我会回复issue，告诉你是否接受修改。但是得提前告诉你我是一个有洁癖的人，包括代码洁癖，文字洁癖等，所以请不要提交太随意的风格的内容哦～ 另外注意：一定先更新你的仓库到最新状态，然后编辑新内容，不然冲突很尴尬～ 7、FAQ 暂时我没有考虑增加评论功能，因为不可避免要增加三方插件，三方插件意味着用户需要注册登录等，体验不会太好。万一哪天这个插件倒闭了，就白忙活了。所以在每章开头我增加了一个FAQ部分，会把这一章中各个小节收到的部分问题汇总在开头的FAQ里。 大家在微信里问我的问题一般我都会耐心解答，但是和go语言本身语法相关的初级问题还是不希望经常遇到，因为我认为语言本身问题是很容易通过网络资料学习掌握的。另外有问题尽量抛到群里，私信多的话有时候我要1个问题很好几个人讲，工作量比较大。 8、支持本书 微信扫一扫，鼓励作者快快更新，写出更多优质的文章～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-18 16:56:36 "},"prepare/":{"url":"prepare/","title":"k8s源码分析准备工作","keywords":"","body":"k8s源码分析准备工作 1. 概述 准备工作分为2部分： 源码准备 测试环境搭建 源码准备阶段主要介绍k8s源码的获取与本地golang编译环境配置等；调试环境搭建是介绍如何准备一个k8s环境，用于后续组件的代码调试。调试环境可以在大体学习完一个组件源码后进行，用于验证自己在看源码过程中的一些想法和疑惑。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-24 18:16:02 k8s源码分析准备工作1. 概述"},"prepare/get-code.html":{"url":"prepare/get-code.html","title":"源码准备","keywords":"","body":"源码准备 环境准备 源码下载 源码编译 IDE 1. 环境准备 操作系统：我们使用Linux作为k8s源码分析和调试环境，fedora、centos、ubuntu都行，我这里使用fedora； golang相关： GOROOT=/usr/local/lib/golang GOPATH=/root/go go version go1.10.3 linux/amd64 2. 源码下载 mkdir -p /root/go/src/k8s.io cd /root/go/src/k8s.io/ git clone https://github.com/kubernetes/kubernetes.git 下载后本地目录： 3. 源码编译 我们先看一下几个主要的目录： 目录名 用途 cmd 每个组件代码入口（main函数） pkg 各个组件的具体功能实现 staging 已经分库的项目 vendor 依赖 考虑到国内网络环境等因素，我们不使用容器化方式构建。我们尝试在kubernetes项目cmd目录下构建一个组件（执行路径：/root/go/src/k8s.io/kubernetes/cmd/kube-scheduler）： 这里需要注意一下，如果报依赖错误，找不到k8s.io下的某些项目，就到vendor下看一下软链接是不是都还在，如下： 注意到k8s是使用这种方式解决k8s.io下的依赖问题的，如果我们在windows下下载的代码，然后copy到linux下，就很容易遇到这些软链接丢失的情况，导致go找不到依赖，编译失败。 4. IDE 我们使用Goland看代码： 最后，别忘了在正式研读源码前切换到release-1.13分支～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 11:08:05 源码准备1. 环境准备2. 源码下载3. 源码编译4. IDE"},"prepare/debug-environment.html":{"url":"prepare/debug-environment.html","title":"测试环境搭建-单节点","keywords":"","body":"测试环境搭建 （k8s-1.13版本单节点环境搭建） 概述 kubeadm简介 操作系统准备 系统信息 配置selinux和firewalld 系统参数与内核模块 配置yum源 禁用swap 安装docker 安装kubeadm、kubelet和kubectl 镜像准备 阿里云镜像加速器配置（可选） 镜像下载 安装k8s master 环境验证 1. 概述 大家注意哦，不一定要先搭建好环境再看源码，大可以先看一个组件，感觉差不多理解了，想要run一把，想要改几行试试的时候回过头来搭建k8s环境。 当然，大家开始看源码的时候，我相信各位都是搭建过不少次k8s集群，敲过N多次kubectl命令了，所以下面我不会解释太基础的命令是做什么的。 今天我们要做的是搭建一个单机版的k8s环境，用于后面的学习。虽然k8s的环境搭建没有openstack来的复杂，但是由于网络等乱七八糟的问题，在国内手动搭建一个五脏俱全的k8s也不算太容易，一个不小心就会被乱七八遭的障碍磨灭兴趣。今天看看我能不能让大家觉得这个过程无痛吧～ 2. kubeadm简介 选择一个好的工具很重要！大家在刚开始学习k8s的时候应该都用二进制文件一步一步搭建过集群吧，那个过程是不是很酸爽？手动一步一步搭建环境对于初学者来说确实大有裨益，可以了解各个组件的细节。我已经懒掉了，我决定从众多k8s自动化安装方案中选择一个来搭建这次的k8s环境。 kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，这不是一个单独的项目哦，我们在kubernetes源码里可以看到这个组件（kubernetes/cmd/kubeadm/）： kubeadm这个工具可以通过简单的kubeadm init和kubeadm join命令来创建一个kubernetes集群，kubeadm提供的其他命令都比较通俗易懂： kubeadm init 启动一个master节点； kubeadm join 启动一个node节点，加入master； kubeadm upgrade 更新集群版本； kubeadm config 从1.8.0版本开始已经用处不大，可以用来view一下配置； kubeadm token 管理kubeadm join的token； kubeadm reset 把kubeadm init或kubeadm join做的更改恢复原状； kubeadm version打印版本信息； kubeadm alpha预览一些alpha特性的命令。 关于kubeadm的成熟度官方有一个表格： Area Maturity Level Command line UX GA Implementation GA Config file API beta CoreDNS GA kubeadm alpha subcommands alpha High availability alpha DynamicKubeletConfig alpha Self-hosting alpha 主要特性其实都已经GA了，虽然还有一些小特性仍处于活跃开发中，但是整体已经接近准生产级别了。对于我们的场景来说用起来已经绰绰有余！ 3. 操作系统准备 我们先使用一个机子来装，后面需要拓展可以增加节点，使用kubeadm join可以很轻松扩展集群。 3.1. 系统信息 内存：2G CPU：2 磁盘：20G 系统版本和内核版本如下所示，大家不需要严格和我保持一致，不要使用太旧的就行了。 # cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) # uname -r 3.10.0-862.9.1.el7.x86_64 3.2. 配置selinux和firewalld # Set SELinux in permissive mode setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config # Stop and disable firewalld systemctl disable firewalld --now 3.3. 系统参数与内核模块 # 修改内核参数 cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 加载内核模块 modprobe br_netfilter lsmod | grep br_netfilter 3.4. 配置yum源 # base repo cd /etc/yum.repos.d mv CentOS-Base.repo CentOS-Base.repo.bak curl -o CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/gpgcheck=1/gpgcheck=0/g' /etc/yum.repos.d/CentOS-Base.repo # docker repo curl -o docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # k8s repo cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # update cache yum clean all yum makecache yum repolist 最终我们可以看到这些repo： 3.5. 禁用swap swapoff -a echo \"vm.swappiness = 0\">> /etc/sysctl.conf sysctl -p 如上配置，重启后swap又会被挂上，我们还需要注释掉/etc/fstab中的一行配置： 最终看到的结果是这样的（这个截图是下一节搭建集群的时候补到这里的，内存大小和单机的不一样）： 4. 安装docker 先看一下有哪些可用版本：yum list docker-ce --showduplicates | sort -r 我们选择一个版本安装： yum install docker-ce- 这里我选择18.06.3，所以我用的命令是： yum install docker-ce-18.06.3.ce 可以用rpm命令看一下docker-ce这个rpm包带来了哪些文件： 启动docker： systemctl enable docker --now 查看服务状态： systemctl status docker 5. 安装kubeadm、kubelet和kubectl kubeadm不管kubelet和kubectl，所以我们需要手动安装kubelet和kubectl： yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 如果你看到这个教程的时候yum源里已经有了1.14版本的kubeadm，那么要么你装新版本，要么通过上面装docker同样的方式指定1.13版本安装，我这里使用的是1.13.3。 最后启动kubelet： systemctl enable --now kubelet 6. 镜像准备 6.1. 阿里云镜像加速器配置（可选） 如果访问dockerhub速度不理想，可以选择配置阿里容器镜像加速器。 访问阿里云官网：https://www.aliyun.com 注册登录（可以使用支付宝直接登录） 找到镜像加速器配置页面（如果一时找不到，可以在页面上使用搜索功能）：https://cr.console.aliyun.com/cn-hangzhou/mirrors 页面上有详细的配置指南，对着操作就可以了 如果懒得登录阿里云折腾，也可以直接使用我的配置： mkdir -p /etc/docker tee /etc/docker/daemon.json 6.2. 镜像下载 然后我们就可以下载image，下载完记得打个tag： docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.13.3 k8s.gcr.io/kube-apiserver:v1.13.3 docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.13.3 k8s.gcr.io/kube-controller-manager:v1.13.3 docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.13.3 k8s.gcr.io/kube-scheduler:v1.13.3 docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.13.3 k8s.gcr.io/kube-proxy:v1.13.3 docker pull mirrorgooglecontainers/pause-amd64:3.1 docker tag mirrorgooglecontainers/pause-amd64:3.1 k8s.gcr.io/pause:3.1 docker pull mirrorgooglecontainers/etcd-amd64:3.2.24 docker tag mirrorgooglecontainers/etcd-amd64:3.2.24 k8s.gcr.io/etcd:3.2.24 docker pull coredns/coredns:1.2.6 docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6 7. 安装k8s master tip：下面的ip地址(192.168.19.100)大家需要替换成自己机器上的！ kubeadm init --pod-network-cidr=10.100.0.0/16 --service-cidr=10.101.0.0/16 --kubernetes-version=v1.13.3 --apiserver-advertise-address 192.168.19.100 --kubernetes-version: 用于指定k8s版本； --apiserver-advertise-address：用于指定kube-apiserver监听的ip地址； --pod-network-cidr：用于指定Pod的网络范围； --service-cidr：用于指定SVC的网络范围； 如上，跑kubeadm init命令后等几分钟。 如果遇到报错，对着错误信息修正一下。比如没有关闭swap会遇到error，系统cpu不够会遇到error，网络不通等等都会出错，仔细看一下错误信息一般都好解决～ 跑完上面的命令后，会看到类似如下的输出： Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.19.100:6443 --token i472cq.tr9a81qxnyqc5zj2 --discovery-token-ca-cert-hash sha256:acba957db29e0efbffe2cf4e484521b3b7e0f9d5c2ab7f9db68a5e31565d0d66 上面输出告诉我们还需要做一些工作： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # flannel (如果上面自定义了pod ip范围，这里需要修改flannel的configmap) kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml flannel的Network修改：如下图，默认是10.244.0.0/16，我这边pod的cidr是10.100.0.0/16，所以下面也改成10.100.0.0/16； 防止kube-flannel.yml下载地址变化，下面贴一份备用： --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - \"\" resources: - nodes verbs: - list - watch - apiGroups: - \"\" resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \"name\": \"cbr0\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.244.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-arm64 namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: arm64 tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-arm64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-arm64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-arm namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: arm tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-arm command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-arm command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-ppc64le namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: ppc64le tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-ppc64le command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-ppc64le command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-s390x namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: s390x tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-s390x command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-s390x command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 创建完后可以看到flannel的configmap如下： 稍等一会，应该可以看到node状态变成ready： # kubectl get node NAME STATUS ROLES AGE VERSION kube-master Ready master 23m v1.13.3 如果你的环境迟迟都是NotReady状态，可以kubectl get pod -n kube-system看一下pod状态，一般可以发现问题，比如flannel的镜像下载失败啦～ 当node Ready的时候，我们可以看到pod也全部ready了： 再看一下核心组件状态： 最后注意到kube-master这个node上有一个Taint： # kubectl describe node kube-master | grep Taints Taints: node-role.kubernetes.io/master:NoSchedule 默认master节点是不跑业务pod的，我们暂时只有一个node，所以先去掉这个Taint： # kubectl taint node kube-master node-role.kubernetes.io/master- node/kube-master untainted # kubectl describe node kube-master | grep Taints Taints: 8. 环境验证 我们来跑一个pod，证明环境正常可用了： 写一个yaml： apiVersion: apps/v1 kind: Deployment metadata: name: mytomcat spec: replicas: 1 selector: matchLabels: app: mytomcat template: metadata: name: mytomcat labels: app: mytomcat spec: containers: - name: mytomcat image: tomcat:8 ports: - containerPort: 8080 如上内容保存为tomcat-deploy.yaml，执行kubectl create -f tomcat-deploy.yaml，然后看pod状态： 确认了，是熟悉的Running，哈哈，基本算大功告成了！最后我们看一下tomcat服务能不能访问到： 很完美，如果加个svc配置，就能够通过浏览器访问到汤姆猫界面了！ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-18 16:42:10 测试环境搭建1. 概述2. kubeadm简介3. 操作系统准备3.1. 系统信息3.2. 配置selinux和firewalld3.3. 系统参数与内核模块3.4. 配置yum源3.5. 禁用swap4. 安装docker5. 安装kubeadm、kubelet和kubectl6. 镜像准备6.1. 阿里云镜像加速器配置（可选）6.2. 镜像下载7. 安装k8s master8. 环境验证"},"prepare/debug-environment-3node.html":{"url":"prepare/debug-environment-3node.html","title":"测试环境搭建-三节点","keywords":"","body":"测试环境搭建-三节点 （k8s-1.13版本三节点环境搭建） 概述 系统准备 镜像和rpms 安装master 添加node节点 安装flannel 环境验证 1. 概述 写在前面：本节不建议在未阅读上一节（单机版环境搭建）的情况下阅读。下面内容稍稍随意，上一节提过的就不重复了。另外都是使用kubeadm实现，没有本质区别，所以下文从简。 前面一节讲了单节点的环境搭建，在调试调度策略等场景的时候单节点不好说明问题，所以今天补一个3节点的集群搭建过程。1个笔记本搭建3节点确实有点压力，外加要源码编译，调试，着实卡到不行。大伙不需要一开始先折腾好环境。在看源码遇到困惑，需要上环境调试或者验证特性单机不够用时再倒腾吧。下附我当前渣渣笔记本配置： 2. 系统准备 和上一节同样的配置方式，这里不再赘述。我这里3个节点基本信息如下： ip hostname 用途 29.123.161.240 kube-master master 节点 29.123.161.207 kube-node1 node 节点 29.123.161.208 kube-node2 node 节点 每个节点的/etc/hosts配置： 3. 镜像和rpms 镜像和rpm包的获取方式和上一节一样，node节点并不需要安装和master一样的rpm包，也不需要全部的镜像，不过我贪方便，直接在3个节点放了一样的“包”；我用的是离线的虚拟机，所以是一下子拷贝了rpm包和镜像tar包这些进去，去区分还不如直接全部装，大家按需自己灵活决定～. 4. 安装master 运行init命令： kubeadm init --pod-network-cidr=10.100.0.0/16 --kubernetes-version=v1.13.3 --apiserver-advertise-address 29.123.161.240 --service-cidr=10.101.0.0/16 运行结束后我们看到如下输出： 对着输出信息初始化： 5. 添加node节点 在2个node节点执行同样的kube join命令（具体命令master安装完输出的信息里可以找到）： 6. 安装flannel 和上一节一样下载yaml文件，镜像可以提前下载好（如果网络不给力）。 yml文件内容如下： --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - \"\" resources: - nodes verbs: - list - watch - apiGroups: - \"\" resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \"name\": \"cbr0\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.244.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-arm64 namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: arm64 tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-arm64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-arm64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-arm namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: arm tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-arm command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-arm command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-ppc64le namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: ppc64le tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-ppc64le command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-ppc64le command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-s390x namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: s390x tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-s390x command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-s390x command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 执行kubectl create -f kube-flannel.yml（同样因为这里自定义了pod的cidr，所以这里需要修改flannel的yaml配置；如果已经创建了资源，同样可以通过修改configmap实现）； 对应的configmap资源： 稍等一会查看pod状态 查看node状态： 7. 环境验证 同样我们用tomcat镜像来测试： apiVersion: apps/v1 kind: Deployment metadata: name: tomcat spec: replicas: 2 selector: matchLabels: app: tomcat template: metadata: name: tomcat labels: app: tomcat spec: containers: - name: tomcat image: tomcat:8 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: tomcat-svc spec: selector: app: tomcat ports: - name: http port: 8080 targetPort: 8080 protocol: TCP 创建资源： 查看pod和svc： 通过svc访问tomcat服务： ok，3节点的环境验证各种特性基本都够用～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-18 16:42:10 测试环境搭建-三节点1. 概述2. 系统准备3. 镜像和rpms4. 安装master5. 添加node节点6. 安装flannel7. 环境验证"},"core/":{"url":"core/","title":"概述","keywords":"","body":"核心组件源码分析 1. 概述 核心组件的源码分析主要包括： scheduler apiserver proxy kubelet controller-manager 在分析第一个组件的时候会穿插一些整体性的介绍，比如源码组织啊、使用的一些三方库啊……；后面有些组件比较依赖其他较大的项目的，比如一个核心组件依赖于对client-go项目的理解，那就会先介绍client-go，当然client-go的介绍不会混在核心组件分析的章节中，我会单独分一个大类“周边项目源码分析”中。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:28:56 核心组件源码分析1. 概述"},"core/scheduler/":{"url":"core/scheduler/","title":"scheduler","keywords":"","body":"scheduler Scheduler部分我们先从设计原理上介绍，然后分析源码，再准备环境调试，最后修改一下源码，实现一个自己的调度器。 1. 本章规划 调度器设计 调度程序启动前逻辑 调度器框架 一般调度过程 预选过程 优选过程 抢占调度 2. FAQ 读者A提问：如果一个pod的资源占用只有100M，能够运行在一个node上，但是配置成了1000M，这个时候node上其实没有1000M，那么predicate过程还能不能过滤通过？ 回答：如果一个人需要100块钱，卡里有1000块钱，这时候找银行要10000块，银行会给吗？银行不会知道你实际需要多少，你告诉他10000，他就看你卡里有没有10000；同样对于k8s来说你配置了需要1000M，k8s就看node上有没有1000M，没有就调度失败。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-18 16:46:53 scheduler1. 本章规划2. FAQ"},"core/scheduler/design.html":{"url":"core/scheduler/design.html","title":"调度器设计","keywords":"","body":"调度器设计 概述 源码层级 调度算法 Predicates 和 priorities 策略 Scheduler 的拓展性 调度策略的修改 1. 概述 我们先整体了解一下Scheduler的设计原理，然后再看这些过程是如何用代码实现的。关于调度器的设计在官网有介绍，我下面结合官网给的说明，简化掉不影响理解的复杂部分，和大家介绍一下Scheduler的工作过程。 英文还可以的小伙伴们可以看一下官网的介绍先：scheduler.md 官网有一段描述如下： The Kubernetes scheduler runs as a process alongside the other master components such as the API server. Its interface to the API server is to watch for Pods with an empty PodSpec.NodeName, and for each Pod, it posts a binding indicating where the Pod should be scheduled. 简单翻译一下，也就是说Scheduler是一个跑在其他组件边上的独立程序，对接Apiserver寻找PodSpec.NodeName为空的Pod，然后用post的方式发送一个api调用，指定这些pod应该跑在哪个node上。 通俗地说，就是scheduler是相对独立的一个组件，主动访问api server，寻找等待调度的pod，然后通过一系列调度算法寻找哪个node适合跑这个pod，然后将这个pod和node的绑定关系发给api server，从而完成了调度的过程。 2. 源码层级 从高level看，scheduler的源码可以分为3层： cmd/kube-scheduler/scheduler.go: main() 函数入口位置，在scheduler过程开始被调用前的一系列初始化工作。 pkg/scheduler/scheduler.go: 调度框架的整体逻辑，在具体的调度算法之上的框架性的代码。 pkg/scheduler/core/generic_scheduler.go: 具体的计算哪些node适合跑哪些pod的算法。 3. 调度算法 调度过程整体如下图所示（官文里这个图没对齐，逼疯强迫症了！！！当然由于中文显示的问题，下图有中文的行也没法完全对齐，这个地方让我很抓狂。。。）： 对于一个给定的pod +---------------------------------------------+ | 可用于调度的nodes如下： | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 初步过滤: node 3 资源不足 +----------------------+----------------------+ | v +----------------------+----------------------+ | 剩下的nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 优先级算法计算结果: node 1: 分数=2 node 2: 分数=5 +----------------------+----------------------+ | v 选择分值最高的节点 = node 2 Scheduler为每个pod寻找一个适合其运行的node，大体分成三步： 通过一系列的“predicates”过滤掉不能运行pod的node，比如一个pod需要500M的内存，有些节点剩余内存只有100M了，就会被剔除； 通过一系列的“priority functions”给剩下的node排一个等级，分出三六九等，寻找能够运行pod的若干node中最合适的一个node； 得分最高的一个node，也就是被“priority functions”选中的node胜出了，获得了跑对应pod的资格。 4. Predicates 和 priorities 策略 Predicates是一些用于过滤不合适node的策略 . Priorities是一些用于区分node排名（分数）的策略（作用在通过predicates过滤的node上）. K8s默认内建了一些predicates 和 priorities 策略，官方文档介绍地址： scheduler_algorithm.md. Predicates 和 priorities 的代码分别在： pkg/scheduler/algorithm/predicates/predicates.go pkg/scheduler/algorithm/priorities. 5. Scheduler 的拓展性 我们可以选择哪些预置策略生效，也可以添加自己的策略。几个月前我司有个奇葩调度需求，当时我就是通过增加一个priorities策略，然后重新编译了一个Scheduler来实现的需求。 6. 调度策略的修改 默认调度策略是通过defaultPredicates() 和 defaultPriorities()函数定义的，源码在 pkg/scheduler/algorithmprovider/defaults/defaults.go，我们可以通过命令行flag --policy-config-file来覆盖默认行为。所以我们可以通过配置文件的方式或者修改pkg/scheduler/algorithm/predicates/predicates.go /pkg/scheduler/algorithm/priorities，然后注册到defaultPredicates()/defaultPriorities()来实现。配置文件类似下面这个样子： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\"}, {\"name\" : \"PodFitsResources\"}, {\"name\" : \"NoDiskConflict\"}, {\"name\" : \"NoVolumeZoneConflict\"}, {\"name\" : \"MatchNodeSelector\"}, {\"name\" : \"HostName\"} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1}, {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10, \"alwaysCheckAllPredicates\" : false } ok，看到这里大伙应该在流程上对Scheduler的原理有个感性的认识了，下一节我们就开始看一下Scheduler源码是怎么写的。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-27 16:24:20 调度器设计1. 概述2. 源码层级3. 调度算法4. Predicates 和 priorities 策略5. Scheduler 的拓展性6. 调度策略的修改"},"core/scheduler/before-scheduler-run.html":{"url":"core/scheduler/before-scheduler-run.html","title":"调度程序启动前逻辑","keywords":"","body":"调度程序启动前逻辑 概述 cobra和main cobra是啥 使用cobra Scheduler的main 1. 概述 前面提到过scheduler程序可以分为三层，第一层是调度器启动前的逻辑，包括命令行参数解析、参数校验、调度器初始化等一系列逻辑。这个部分我不会太详细地介绍，因为这些代码位于调度框架之前，相对比较枯燥无趣，讲多了磨灭大伙对源码的兴趣～ 2. cobra和main 剧透一下先，如果你之前没有用过cobra，那么在第一次见到cobra之后，很可能以后你自己写的程序，开发的小工具会全部变成cobra风格。我最近半年写的命令行程序就全部是基于cobra+pflag的。cobra有多优雅呢，且听我慢慢道来～ 2.1. cobra是啥 从github上我们可以找到这个项目，截至今天已经有上万个star，一百多个contributors，可见来头不小！Cobra官方描述是： Cobra is both a library for creating powerful modern CLI applications as well as a program to generate applications and command files. 也就是这个意思：Cobra既是一个创建强大的现代化命令行程序的库，又是一个用于生成应用和命令行文件的程序。有很多流行的Go项目用了Cobra，其中当然包括我们最最熟知的k8s和docker，大致列出来有这些： Kubernetes Hugo rkt etcd Moby (former Docker) Docker (distribution) OpenShift Delve GopherJS CockroachDB Bleve ProjectAtomic (enterprise) Giant Swarm's gsctl Nanobox/Nanopack rclone nehm Pouch 如果你是云计算方向的攻城狮，上面半数项目应该都耳熟能详～ 2.2. 使用cobra 下面我们实践一下cobra，先下载这个项目编译一下： # 如果你的网络很给力，那么下面这个命令就够了； go get -u github.com/spf13/cobra/cobra # 如果你的网络不给力，那就下载cobra的zip包，丢到GOPATH下对应目录，然后解决依赖，再build 于是我们得到了这样一个可执行文件及项目源码： 我们试一下这个命令：cobra init ${project-name} [root@farmer-hutao src]# cobra init myapp Your Cobra application is ready at /root/go/src/myapp Give it a try by going there and running `go run main.go`. Add commands to it by running `cobra add [cmdname]`. [root@farmer-hutao src]# ls myapp/ cmd LICENSE main.go [root@farmer-hutao src]# pwd /root/go/src 如上，本地可以看到一个main.go和一个cmd目录，这个cmd和k8s源码里的cmd是不是很像～ main.go里面的代码很精简，如下： main.go package main import \"myapp/cmd\" func main() { cmd.Execute() } 这里注意到调用了一个cmd的Execute()方法，我们继续看cmd是什么： 如上图，在main.go里面import了myapp/cmd，也就是这个root.go文件。所以Execute()函数就很好找了。在Execute里面调用了rootCmd.Execute()方法，这个rootCmd是*cobra.Command类型的。我们关注一下这个类型。 下面我们继续使用cobra命令给myapp添加一个子命令： 如上，我们的程序可以使用version子命令了！我们看一下源码发生了什么变化： 多了一个version.go，在这个源文件的init()函数里面调用了一个rootCmd.AddCommand(versionCmd)，这里可以猜到是根命令下添加一个子命令的意思，根命令表示的就是我们直接执行这个可执行文件，子命令就是version，放在一起的感觉就类似大家使用kubectl version的感觉。 另外注意到这里的Run属性是一个匿名函数，这个函数中输出了“version called”字样，也就是说我们执行version子命令的时候其实是调用到了这里的Run. 最后我们实践一下多级子命令： 套路也就这样，通过serverCmd.AddCommand(createCmd)调用后就能够把*cobra.Command类型的createCmd变成serverCmd的子命令了，这个时候我们玩起来就像kubectl get pods. 行，看到这里我们回头看一下scheduler的源码就能找到main的逻辑了。 3. Scheduler的main 我们打开文件：cmd/kube-scheduler/scheduler.go可以找到scheduler的main()函数，很简短，去掉枝干后如下： cmd/kube-scheduler/scheduler.go:34 func main() { command := app.NewSchedulerCommand() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } 看到这里猜都能猜到kube-scheduler这个二进制文件在运行的时候是调用了command.Execute()函数背后的那个Run，那个Run躲在command := app.NewSchedulerCommand()这行代码调用的NewSchedulerCommand()方法里，这个方法一定返回了一个*cobra.Command类型的对象。我们跟进去这个函数，看一下是不是这个样子： cmd/kube-scheduler/app/server.go:70 / NewSchedulerCommand creates a *cobra.Command object with default parameters func NewSchedulerCommand() *cobra.Command { cmd := &cobra.Command{ Use: \"kube-scheduler\", Long: `The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity. The scheduler needs to take into account individual and collective resource requirements, quality of service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, deadlines, and so on. Workload-specific requirements will be exposed through the API as necessary.`, Run: func(cmd *cobra.Command, args []string) { if err := runCommand(cmd, args, opts); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } return cmd } 如上，同样我先删掉了一些枝干代码，剩下的可以很清楚地看到，schduler启动时调用了runCommand(cmd, args, opts)，这个函数在哪里呢，继续跟一下： cmd/kube-scheduler/app/server.go:117 // runCommand runs the scheduler. func runCommand(cmd *cobra.Command, args []string, opts *options.Options) error { c, err := opts.Config() stopCh := make(chan struct{}) // Get the completed config cc := c.Complete() return Run(cc, stopCh) } 如上，可以看到这里是处理配置问题后调用了一个Run()函数，Run()的作用是基于给定的配置启动scheduler，它只会在出错时或者channel stopCh被关闭时才退出，代码主要部分如下： cmd/kube-scheduler/app/server.go:167 // Run executes the scheduler based on the given configuration. It only return on error or when stopCh is closed. func Run(cc schedulerserverconfig.CompletedConfig, stopCh 可以看到这里最终是要跑sched.Run()这个方法来启动scheduler，sched.Run()方法已经在pkg下，具体位置是pkg/scheduler/scheduler.go:276，也就是scheduler框架真正运行的逻辑了。于是我们已经从main出发，找到了scheduler主框架的入口，具体的scheduler逻辑我们下一讲再来仔细分析。 最后我们来看一下sched的定义，在linux里我们经常会看到一些软件叫做什么什么d，d也就是daemon，守护进程的意思，也就是一直跑在后台的一个程序。这里的sched也就是“scheduler daemon”的意思。sched的其实是*Scheduler类型，定义在： pkg/scheduler/scheduler.go:58 // Scheduler watches for new unscheduled pods. It attempts to find // nodes that they fit on and writes bindings back to the api server. type Scheduler struct { config *factory.Config } 如上，注释也很清晰，说Scheduler watch新创建的未被调度的pods，然后尝试寻找合适的node，回写一个绑定关系到api server.这里也可以体会到daemon的感觉，我们平时搭建的k8s集群中运行着一个daemon进程叫做kube-scheduler，这个一直跑着的进程做的就是上面注释里说的事情，在程序里面也就对应这样一个对象：Scheduler. Scheduler结构体中的Config对象我们再简单看一下： pkg/scheduler/factory/factory.go:96 // Config is an implementation of the Scheduler's configured input data. type Config struct { // It is expected that changes made via SchedulerCache will be observed // by NodeLister and Algorithm. SchedulerCache schedulerinternalcache.Cache // Ecache is used for optimistically invalid affected cache items after // successfully binding a pod Ecache *equivalence.Cache NodeLister algorithm.NodeLister Algorithm algorithm.ScheduleAlgorithm GetBinder func(pod *v1.Pod) Binder // PodConditionUpdater is used only in case of scheduling errors. If we succeed // with scheduling, PodScheduled condition will be updated in apiserver in /bind // handler so that binding and setting PodCondition it is atomic. PodConditionUpdater PodConditionUpdater // PodPreemptor is used to evict pods and update pod annotations. PodPreemptor PodPreemptor // NextPod should be a function that blocks until the next pod // is available. We don't use a channel for this, because scheduling // a pod may take some amount of time and we don't want pods to get // stale while they sit in a channel. NextPod func() *v1.Pod // SchedulingQueue holds pods to be scheduled SchedulingQueue internalqueue.SchedulingQueue } 如上，同样我只保留了一些好理解的字段，我们随便扫一下可以看到譬如：SchedulingQueue、NextPod、NodeLister这些很容易从字面上理解的字段，也就是Scheduler对象在工作（完成调度这件事）中需要用到的一些对象。 ok，下一讲我们开始聊Scheduler的工作过程！ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 11:13:36 调度程序启动前逻辑1. 概述2. cobra和main2.1. cobra是啥2.2. 使用cobra3. Scheduler的main"},"core/scheduler/scheduler-framework.html":{"url":"core/scheduler/scheduler-framework.html","title":"调度器框架","keywords":"","body":"调度器框架 写在前面 调度器启动运行 一个pod的调度流程 潜入第三层前的一点逻辑 1. 写在前面 今天我们从pkg/scheduler/scheduler.go出发，分析Scheduler的整体框架。前面讲Scheduler设计的时候有提到过源码的3层结构，pkg/scheduler/scheduler.go也就是中间这一层，负责Scheduler除了具体node过滤算法外的工作逻辑～ 这一层我们先尽可能找主线，顺着主线走通一遍，就像走一个迷宫，一条通路走出去后心里就有地了，但是迷宫中的很多角落是未曾涉足的。我们尽快走通主流程后，再就一些主要知识点专题攻破，比如k8s里面的List-Watch，Informer等好玩的东西。 2. 调度器启动运行 从goland的Structure中可以看到这个源文件(pkg/scheduler/scheduler.go)主要有这些对象： 大概浏览一下可以很快找到我们的第一个关注点应该是Scheduler这个struct和Scheduler的Run()方法： pkg/scheduler/scheduler.go:58 // Scheduler watches for new unscheduled pods. It attempts to find // nodes that they fit on and writes bindings back to the api server. type Scheduler struct { config *factory.Config } 这个struct在上一讲有跟到过，代码注释说的是： Scheduler watch新创建的未被调度的pods，然后尝试寻找合适的node，回写一个绑定关系到api server. 这个注释有个小问题就是用了复数形式，其实最后过滤出来的只有一个node；当然这种小问题知道就好，提到github上人家会觉得你在刷commit.接着往下看，Scheduler绑定了一个Run()方法，如下： pkg/scheduler/scheduler.go:276 // Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } 注释说这个函数开始watching and scheduling，也就是调度器主要逻辑了！注释后半段说到Run()方法起了一个goroutine后马上返回了，这个怎么理解呢？我们先看一下调用Run的地方： cmd/kube-scheduler/app/server.go:240 // Prepare a reusable runCommand function. run := func(ctx context.Context) { sched.Run() 可以发现调用了sched.Run()之后就在等待ctx.Done()了，所以Run中启动的goroutine自己不退出就ok. wait.Until这个函数做的事情是：每隔n时间调用f一次，除非channel c被关闭。这里的n就是0，也就是一直调用，前一次调用返回下一次调用就开始了。这里的f当然就是sched.scheduleOne，c就是sched.config.StopEverything. 3. 一个pod的调度流程 于是我们的关注点就转到了sched.scheduleOne这个方法上，看一下： scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. 注释里说scheduleOne实现1个pod的完整调度工作流，这个过程是顺序执行的，也就是非并发的。结合前面的wait.Until逻辑，也就是说前一个pod的scheduleOne一完成，一个return，下一个pod的scheduleOne立马接着执行！ 这里的串行逻辑也好理解，如果是同时调度N个pod，计算的时候觉得一个node很空闲，实际调度过去启动的时候发现别人的一群pod先起来了，端口啊，内存啊，全给你抢走了！所以这里的调度算法执行过程用串行逻辑很好理解。注意哦，调度过程跑完不是说要等pod起来，最后一步是写一个binding到apiserver，所以不会太慢。下面我们看一下scheduleOne的主要逻辑： pkg/scheduler/scheduler.go:513 func (sched *Scheduler) scheduleOne() { pod := sched.config.NextPod() suggestedHost, err := sched.schedule(pod) if err != nil { if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) } return } assumedPod := pod.DeepCopy() allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) err = sched.assume(assumedPod, suggestedHost) go func() { err := sched.bind(assumedPod, &v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) }() } 上面几行代码只保留了主干，对于我们理解scheduleOne的过程足够了，这里来个流程图吧： 不考虑scheduleOne的所有细节和各种异常情况，基本是上图的流程了，主流程的核心步骤当然是suggestedHost, err := sched.schedule(pod)这一行，这里完成了不需要抢占的场景下node的计算，我们耳熟能详的预选过程，优选过程等就是在这里面。 4. 潜入第三层前的一点逻辑 ok，这时候重点就转移到了suggestedHost, err := sched.schedule(pod)这个过程，强调一下这个过程是“同步”执行的。 pkg/scheduler/scheduler.go:290 // schedule implements the scheduling algorithm and returns the suggested host. func (sched *Scheduler) schedule(pod *v1.Pod) (string, error) { host, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) if err != nil { pod = pod.DeepCopy() sched.config.Error(pod, err) sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"%v\", err) sched.config.PodConditionUpdater.Update(pod, &v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, LastProbeTime: metav1.Now(), Reason: v1.PodReasonUnschedulable, Message: err.Error(), }) return \"\", err } return host, err } schedule方法很简短，我们关注一下第一行，调用sched.config.Algorithm.Schedule()方法，入参是pod和nodes，返回一个host，继续看一下这个Schedule方法： pkg/scheduler/algorithm/scheduler_interface.go:78 type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) Predicates() map[string]FitPredicate Prioritizers() []PriorityConfig } 发现是个接口，这个接口有4个方法，实现ScheduleAlgorithm接口的对象意味着知道如何调度pods到nodes上。默认的实现是pkg/scheduler/core/generic_scheduler.go:98 genericScheduler这个struct.我们先继续看一下ScheduleAlgorithm接口定义的4个方法： Schedule() //给定pod和nodes，计算出一个适合跑pod的node并返回； Preempt() //抢占 Predicates() //预选 Prioritizers() //优选 前面流程里讲到的sched.config.Algorithm.Schedule()也就是genericScheduler.Schedule()方法了，这个方法位于：pkg/scheduler/core/generic_scheduler.go:139一句话概括这个方法就是：尝试将指定的pod调度到给定的node列表中的一个，如果成功就返回这个node的名字。最后看一眼签名： func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) 从如参和返回值其实可以猜到很多东西，行，今天就到这里，具体的逻辑下回我们再分析～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 14:00:16 调度器框架1. 写在前面2. 调度器启动运行3. 一个pod的调度流程4. 潜入第三层前的一点逻辑"},"core/scheduler/generic-scheduler.html":{"url":"core/scheduler/generic-scheduler.html","title":"一般调度过程","keywords":"","body":"一般调度过程 进入Scheduler的第三层逻辑 Computing predicates Prioritizing Selecting host 1. 进入Scheduler的第三层逻辑 今天分析的代码，就已经算kube-scheduler的第三层逻辑了，我们要找到预选和优选的入口，讲完太长，干脆后面单独分2节讲预选和优选过程。所以本小节会比较简短哦～ 今天我们从pkg/scheduler/core/generic_scheduler.go:139开始，也就是从这个generic scheduler的Schedule()方法下手！ 我们依旧关心主干先，这个方法主要涉及的是预选过程+优选过程，看下主要代码： pkg/scheduler/core/generic_scheduler.go:139 func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { nodes, err := nodeLister.List() trace.Step(\"Computing predicates\") filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) trace.Step(\"Prioritizing\") priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) trace.Step(\"Selecting host\") return g.selectHost(priorityList) } 如上，我手一抖就删的只剩下这几行了，大伙应该从这不到十行的代码里找到3个步骤： \"Computing predicates\"：调用findNodesThatFit()方法； \"Prioritizing\"：调用PrioritizeNodes()方法； \"Selecting host\"：调用selectHost()方法。 接着当然是先浏览一下这3步分别完成了哪些工作咯～ 1.1. Computing predicates 这个过程的入口是： filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) 从变量命名上其实就可以猜到一大半，filteredNodes肯定就是过滤出来的nodes，也就是经受住了预选算法考验的node集合，我们从findNodesThatFit方法的函数签名中可以得到准确一些的信息： pkg/scheduler/core/generic_scheduler.go:389 func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) 入参是1个pod + 一堆node，返回值是一堆node（这个堆堆当然 1.2. Prioritizing Prioritizing的入口看着复杂一点： priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 注意到这里的返回值叫做priorityList，什么什么List也就是不止一个了，优选过程不是选出1个最佳节点吗？我们继续看： pkg/scheduler/core/generic_scheduler.go:624 func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) 首选关注返回值是什么意思： pkg/scheduler/api/types.go:305 type HostPriority struct { // Name of the host Host string // Score associated with the host Score int } // HostPriorityList declares a []HostPriority type. type HostPriorityList []HostPriority 看到这里就清晰了，原来有个HostPriority类型记录一个Host的名字和分值，HostPriorityList类型也就是HostPriority类型的集合，意味着记录了多个Host的名字和分值，于是我们可以判断PrioritizeNodes()方法的作用是计算前面的predicates过程筛选出来的nodes各自的Score.所以肯定还有一个根据Score决定哪个node胜出的逻辑咯～，继续往下看吧～ 1.3. Selecting host 这个过程比较明显了，我们直接看代码： pkg/scheduler/core/generic_scheduler.go:227 func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) 这个selectHost()方法大家应该都已经猜到了，就是从上一步的优选过程的结果集中选出一个Score最高的Host，并且返回这个Host的name. genericScheduler的Schedule()方法主要就是这3个过程，下一讲我们开始分析predicates过程。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-04 12:45:17 一般调度过程1. 进入Scheduler的第三层逻辑1.1. Computing predicates1.2. Prioritizing1.3. Selecting host"},"core/scheduler/predicate.html":{"url":"core/scheduler/predicate.html","title":"预选过程","keywords":"","body":"预选过程 预选流程 predicate的并发 一个node的predicate predicates的顺序 单个predicate执行过程 具体的predicate函数 1. 预选流程 predicate过程从pkg/scheduler/core/generic_scheduler.go:389 findNodesThatFit()方法就算正式开始了，这个方法根据给定的predicate functions过滤所有的nodes来寻找一堆可以跑pod的node集。老规矩，我们来看主干代码： pkg/scheduler/core/generic_scheduler.go:389 func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) { checkNode := func(i int) { fits, failedPredicates, err := podFitsOnNode( //…… ) if fits { length := atomic.AddInt32(&filteredLen, 1) filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) if len(filtered) > 0 && len(g.extenders) != 0 { for _, extender := range g.extenders { // Logic of extenders } } return filtered, failedPredicateMap, nil } 如上，删的有点多，大家也可以看一下原函数然后对比一下，看看我为什么只保留这一点。从上面代码中我们可以发现，最重要的是一个子函数调用过程fits, failedPredicates, err := podFitsOnNode()，这个函数的参数我没有贴出来，下面会详细讲；下半部分是一个extender过程，extender不影响对predicate过程的理解，我们后面专门当作一个主题讲。所以这里的关注点是podFitsOnNode()函数。 2. predicate的并发 进入podFitsOnNode()函数逻辑之前，我们先看一下调用到podFitsOnNode()函数的匿名函数变量checkNode是怎么被调用的： pkg/scheduler/core/generic_scheduler.go:458 workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) ParallelizeUntil()函数是用于并行执行N个独立的工作过程的，这个逻辑写的挺有意思，我们看一下完整的代码(这段的分析思路写到注释里哦)： vendor/k8s.io/client-go/util/workqueue/parallelizer.go:38 func ParallelizeUntil(ctx context.Context, workers, pieces int, doWorkPiece DoWorkPieceFunc) { // 从形参列表看，需要关注的有workers和pieces两个数字类型的参数，doworkPiece这个函数类型的参数 // DoWorkPieceFunc类型也就是func(piece int)类型 // 注意到上面调用的时候workers的实参是16，pieces是allNodes，也就是node数量 var stop 回想一下前面的checkNode := func(i int){……}，上面的doWorkPiece(piece)也就是调用到了这里的这个匿名函数func(i int){……}；到这里就清楚如何实现并发执行多个node的predicate过程了。 3. 一个node的predicate checkNode的主要逻辑就是上面介绍的并发加上下面这个podFitsOnNode()函数逻辑： pkg/scheduler/core/generic_scheduler.go:425 fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) 我们从podFitsOnNode()的函数定义入手： pkg/scheduler/core/generic_scheduler.go:537 func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, nodeCache *equivalence.NodeCache, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) 关于这个函数的逻辑，注释里的描述翻译过来大概是这个意思： podFitsOnNode()函数检查一个通过NodeInfo形式给定的node是否满足指定的predicate functions. 对于给定的一个Pod，podFitsOnNode()函数会检查是否有某个“等价的pod”存在，然后重用那个等价pod缓存的predicate结果。 这个函数的调用入口有2处: Schedule and Preempt. 当从Schedule进入时：这个函数想要测试node上所有已经存在的pod外加被指定将要调度到这个node上的其他所有高优先级（优先级不比自己低，也就是>=）的pod后，当前pod是否可以被调度到这个node上。 当从Preempt进入时：后面讲preempt时再详细分析。 podFitsOnNode()函数的参数有点多，每个跟进去就是一堆知识点。这里建议大家从字面先过一边，然后跟进去看一下类型定义，类型的注释等，了解一下功能，先不深究。整体看完一边调度器代码后回过头深入细节。 我们一起看一下其中这个参数：predicateFuncs map[string]algorithm.FitPredicate；这里的predicateFuncs是一个map，表示所有的predicate函数。这个map的key是个字符串，也就是某种形式的name了；value类型跟进去看一下： pkg/scheduler/algorithm/types.go:36 // FitPredicate is a function that indicates if a pod fits into an existing node. // The failure information is given by the error. type FitPredicate func(pod *v1.Pod, meta PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []PredicateFailureReason, error) FitPredicate是一个函数类型，3个参数，pod和node都很好理解，meta跟进去简单看一下可以发现定义的是一些和predicate相关的一些元数据，这些数据是根据pod和node信息获取到的，类似pod的端口有哪些，pod亲和的pod列表等。返回值是一个表示是否fit的bool值，predicate失败的原因列表，一个错误类型。 也就是说，FitPredicate这个函数类型也就是前面一直说的predicate functions的真面目了。下面看podFitsOnNode()函数的具体逻辑吧： pkg/scheduler/core/generic_scheduler.go:537 func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, nodeCache *equivalence.NodeCache, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) { podsAdded := false for i := 0; i 这里的逻辑是从一个for循环开始的，关于这个2次循环的含义代码里有很长的一段注释，我们先看一下注释里怎么说的（这里可以多看几遍体会一下）： 出于某些原因考虑我们需要运行两次predicate. 如果node上有更高或者相同优先级的“指定pods”（这里的“指定pods”指的是通过schedule计算后指定要跑在一个node上但是还未真正运行到那个node上的pods），我们将这些pods加入到meta和nodeInfo后执行一次计算过程。 如果这个过程所有的predicates都成功了，我们再假设这些“指定pods”不会跑到node上再运行一次。第二次计算是必须的，因为有一些predicates比如pod亲和性，也许在“指定pods”没有成功跑到node的情况下会不满足。 如果没有“指定pods”或者第一次计算过程失败了，那么第二次计算不会进行。 我们在第一次调度的时候只考虑相等或者更高优先级的pods，因为这些pod是当前pod必须“臣服”的，也就是说不能够从这些pod中抢到资源，这些pod不会被当前pod“抢占”；这样当前pod也就能够安心从低优先级的pod手里抢资源了。 新pod在上述2种情况下都可调度基于一个保守的假设：资源和pod反亲和性等的predicate在“指定pods”被处理为Running时更容易失败；pod亲和性在“指定pods”被处理为Not Running时更加容易失败。 我们不能假设“指定pods”是Running的因为它们当前还没有运行，而且事实上，它们确实有可能最终又被调度到其他node上了。 看了这个注释后，上面代码里的前几行就很好理解了，在第一次进入循环体和第二次进入时做了不同的处理，具体怎么做的处理我们暂时不关注。下面看省略的这个for循环做了啥： pkg/scheduler/core/generic_scheduler.go:583 // predicates.Ordering()得到的是一个[]string，predicate名字集合 for predicateID, predicateKey := range predicates.Ordering() { var ( fit bool reasons []algorithm.PredicateFailureReason err error ) // 如果predicateFuncs有这个key，则调用这个predicate；也就是说predicateFuncs如果定义了一堆乱七八遭的名字，会被忽略调，因为predicateKey是内置的。 if predicate, exist := predicateFuncs[predicateKey]; exist { // 降低难度，先不看缓存情况。 if eCacheAvailable { fit, reasons, err = nodeCache.RunPredicate(predicate, predicateKey, predicateID, pod, metaToUse, nodeInfoToUse, equivClass) } else { // 真正调用predicate函数了！！！！！！！！！ fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) } if err != nil { return false, []algorithm.PredicateFailureReason{}, err } if !fit { // …… } } } 如上，我们看一下2个地方： predicates.Ordering() fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) 分两个小节吧～ 3.1. predicates的顺序 pkg/scheduler/algorithm/predicates/predicates.go:130 var ( predicatesOrdering = []string{ CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred} ) 如上，这里定义了一个次序，前面的for循环遍历的是这个[]string，这样也就实现了不管predicateFuncs里定义了怎样的顺序，影响不了predicate的实际调用顺序。官网对于这个顺序有这样一个表格解释： Position Predicate comments (note, justification...) 1 CheckNodeConditionPredicate we really don’t want to check predicates against unschedulable nodes. 2 PodFitsHost we check the pod.spec.nodeName. 3 PodFitsHostPorts we check ports asked on the spec. 4 PodMatchNodeSelector check node label after narrowing search. 5 PodFitsResources this one comes here since it’s not restrictive enough as we do not try to match values but ranges. 6 NoDiskConflict Following the resource predicate, we check disk 7 PodToleratesNodeTaints check toleration here, as node might have toleration 8 PodToleratesNodeNoExecuteTaints check toleration here, as node might have toleration 9 CheckNodeLabelPresence labels are easy to check, so this one goes before 10 checkServiceAffinity - 11 MaxPDVolumeCountPredicate - 12 VolumeNodePredicate - 13 VolumeZonePredicate - 14 CheckNodeMemoryPressurePredicate doesn’t happen often 15 CheckNodeDiskPressurePredicate doesn’t happen often 16 InterPodAffinityMatches Most expensive predicate to compute 这个表格大家对着字面意思体会一下吧，基本还是可以联想到意义的。 当然这个顺序是可以被配置文件覆盖的，用户可以使用类似这样的配置： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\", \"order\": 2}, {\"name\" : \"PodFitsResources\", \"order\": 3}, {\"name\" : \"NoDiskConflict\", \"order\": 5}, {\"name\" : \"PodToleratesNodeTaints\", \"order\": 4}, {\"name\" : \"MatchNodeSelector\", \"order\": 6}, {\"name\" : \"PodFitsHost\", \"order\": 1} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1}, {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10 } 整体过完源码后我们再实际尝试一下这些特性，这一边先知道有这回事吧，ok，继续～ 3.2. 单个predicate执行过程 fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) 这行代码其实没有啥复杂逻辑，不过我们还是重复讲一下，清晰理解这一行很有必要。这里的predicate()来自前几行的if语句predicate, exist := predicateFuncs[predicateKey]，往前跟也就是FitPredicate类型，我们前面提过，类型定义在pkg/scheduler/algorithm/types.go:36，这个类型表示的是一个具体的predicate函数，这里使用predicate()也就是一个函数调用的语法，很和谐了。 3.3. 具体的predicate函数 一直在讲predicate，那么predicate函数到底长什么样子呢，我们从具体的实现函数找一个看一下。开始讲design的时候提到过predicate的实现在pkg/scheduler/algorithm/predicates/predicates.go文件中，先看一眼Structure吧： 这个文件中predicate函数有点多，这样看眼花，我们具体点开一个观察一下： pkg/scheduler/algorithm/predicates/predicates.go:277 func NoDiskConflict(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { for _, v := range pod.Spec.Volumes { for _, ev := range nodeInfo.Pods() { if isVolumeConflict(v, ev) { return false, []algorithm.PredicateFailureReason{ErrDiskConflict}, nil } } } return true, nil, nil } 我们知道predicate函数的特点，这样就很好在这个一千六百多行go文件中寻找predicate函数了。像上面这个NoDiskConflict()函数，参数是pod、meta和nodeinfo，很明显是FitPredicate类型的，标准的predicate函数。 这个函数的实现也特别简单，遍历pod的Volumes，然后对于pod的每一个Volume，遍历node上的每个pod，看是否和当前podVolume冲突。如果不fit就返回false加原因；如果fit就返回true，很清晰。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-06 18:50:29 预选过程1. 预选流程2. predicate的并发3. 一个node的predicate3.1. predicates的顺序3.2. 单个predicate执行过程3.3. 具体的predicate函数"},"core/scheduler/priority.html":{"url":"core/scheduler/priority.html","title":"优选过程","keywords":"","body":"优选过程 走近priority过程 PrioritizeNodes流程 results 老式priority函数 Map-Reduce Map-reduce方式的优选过程 Map-Reduce形式的priority函数 1. 走近priority过程 pkg/scheduler/core/generic_scheduler.go:186 priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 今天的分析从这行代码开始。 PrioritizeNodes要做的事情是给已经通过predicate的nodes赋上一个分值，从而抉出一个最优node用于运行当前pod. 前面已经有分析predicate过程的经验了，所以priority过程看起来应该会轻松很多吧～（现实可能比较残酷，我第一次看完predicate后看priority是一脸蒙，和想象中的不太一样；大伙得耐下性子多思考，实在有障碍就先不求甚解，整体过完后再二刷代码，再不行三刷，总会大彻大悟的！） 我们先看看函数签名： pkg/scheduler/core/generic_scheduler.go:624 func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) 源码中的注释先理解一下： PrioritizeNodes通过并发调用一个个priority函数来给node排优先级。每一个priority函数会给一个1-10之间的分值，0最低10最高。 每一个priority函数可以有自己的权重，单个函数返回的分值*权重后得到一个加权分值，最终所有的加权分值加在一起就是这个node的最终分值。 行，大概知道优选过程要干嘛了，然后我们关注一下PrioritizeNodes()函数的形参定义和返回值： pod v1.Pod // pod就不用说了； nodeNameToInfo map[string]schedulercache.NodeInfo // 这个也不需要讲，字面意思代表一切； meta interface{} // 和predicate里的meta不太一样，先略过； priorityConfigs []algorithm.PriorityConfig // 包含优选算法各种信息，比较重要； nodes []*v1.Node // node集合，不需要解释了； extenders []algorithm.SchedulerExtender // extender逻辑放到后面单独讲。 返回值只需要看一下schedulerapi.HostPriorityList类型的含义了，这个类型之前也提过，后面频繁涉及到操作这个结构，所以这里再贴一次，大伙得烂熟于心才行！ pkg/scheduler/api/types.go:305 type HostPriority struct { Host string Score int } type HostPriorityList []HostPriority 着重分析一下这2个type，虽然很简单，还是有必要啰嗦一下，必须记在心里。HostPriority这个struct的属性是Host和Score，一个是string一个是int，所以很明显HostPriority所能够保存的信息是一个node的名字和分值，再仔细一点说就是这个结构保存的是一个node在一个priority算法计算后所得到的结果；然后看HostPriorityList类型，这个类型是上一个类型的集合，集合表达的是一个node多个算法还是多个node一个算法呢？稍微思考一下可以知道HostPriorityList中存的是多个Host和Score的组合，也就是每个node的Score信息都有了。我们知道一个算法作用在所有node上就会得到每个node对应的一个Score，所以HostPriorityList这个结构是要保存一个算法作用于所有node之后，得到的所有node的Score信息的。（这里我们先理解成一个算法的结果，作为函数返回值这里肯定是要保留所有算法作用后的最终node的Score，所以函数后半部分肯定有整合分值的步骤。） 2. PrioritizeNodes流程 前面说到PrioritizeNodes()函数也就是node优选的具体逻辑，这个函数略长，我们分段讲解。 2.1. results PrioritizeNodes()函数开头的逻辑很简单，我们先从第一行看到results定义的这一行。 pkg/scheduler/core/generic_scheduler.go:634 if len(priorityConfigs) == 0 && len(extenders) == 0 { // 这个if很明显是处理特殊场景的，就是优选算法一个都没有配置的时候怎么做（extenders同样没有）； // 这个result是要当作返回值的，HostPriorityList类型前面唠叨了很多了，大家得心里有数； result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { // 这一行代码是唯一的“逻辑了”，下面直到for结束都是简单代码；所以我们看一下EqualPriorityMap // 函数的作用就行了。这里我不贴代码，这个函数很短，作用就是设置每个node的Score相同（都为1） // hostPriority的类型也就是schedulerapi.HostPriority类型，再次强调这个类型是要烂熟于心的； hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } // 最终的result也就是设置了每个node的Score为1的schedulerapi.HostPriorityList类型数据； result = append(result, hostPriority) } return result, nil } // 这里只是简单定义3个变量，一把锁，一个并发等待相关的wg，一个错误集合errs； var ( mu = sync.Mutex{} wg = sync.WaitGroup{} errs []error ) // 这里定义了一个appendError小函数，逻辑很简单，并发场景下将错误信息收集到errs中； appendError := func(err error) { mu.Lock() defer mu.Unlock() errs = append(errs, err) } // 最后一个变量results也不难理解，类型是[]schedulerapi.HostPriorityList，这里需要注意这个类型 // 的作用，它保存的是所有算法作用所有node之后得到的结果集，相当于一个二维数组，每个格子是1个算法 // 作用于1个节点的结果，一行也就是1个算法作用于所有节点的结果；一行展成1个二维就是所有算法作用于所有节点； results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs)) 到这里要求大家心中能够想象上面提到的results是什么样的，不好想象可以借助纸笔。下面的代码会往这个二维结构里面存储数据。 2.2. 老式priority函数 我们既然讲到“老式”，后面肯定有对应的“新式”。虽然这种函数已经DEPRECATED了，不过对于我们学习掌握优选流程还是很有帮助的。贴这块代码之前我们先关注一下多次出现的priorityConfigs这个变量的类型： 函数形参中有写到：priorityConfigs []algorithm.PriorityConfig，所以我们直接看PriorityConfig是什么类型： pkg/scheduler/algorithm/types.go:62 // PriorityConfig is a config used for a priority function. type PriorityConfig struct { Name string Map PriorityMapFunction Reduce PriorityReduceFunction // TODO: Remove it after migrating all functions to // Map-Reduce pattern. Function PriorityFunction Weight int } PriorityConfig中有一个Name，一个Weight，很好猜到意思。剩下的Map、Reduce和Function目测代表的就是优选函数的新旧两种表达方式了。我们先看旧的Function属性的类型PriorityFunction是什么： pkg/scheduler/algorithm/types.go:59 type PriorityFunction func(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) 很明显这个类型代表了一个priority函数，入参是pod、nodeNameToInfo和nodes，返回值是HostPriorityList，也就是我们前面提到的1个priority函数作用于每个node后得到了Score信息，存结果的结构就是这个HostPriorityList； 然后讲回PrioritizeNodes过程： pkg/scheduler/core/generic_scheduler.go:661 // DEPRECATED: we can remove this when all priorityConfigs implement the // Map-Reduce pattern. for i := range priorityConfigs { // 如果第i个优选配置定义了老函数，则调用之； if priorityConfigs[i].Function != nil { wg.Add(1) // 注意这里的参数index，这里传入的实参是上面的i； go func(index int) { defer wg.Done() var err error // 所以这里的results[index]就好理解了；后面priorityConfigs[index]的索引也是index， // 这里表达的是第N个优选配置里有Function，那么这个Function的计算结果保存在 // results的第N个格子里； results[index], err = priorityConfigs[index].Function(pod, nodeNameToInfo, nodes) if err != nil { appendError(err) } }(i) } else { // 如果没有定义Function，其实也就是使用了Map-Reduce方式的，这里先存个空的结构占位； results[i] = make(schedulerapi.HostPriorityList, len(nodes)) } } 上面这段代码逻辑还算好理解，唯一有点绕的还是前面强调的HostPriorityList相关类型的操作上。 2.3. Map-Reduce 关于map-reduce思想我就不在这里赘述了，数据处理很流行的一种思想，百度一下（如果你能够google，自然更好咯）一大堆介绍的文章。 简单说map-reduce就是：Map是映射，Reduce是规约；map是统计一本书中的一页出现了多少次k8s这个词，reduce是将这些词加在一起得到最终结果。（map一般都是将一个算法作用于一堆数据集的每一个元素，得到一个结果集，reduce有各种形式，我没有深入看map-reduce的思想，也没有玩过大数据领域的Hadoop和MapReduce，这里可能理解有出入） 2.3.1. Map-reduce方式的优选过程 看看在Scheduler里面是怎么用Map-Reduce的吧： // 这个并发逻辑之前介绍过了，我们直接看ParallelizeUntil的最后一个参数就行，这里直接写了一个匿名函数； workqueue.ParallelizeUntil(context.TODO(), 16, len(nodes), func(index int) { // 这里的index是[0，len(nodes)-1]，相当于遍历所有的nodes； nodeInfo := nodeNameToInfo[nodes[index].Name] // 这个for循环遍历的是所有的优选配置，如果有老Fun就跳过，新逻辑就继续； for i := range priorityConfigs { if priorityConfigs[i].Function != nil { continue } var err error // 这里的i和前面老Fun的互补，老Fun中没有赋值的results中的元素就在这里赋值了； // 注意到这里调用了一个Map函数就直接赋值给了results[i][index]，这里的index通过 // ParallelizeUntil这个并发实现所有node对于一个优选算法的分值计算； results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) results[i][index].Host = nodes[index].Name } } }) for i := range priorityConfigs { // 没有定义Reduce函数就不处理； if priorityConfigs[i].Reduce == nil { continue } wg.Add(1) go func(index int) { defer wg.Done() // 调用Reduce函数 if err := priorityConfigs[index].Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if klog.V(10) { for _, hostPriority := range results[index] { klog.Infof(\"%v -> %v: %v, Score: (%d)\", util.GetPodFullName(pod), hostPriority.Host, priorityConfigs[index].Name, hostPriority.Score) } } }(i) } // Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } 看到这里我们可以发现老Fun和Map的区别不大，都是优选函数的执行过程。这里可以注意到有一个区别就是老Fun执行结果是给了results[index]，这个类型是schedulerapi.HostPriorityList；而Map的执行结果是给了results[i][index]，这个类型是HostPriority类型，开头的时候多次强调过这2个类型。但是Map本身在并发执行的时候，遍历的是nodes，所以Map的结果其实给了results[i]，也就是填充了schedulerapi.HostPriorityList给results. 所以不管是老的Fun还是新的Map，做的事情是一样的，都是把1个算法作用于所有的nodes，从而得到1个result，也就是1个schedulerapi.HostPriorityList，无非是Map函数是一个一个node计算，Fun是一下子接受了nodes参数拿去计算。 开始时可能大家会以为Fun直接计算出了一个优选函数作用于所有node的结果，Map是一个优选函数作用于一个node的结果，然后使用reduce汇总，然而并不是这样。那Fun和Map-Reduce到底还有啥区别呢？继续看吧。 我们关注一下Reduce函数的一个参数是results[index]，这个index表达的是第index个优选算法，没有啥实质返回值，所以Reduce并不将什么数据整合了，最终results还是[]HostPriorityList类型，也就是[][]HostPriority，存了每个算法作用于每个node的分值。 看来只能看几个Reduce函数的实例才能理解Reduce在这个场景的作用了～ 2.3.2. Map-Reduce形式的priority函数 CalculateAntiAffinityPriority计算反亲的目的是让“同一个service下的pod尽量分散在给定的nodes上”。下面我们来看这个策略的Map和Reduce过程分别是怎么写的： map函数 pkg/scheduler/algorithm/priorities/selector_spreading.go:221 func (s *ServiceAntiAffinity) CalculateAntiAffinityPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { var firstServiceSelector labels.Selector node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } priorityMeta, ok := meta.(*priorityMetadata) if ok { firstServiceSelector = priorityMeta.podFirstServiceSelector } else { firstServiceSelector = getFirstServiceSelector(pod, s.serviceLister) } // 查找给定node在给定namespace下符合selector的pod，返回值是[]*v1.Pod matchedPodsOfNode := filteredPod(pod.Namespace, firstServiceSelector, nodeInfo) return schedulerapi.HostPriority{ Host: node.Name, // 返回值中Score设置成上面找到的pod的数量 Score: int(len(matchedPodsOfNode)), }, nil } 这个函数比较短，可以看到在指定node上查询到匹配selector的pod越多，分值就越高。假设找到了20个，那么这里的分值就是20；假设找到的是2，那这里的分值就是2. reduce函数 pkg/scheduler/algorithm/priorities/selector_spreading.go:245 func (s *ServiceAntiAffinity) CalculateAntiAffinityPriorityReduce(pod *v1.Pod, meta interface{}, nodeNameToInfo map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error { var numServicePods int var label string podCounts := map[string]int{} labelNodesStatus := map[string]string{} maxPriorityFloat64 := float64(schedulerapi.MaxPriority) for _, hostPriority := range result { // Score为n就是map函数中写入的匹配pod的数量为n; // 累加也就得到了所有node的匹配pod数量之和； numServicePods += hostPriority.Score // 如果nodes上的labels中不包含当前服务指定的反亲和label，则continue； if !labels.Set(nodeNameToInfo[hostPriority.Host].Node().Labels).Has(s.label) { continue } // 这里也就是没有continue的逻辑，说明在当前node上找到了反亲和的label，通过Get获取label值； label = labels.Set(nodeNameToInfo[hostPriority.Host].Node().Labels).Get(s.label) // 存一个host name和label值的map； labelNodesStatus[hostPriority.Host] = label // label和pod数的map； podCounts[label] += hostPriority.Score } // 遍历result也就是遍历每个node for i, hostPriority := range result { // 如果这个node上有设置的label label, ok := labelNodesStatus[hostPriority.Host] if !ok { result[i].Host = hostPriority.Host result[i].Score = int(0) continue } // 设置fScore为默认最大值10.0 fScore := maxPriorityFloat64 if numServicePods > 0 { fScore = maxPriorityFloat64 * (float64(numServicePods-podCounts[label]) / float64(numServicePods)) } result[i].Host = hostPriority.Host result[i].Score = int(fScore) } return nil } Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-18 16:42:10 优选过程1. 走近priority过程2. PrioritizeNodes流程2.1. results2.2. 老式priority函数2.3. Map-Reduce2.3.1. Map-reduce方式的优选过程2.3.2. Map-Reduce形式的priority函数"},"core/scheduler/preempt.html":{"url":"core/scheduler/preempt.html","title":"抢占调度","keywords":"","body":"抢占调度 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-06 17:00:05 "},"core/apiserver/":{"url":"core/apiserver/","title":"apiserver","keywords":"","body":"apiserver Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/proxy/":{"url":"core/proxy/","title":"proxy","keywords":"","body":"proxy Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/kubelet/":{"url":"core/kubelet/","title":"kubelet","keywords":"","body":"kubelet Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/controller-manager/":{"url":"core/controller-manager/","title":"controller-manager","keywords":"","body":"controller-manager Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"around/":{"url":"around/","title":"概述","keywords":"","body":"周边项目源码分析 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:30:13 "},"around/client-go/":{"url":"around/client-go/","title":"client-go","keywords":"","body":"client-go Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:30:13 "}}