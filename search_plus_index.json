{"./":{"url":"./","title":"前言","keywords":"","body":" 1、关于本书 本书将系统讲解kubernetes的核心组件源码，附带介绍相关的周边项目，比如client-go等。 建议通过公众号CloudGeek（微信上直接搜索可以找到）接收更新消息，通过github pages阅读本书。 寻找组织可以加我微信（ITNT01），一句话证明自己是源码学习者，然后我会拉你进群。 2、内容更新 本项目会不定期更新，一般会在每周五更新一节；更新内容将同步发到公众号CloudGeek、博客园CloudGeek等。细微的更新，比如错别字修改等不会同步到其他所有平台。 每次新章节会选择性提前发到微信群内，比如本周五要发出来的新内容周一可能就内部发到群里了，然后接受读者反馈，定稿后上传到github，然后同步到微信公众号等平台。 3、本教程适合谁读 任何对k8s源码感兴趣的人都可以看本教程，但是我建议你至少有golang项目开发经验，简单的golang开源项目的源码分析经验，k8s应用经验，对golang的基础特性和k8s的基础特性有一定的了解；不然直接上手看k8s源码会郁闷的。 4、章节规划与负责人 Scheduler - farmer-hutao Controller-manager - farmer-hutao Kube-proxy - XiaoYang Apiserver Kubelet 5、版本说明 本书基于：v1.13版本源码讲解。 6、协议 本书使用Apache License 2.0协议，但是保留出版图书的权利。 7、贡献 欢迎参与本书编写！如果你对开源项目源码分析感兴趣，可以和我联系，如果你擅长某个项目，可以主导一个章节的编写。 如果想要提交pull request，建议先开一个issue，说明你要做什么修改，一般1天内我会回复issue，告诉你是否接受修改。但是得提前告诉你我是一个有洁癖的人，包括代码洁癖，文字洁癖等，所以请不要提交太随意的风格的内容哦～ 另外注意：一定先更新你的仓库到最新状态，然后编辑新内容，不然冲突很尴尬～ 8、FAQ 暂时我没有考虑增加评论功能，因为不可避免要增加三方插件，三方插件意味着用户需要注册登录等，体验不会太好。万一哪天这个插件倒闭了，就白忙活了。所以在每章开头我增加了一个FAQ部分，会把这一章中各个小节收到的部分问题汇总在开头的FAQ里。 大家在微信里问我的问题一般我都会耐心解答，但是和go语言本身语法相关的初级问题还是不希望经常遇到，因为我认为语言本身问题是很容易通过网络资料学习掌握的。另外有问题尽量抛到群里，私信多的话有时候我要1个问题很好几个人讲，工作量比较大。 9、支持本书 微信扫一扫，鼓励作者快快更新，写出更多优质的文章～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 "},"prepare/":{"url":"prepare/","title":"k8s源码分析准备工作","keywords":"","body":"k8s源码分析准备工作 1. 本章规划 源码准备 测试环境搭建-单节点 测试环境搭建-三节点 源码调试 2. 概述 源码准备阶段主要介绍k8s源码的获取与本地golang编译环境配置等；测试环境搭建是介绍如何准备一个k8s环境，用于后续组件的代码测试。测试环境可以在大体学习完一个组件源码后进行，用于验证自己在看源码过程中的一些想法和疑惑。单节点还是多节点大家可以自己灵活决定。 3. 一些经验 看k8s源码的这些日子里遇到过不少困难，前前后后掉过不少坑，有些经验分享给大家： 语言基本功不扎实不要开始刷k8s，脚踏实地，从小项目开始练手，至少一两千行的小型开源项目能够完全看懂之后再尝试看k8s，不然会受打击的，k8s不是一个入门级项目，也不是用来入门编程语言的项目。 广度优先，大项目一般有很好的层级结构，从高层理解项目的逻辑，再一层一层深入。说个简单的例子，找到main函数后发现main只有10行代码，这时候看完十行代码你得认为自己看完了。确实已经看完了整个流程呀，只是没有深入main里面调用到的几个函数嘛，但是看函数名就知道这个函数的作用了，不知道实现而已。ok，这就是第一遍；然后深入下一层；再举个例子，看调度器的时候第一遍过源码不跟到具体的调度算法，深度优先就陷进去了。第一次应该看整体框架逻辑，找到调用预选、优选代码的入口，知道这个函数完成整个预选过程，就往下继续走。这样一遍过完后心里就有调度器的架子了。然后深入预选逻辑，再看优选逻辑。接着可以看具体的实现算法，可以看调度算法是怎么注册的，怎么初始化的。如果你一开始纠结于调度器如何初始化，可能会很痛苦。广度优先，好好体会。 遇到问题先思考，尤其语言层面的问题要能够自己通过网上资料解决。不愿意静下心多思考是肯定看不完k8s的。假如k8s换成C语言实现，涉及到的语言层面的技巧就变了，但是k8s要解决的问题还是一样的，还是一样的思想，一样的流程，这些语言无关的技巧才是k8s特有的，我觉得这才是重点关注的地方。而类似无缓存channel和有缓存channel区别这类问题，配置文件怎么生效这类问题，可以说都和k8s本身无关。 把学到知识，提升技能作为目的；把看完源码作为结果。一味追求速度很快就会发现看不下去了，身心疲惫。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-21 19:47:58 k8s源码分析准备工作1. 本章规划2. 概述3. 一些经验"},"prepare/get-code.html":{"url":"prepare/get-code.html","title":"源码准备","keywords":"","body":"源码准备 环境准备 源码下载 源码编译 IDE 1. 环境准备 操作系统：我们使用Linux作为k8s源码分析和调试环境，fedora、centos、ubuntu都行，我这里使用fedora； golang相关： GOROOT=/usr/local/lib/golang GOPATH=/root/go go version go1.10.3 linux/amd64 2. 源码下载 mkdir -p /root/go/src/k8s.io cd /root/go/src/k8s.io/ git clone https://github.com/kubernetes/kubernetes.git 下载后本地目录： 3. 源码编译 我们先看一下几个主要的目录： 目录名 用途 cmd 每个组件代码入口（main函数） pkg 各个组件的具体功能实现 staging 已经分库的项目 vendor 依赖 考虑到国内网络环境等因素，我们不使用容器化方式构建。我们尝试在kubernetes项目cmd目录下构建一个组件（执行路径：/root/go/src/k8s.io/kubernetes/cmd/kube-scheduler）： 这里需要注意一下，如果报依赖错误，找不到k8s.io下的某些项目，就到vendor下看一下软链接是不是都还在，如下： 注意到k8s是使用这种方式解决k8s.io下的依赖问题的，如果我们在windows下下载的代码，然后copy到linux下，就很容易遇到这些软链接丢失的情况，导致go找不到依赖，编译失败。 4. IDE 我们使用Goland看代码： 最后，别忘了在正式研读源码前切换到release-1.13分支～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 11:08:05 源码准备1. 环境准备2. 源码下载3. 源码编译4. IDE"},"prepare/debug-environment.html":{"url":"prepare/debug-environment.html","title":"测试环境搭建-单节点","keywords":"","body":"测试环境搭建 （k8s-1.13版本单节点环境搭建） 概述 kubeadm简介 操作系统准备 系统信息 配置selinux和firewalld 系统参数与内核模块 配置yum源 禁用swap 安装docker 安装kubeadm、kubelet和kubectl 镜像准备 阿里云镜像加速器配置（可选） 镜像下载 安装k8s master 环境验证 1. 概述 大家注意哦，不一定要先搭建好环境再看源码，大可以先看一个组件，感觉差不多理解了，想要run一把，想要改几行试试的时候回过头来搭建k8s环境。 当然，大家开始看源码的时候，我相信各位都是搭建过不少次k8s集群，敲过N多次kubectl命令了，所以下面我不会解释太基础的命令是做什么的。 今天我们要做的是搭建一个单机版的k8s环境，用于后面的学习。虽然k8s的环境搭建没有openstack来的复杂，但是由于网络等乱七八糟的问题，在国内手动搭建一个五脏俱全的k8s也不算太容易，一个不小心就会被乱七八遭的障碍磨灭兴趣。今天看看我能不能让大家觉得这个过程无痛吧～ 2. kubeadm简介 选择一个好的工具很重要！大家在刚开始学习k8s的时候应该都用二进制文件一步一步搭建过集群吧，那个过程是不是很酸爽？手动一步一步搭建环境对于初学者来说确实大有裨益，可以了解各个组件的细节。我已经懒掉了，我决定从众多k8s自动化安装方案中选择一个来搭建这次的k8s环境。 kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，这不是一个单独的项目哦，我们在kubernetes源码里可以看到这个组件（kubernetes/cmd/kubeadm/）： kubeadm这个工具可以通过简单的kubeadm init和kubeadm join命令来创建一个kubernetes集群，kubeadm提供的其他命令都比较通俗易懂： kubeadm init 启动一个master节点； kubeadm join 启动一个node节点，加入master； kubeadm upgrade 更新集群版本； kubeadm config 从1.8.0版本开始已经用处不大，可以用来view一下配置； kubeadm token 管理kubeadm join的token； kubeadm reset 把kubeadm init或kubeadm join做的更改恢复原状； kubeadm version打印版本信息； kubeadm alpha预览一些alpha特性的命令。 关于kubeadm的成熟度官方有一个表格： Area Maturity Level Command line UX GA Implementation GA Config file API beta CoreDNS GA kubeadm alpha subcommands alpha High availability alpha DynamicKubeletConfig alpha Self-hosting alpha 主要特性其实都已经GA了，虽然还有一些小特性仍处于活跃开发中，但是整体已经接近准生产级别了。对于我们的场景来说用起来已经绰绰有余！ 3. 操作系统准备 我们先使用一个机子来装，后面需要拓展可以增加节点，使用kubeadm join可以很轻松扩展集群。 3.1. 系统信息 内存：2G CPU：2 磁盘：20G 系统版本和内核版本如下所示，大家不需要严格和我保持一致，不要使用太旧的就行了。 # cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) # uname -r 3.10.0-862.9.1.el7.x86_64 3.2. 配置selinux和firewalld # Set SELinux in permissive mode setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config # Stop and disable firewalld systemctl disable firewalld --now 3.3. 系统参数与内核模块 # 修改内核参数 cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 加载内核模块 modprobe br_netfilter lsmod | grep br_netfilter 3.4. 配置yum源 # base repo cd /etc/yum.repos.d mv CentOS-Base.repo CentOS-Base.repo.bak curl -o CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/gpgcheck=1/gpgcheck=0/g' /etc/yum.repos.d/CentOS-Base.repo # docker repo curl -o docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # k8s repo cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # update cache yum clean all yum makecache yum repolist 最终我们可以看到这些repo： 3.5. 禁用swap swapoff -a echo \"vm.swappiness = 0\">> /etc/sysctl.conf sysctl -p 如上配置，重启后swap又会被挂上，我们还需要注释掉/etc/fstab中的一行配置： 最终看到的结果是这样的（这个截图是下一节搭建集群的时候补到这里的，内存大小和单机的不一样）： 4. 安装docker 先看一下有哪些可用版本：yum list docker-ce --showduplicates | sort -r 我们选择一个版本安装： yum install docker-ce- 这里我选择18.06.3，所以我用的命令是： yum install docker-ce-18.06.3.ce 可以用rpm命令看一下docker-ce这个rpm包带来了哪些文件： 启动docker： systemctl enable docker --now 查看服务状态： systemctl status docker 5. 安装kubeadm、kubelet和kubectl kubeadm不管kubelet和kubectl，所以我们需要手动安装kubelet和kubectl： yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 如果你看到这个教程的时候yum源里已经有了1.14版本的kubeadm，那么要么你装新版本，要么通过上面装docker同样的方式指定1.13版本安装，我这里使用的是1.13.3。 最后启动kubelet： systemctl enable --now kubelet 6. 镜像准备 6.1. 阿里云镜像加速器配置（可选） 如果访问dockerhub速度不理想，可以选择配置阿里容器镜像加速器。 访问阿里云官网：https://www.aliyun.com 注册登录（可以使用支付宝直接登录） 找到镜像加速器配置页面（如果一时找不到，可以在页面上使用搜索功能）：https://cr.console.aliyun.com/cn-hangzhou/mirrors 页面上有详细的配置指南，对着操作就可以了 如果懒得登录阿里云折腾，也可以直接使用我的配置： mkdir -p /etc/docker tee /etc/docker/daemon.json 6.2. 镜像下载 然后我们就可以下载image，下载完记得打个tag： docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.13.3 k8s.gcr.io/kube-apiserver:v1.13.3 docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.13.3 k8s.gcr.io/kube-controller-manager:v1.13.3 docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.13.3 k8s.gcr.io/kube-scheduler:v1.13.3 docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.13.3 docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.13.3 k8s.gcr.io/kube-proxy:v1.13.3 docker pull mirrorgooglecontainers/pause-amd64:3.1 docker tag mirrorgooglecontainers/pause-amd64:3.1 k8s.gcr.io/pause:3.1 docker pull mirrorgooglecontainers/etcd-amd64:3.2.24 docker tag mirrorgooglecontainers/etcd-amd64:3.2.24 k8s.gcr.io/etcd:3.2.24 docker pull coredns/coredns:1.2.6 docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6 7. 安装k8s master tip：下面的ip地址(192.168.19.100)大家需要替换成自己机器上的！ kubeadm init --pod-network-cidr=10.100.0.0/16 --service-cidr=10.101.0.0/16 --kubernetes-version=v1.13.3 --apiserver-advertise-address 192.168.19.100 --kubernetes-version: 用于指定k8s版本； --apiserver-advertise-address：用于指定kube-apiserver监听的ip地址； --pod-network-cidr：用于指定Pod的网络范围； --service-cidr：用于指定SVC的网络范围； 如上，跑kubeadm init命令后等几分钟。 如果遇到报错，对着错误信息修正一下。比如没有关闭swap会遇到error，系统cpu不够会遇到error，网络不通等等都会出错，仔细看一下错误信息一般都好解决～ 跑完上面的命令后，会看到类似如下的输出： Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.19.100:6443 --token i472cq.tr9a81qxnyqc5zj2 --discovery-token-ca-cert-hash sha256:acba957db29e0efbffe2cf4e484521b3b7e0f9d5c2ab7f9db68a5e31565d0d66 上面输出告诉我们还需要做一些工作： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # flannel (如果上面自定义了pod ip范围，这里需要修改flannel的configmap) kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml flannel的Network修改：如下图，默认是10.244.0.0/16，我这边pod的cidr是10.100.0.0/16，所以下面也改成10.100.0.0/16； 防止kube-flannel.yml下载地址变化留一份本地链接：点击查看> 创建完后可以看到flannel的configmap如下： 稍等一会，应该可以看到node状态变成ready： # kubectl get node NAME STATUS ROLES AGE VERSION kube-master Ready master 23m v1.13.3 如果你的环境迟迟都是NotReady状态，可以kubectl get pod -n kube-system看一下pod状态，一般可以发现问题，比如flannel的镜像下载失败啦～ 当node Ready的时候，我们可以看到pod也全部ready了： 再看一下核心组件状态： 最后注意到kube-master这个node上有一个Taint： # kubectl describe node kube-master | grep Taints Taints: node-role.kubernetes.io/master:NoSchedule 默认master节点是不跑业务pod的，我们暂时只有一个node，所以先去掉这个Taint： # kubectl taint node kube-master node-role.kubernetes.io/master- node/kube-master untainted # kubectl describe node kube-master | grep Taints Taints: 8. 环境验证 我们来跑一个pod，证明环境正常可用了： 写一个yaml： apiVersion: apps/v1 kind: Deployment metadata: name: mytomcat spec: replicas: 1 selector: matchLabels: app: mytomcat template: metadata: name: mytomcat labels: app: mytomcat spec: containers: - name: mytomcat image: tomcat:8 ports: - containerPort: 8080 如上内容保存为tomcat-deploy.yaml，执行kubectl create -f tomcat-deploy.yaml，然后看pod状态： 确认了，是熟悉的Running，哈哈，基本算大功告成了！最后我们看一下tomcat服务能不能访问到： 很完美，如果加个svc配置，就能够通过浏览器访问到汤姆猫界面了！ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 测试环境搭建1. 概述2. kubeadm简介3. 操作系统准备3.1. 系统信息3.2. 配置selinux和firewalld3.3. 系统参数与内核模块3.4. 配置yum源3.5. 禁用swap4. 安装docker5. 安装kubeadm、kubelet和kubectl6. 镜像准备6.1. 阿里云镜像加速器配置（可选）6.2. 镜像下载7. 安装k8s master8. 环境验证"},"prepare/debug-environment-3node.html":{"url":"prepare/debug-environment-3node.html","title":"测试环境搭建-三节点","keywords":"","body":"测试环境搭建-三节点 （k8s-1.13版本三节点环境搭建） 概述 系统准备 镜像和rpms 安装master 添加node节点 安装flannel 环境验证 1. 概述 写在前面：本节不建议在未阅读上一节（单机版环境搭建）的情况下阅读。下面内容稍稍随意，上一节提过的就不重复了。另外都是使用kubeadm实现，没有本质区别，所以下文从简。 前面一节讲了单节点的环境搭建，在调试调度策略等场景的时候单节点不好说明问题，所以今天补一个3节点的集群搭建过程。1个笔记本搭建3节点确实有点压力，外加要源码编译，调试，着实卡到不行。大伙不需要一开始先折腾好环境。在看源码遇到困惑，需要上环境调试或者验证特性单机不够用时再倒腾吧。下附我当前渣渣笔记本配置： 2. 系统准备 和上一节同样的配置方式，这里不再赘述。我这里3个节点基本信息如下： ip hostname 用途 29.123.161.240 kube-master master 节点 29.123.161.207 kube-node1 node 节点 29.123.161.208 kube-node2 node 节点 每个节点的/etc/hosts配置： 3. 镜像和rpms 镜像和rpm包的获取方式和上一节一样，node节点并不需要安装和master一样的rpm包，也不需要全部的镜像，不过我贪方便，直接在3个节点放了一样的“包”；我用的是离线的虚拟机，所以是一下子拷贝了rpm包和镜像tar包这些进去，去区分还不如直接全部装，大家按需自己灵活决定～. 4. 安装master 运行init命令： kubeadm init --pod-network-cidr=10.100.0.0/16 --kubernetes-version=v1.13.3 --apiserver-advertise-address 29.123.161.240 --service-cidr=10.101.0.0/16 运行结束后我们看到如下输出： 对着输出信息初始化： 5. 添加node节点 在2个node节点执行同样的kube join命令（具体命令master安装完输出的信息里可以找到）： 6. 安装flannel 和上一节一样下载yaml文件，镜像可以提前下载好（如果网络不给力）。 yml文件本地链接：点击查看> 执行kubectl create -f kube-flannel.yml（同样因为这里自定义了pod的cidr，所以这里需要修改flannel的yaml配置；如果已经创建了资源，同样可以通过修改configmap实现）； 对应的configmap资源： 稍等一会查看pod状态 查看node状态： 7. 环境验证 同样我们用tomcat镜像来测试： apiVersion: apps/v1 kind: Deployment metadata: name: tomcat spec: replicas: 2 selector: matchLabels: app: tomcat template: metadata: name: tomcat labels: app: tomcat spec: containers: - name: tomcat image: tomcat:8 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: tomcat-svc spec: selector: app: tomcat ports: - name: http port: 8080 targetPort: 8080 protocol: TCP 创建资源： 查看pod和svc： 通过svc访问tomcat服务： ok，3节点的环境验证各种特性基本都够用～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 测试环境搭建-三节点1. 概述2. 系统准备3. 镜像和rpms4. 安装master5. 添加node节点6. 安装flannel7. 环境验证"},"prepare/debug.html":{"url":"prepare/debug.html","title":"源码调试","keywords":"","body":"源码调试 概述 k8s环境 配置goland 写在后面 1. 概述 本节内容并不是在准备好k8s环境后就写的，大家也不需要在开始看源码前研究怎么调试代码。今天之前我已经发了《调度器优选过程》也就是差不多准备好k8s环境后一个月了。所以大家也可以先跳过本节，先开始看后面的源码分析，灵活把握开始调试源码的时间吧。 Debug应该是所有“攻城狮”的基本技能。除非是“Hello World”之类的小程序，不然都会出错，俗称bug，有bug就要debug. 今天不赘述调试本身的技术，我们只以调度器为例聊聊怎么把k8s的代码单步执行跑起来，看看内存里是个啥～ 2. k8s环境 我们前面已经搭建了3节点的集群，相关组件主要是static pod的方式在运行，所以调试调度器的时候我们应该把kube-scheduler容器停掉： 1、找到static pod 的yaml文件： 这时候scheduler是运行状态： 2、挪开yaml文件，让scheduler停止： 这时候再看scheduler可以发现pod已经没了 3. 配置goland 在main函数前面点一下这个绿色的三角形，当然这样运行肯定会失败，但是点一下会为我们生成一些配置，可以简化很多事情。点完之后开始配置： 点击上面的Edit，可以看到下面窗口： 这里的Program arguments默认是空的，我们怎么知道这里配置啥呢？ 从前面挪动的yaml中可以看到如下配置： 很明显，拷贝这个scheduler.conf到goland所在的机子，加上--kubeconfig这个flag之后就可以运行了。从前面的截图中可以看到我是将其放在了/etc/kubernetes/scheduler.conf. 开启调试： 如上，进入了熟悉的界面。 当然到这里还没有和api server交互，要进入调度逻辑需要有待调度的pod才行。我们使用前面验证环境的使用的tomcat： 创建这个Deployment之后可以看到pod是pending的： 我们把断点打在scheduleOne()里面： 非常熟悉的界面来了： 这样就能跟到调度器里的各种逻辑了。 当调度器跑完后，pod也就起来了： 最后说下我的goland是跑在哪里的： k8s master：29.123.161.240 k8s node：29.123.161.207 29.123.161.208 goland：29.123.161.241 所以goland不在k8s集群内。当然这个没有啥讲究，也可以跑在一起，灵活决定。 4. 写在后面 本来计划讲一下scheduler里的一些主要流程的调试，但是debug这一节放在调度器那章里感觉又不合适，最后决定放在环境准备里面。让新接触本书的小伙伴可以早点看到，灵活决定开始调试的时间。放在这里的话讲太多调度器的知识也不合适。所以这次就不多说具体代码的调试了，在各个章节里如果哪个模块讲解时我觉得需要插一个调试过程，就直接插在对应的地方吧～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-21 19:47:58 源码调试1. 概述2. k8s环境3. 配置goland4. 写在后面"},"core/":{"url":"core/","title":"概述","keywords":"","body":"核心组件源码分析 1. 概述 核心组件的源码分析主要包括： kube-scheduler kube-controller-manager apiserver kube-proxy kubelet 在分析第一个组件的时候会穿插一些整体性的介绍，比如源码组织啊、使用的一些三方库啊……；后面有些组件比较依赖其他较大的项目的，比如一个核心组件依赖于对client-go项目的理解，那就会先介绍client-go，当然client-go的介绍不会混在核心组件分析的章节中，我会单独分一个大类“周边项目源码分析”中。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 核心组件源码分析1. 概述"},"core/scheduler/":{"url":"core/scheduler/","title":"scheduler","keywords":"","body":"scheduler 本章 owner：farmer-hutao Scheduler部分我们先从设计原理上介绍，然后分析源码，最后针对一些主要算法做专题分析。 1. 本章规划 调度器设计 调度程序启动前逻辑 调度器框架 一般调度过程 预选过程 优选过程 抢占调度 调度器初始化 专题-亲和性调度2. FAQ 读者A提问：如果一个pod的资源占用只有100M，能够运行在一个node上，但是配置成了1000M，这个时候node上其实没有1000M，那么predicate过程还能不能过滤通过？ 回答：如果一个人需要100块钱，卡里有1000块钱，这时候找银行要10000块，银行会给吗？银行不会知道你实际需要多少，你告诉他10000，他就看你卡里有没有10000；同样对于k8s来说你配置了需要1000M，k8s就看node上有没有1000M，没有就调度失败。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 scheduler1. 本章规划2. FAQ"},"core/scheduler/design.html":{"url":"core/scheduler/design.html","title":"调度器设计","keywords":"","body":"调度器设计 概述 源码层级 调度算法 Predicates 和 priorities 策略 Scheduler 的拓展性 调度策略的修改 1. 概述 我们先整体了解一下Scheduler的设计原理，然后再看这些过程是如何用代码实现的。关于调度器的设计在官网有介绍，我下面结合官网给的说明，简化掉不影响理解的复杂部分，和大家介绍一下Scheduler的工作过程。 英文还可以的小伙伴们可以看一下官网的介绍先：scheduler.md 官网有一段描述如下： The Kubernetes scheduler runs as a process alongside the other master components such as the API server. Its interface to the API server is to watch for Pods with an empty PodSpec.NodeName, and for each Pod, it posts a binding indicating where the Pod should be scheduled. 简单翻译一下，也就是说Scheduler是一个跑在其他组件边上的独立程序，对接Apiserver寻找PodSpec.NodeName为空的Pod，然后用post的方式发送一个api调用，指定这些pod应该跑在哪个node上。 通俗地说，就是scheduler是相对独立的一个组件，主动访问api server，寻找等待调度的pod，然后通过一系列调度算法寻找哪个node适合跑这个pod，然后将这个pod和node的绑定关系发给api server，从而完成了调度的过程。 2. 源码层级 从高level看，scheduler的源码可以分为3层： cmd/kube-scheduler/scheduler.go: main() 函数入口位置，在scheduler过程开始被调用前的一系列初始化工作。 pkg/scheduler/scheduler.go: 调度框架的整体逻辑，在具体的调度算法之上的框架性的代码。 pkg/scheduler/core/generic_scheduler.go: 具体的计算哪些node适合跑哪些pod的算法。 3. 调度算法 调度过程整体如下图所示（官文里这个图没对齐，逼疯强迫症了！！！当然由于中文显示的问题，下图有中文的行也没法完全对齐，这个地方让我很抓狂。。。）： 对于一个给定的pod +---------------------------------------------+ | 可用于调度的nodes如下： | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 初步过滤: node 3 资源不足 +----------------------+----------------------+ | v +----------------------+----------------------+ | 剩下的nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 优先级算法计算结果: node 1: 分数=2 node 2: 分数=5 +----------------------+----------------------+ | v 选择分值最高的节点 = node 2 Scheduler为每个pod寻找一个适合其运行的node，大体分成三步： 通过一系列的“predicates”过滤掉不能运行pod的node，比如一个pod需要500M的内存，有些节点剩余内存只有100M了，就会被剔除； 通过一系列的“priority functions”给剩下的node排一个等级，分出三六九等，寻找能够运行pod的若干node中最合适的一个node； 得分最高的一个node，也就是被“priority functions”选中的node胜出了，获得了跑对应pod的资格。 4. Predicates 和 priorities 策略 Predicates是一些用于过滤不合适node的策略 . Priorities是一些用于区分node排名（分数）的策略（作用在通过predicates过滤的node上）. K8s默认内建了一些predicates 和 priorities 策略，官方文档介绍地址： scheduler_algorithm.md. Predicates 和 priorities 的代码分别在： pkg/scheduler/algorithm/predicates/predicates.go pkg/scheduler/algorithm/priorities. 5. Scheduler 的拓展性 我们可以选择哪些预置策略生效，也可以添加自己的策略。几个月前我司有个奇葩调度需求，当时我就是通过增加一个priorities策略，然后重新编译了一个Scheduler来实现的需求。 6. 调度策略的修改 默认调度策略是通过defaultPredicates() 和 defaultPriorities()函数定义的，源码在 pkg/scheduler/algorithmprovider/defaults/defaults.go，我们可以通过命令行flag --policy-config-file来覆盖默认行为。所以我们可以通过配置文件的方式或者修改pkg/scheduler/algorithm/predicates/predicates.go /pkg/scheduler/algorithm/priorities，然后注册到defaultPredicates()/defaultPriorities()来实现。配置文件类似下面这个样子： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\"}, {\"name\" : \"PodFitsResources\"}, {\"name\" : \"NoDiskConflict\"}, {\"name\" : \"NoVolumeZoneConflict\"}, {\"name\" : \"MatchNodeSelector\"}, {\"name\" : \"HostName\"} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1}, {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10, \"alwaysCheckAllPredicates\" : false } ok，看到这里大伙应该在流程上对Scheduler的原理有个感性的认识了，下一节我们就开始看一下Scheduler源码是怎么写的。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-27 16:24:20 调度器设计1. 概述2. 源码层级3. 调度算法4. Predicates 和 priorities 策略5. Scheduler 的拓展性6. 调度策略的修改"},"core/scheduler/before-scheduler-run.html":{"url":"core/scheduler/before-scheduler-run.html","title":"调度程序启动前逻辑","keywords":"","body":"调度程序启动前逻辑 概述 cobra和main cobra是啥 使用cobra Scheduler的main 1. 概述 前面提到过scheduler程序可以分为三层，第一层是调度器启动前的逻辑，包括命令行参数解析、参数校验、调度器初始化等一系列逻辑。这个部分我不会太详细地介绍，因为这些代码位于调度框架之前，相对比较枯燥无趣，讲多了磨灭大伙对源码的兴趣～ 2. cobra和main 剧透一下先，如果你之前没有用过cobra，那么在第一次见到cobra之后，很可能以后你自己写的程序，开发的小工具会全部变成cobra风格。我最近半年写的命令行程序就全部是基于cobra+pflag的。cobra有多优雅呢，且听我慢慢道来～ 2.1. cobra是啥 从github上我们可以找到这个项目，截至今天已经有上万个star，一百多个contributors，可见来头不小！Cobra官方描述是： Cobra is both a library for creating powerful modern CLI applications as well as a program to generate applications and command files. 也就是这个意思：Cobra既是一个创建强大的现代化命令行程序的库，又是一个用于生成应用和命令行文件的程序。有很多流行的Go项目用了Cobra，其中当然包括我们最最熟知的k8s和docker，大致列出来有这些： Kubernetes Hugo rkt etcd Moby (former Docker) Docker (distribution) OpenShift Delve GopherJS CockroachDB Bleve ProjectAtomic (enterprise) Giant Swarm's gsctl Nanobox/Nanopack rclone nehm Pouch 如果你是云计算方向的攻城狮，上面半数项目应该都耳熟能详～ 2.2. 使用cobra 下面我们实践一下cobra，先下载这个项目编译一下： # 如果你的网络很给力，那么下面这个命令就够了； go get -u github.com/spf13/cobra/cobra # 如果你的网络不给力，那就下载cobra的zip包，丢到GOPATH下对应目录，然后解决依赖，再build 于是我们得到了这样一个可执行文件及项目源码： 我们试一下这个命令：cobra init ${project-name} [root@farmer-hutao src]# cobra init myapp Your Cobra application is ready at /root/go/src/myapp Give it a try by going there and running `go run main.go`. Add commands to it by running `cobra add [cmdname]`. [root@farmer-hutao src]# ls myapp/ cmd LICENSE main.go [root@farmer-hutao src]# pwd /root/go/src 如上，本地可以看到一个main.go和一个cmd目录，这个cmd和k8s源码里的cmd是不是很像～ main.go里面的代码很精简，如下： main.go package main import \"myapp/cmd\" func main() { cmd.Execute() } 这里注意到调用了一个cmd的Execute()方法，我们继续看cmd是什么： 如上图，在main.go里面import了myapp/cmd，也就是这个root.go文件。所以Execute()函数就很好找了。在Execute里面调用了rootCmd.Execute()方法，这个rootCmd是*cobra.Command类型的。我们关注一下这个类型。 下面我们继续使用cobra命令给myapp添加一个子命令： 如上，我们的程序可以使用version子命令了！我们看一下源码发生了什么变化： 多了一个version.go，在这个源文件的init()函数里面调用了一个rootCmd.AddCommand(versionCmd)，这里可以猜到是根命令下添加一个子命令的意思，根命令表示的就是我们直接执行这个可执行文件，子命令就是version，放在一起的感觉就类似大家使用kubectl version的感觉。 另外注意到这里的Run属性是一个匿名函数，这个函数中输出了“version called”字样，也就是说我们执行version子命令的时候其实是调用到了这里的Run. 最后我们实践一下多级子命令： 套路也就这样，通过serverCmd.AddCommand(createCmd)调用后就能够把*cobra.Command类型的createCmd变成serverCmd的子命令了，这个时候我们玩起来就像kubectl get pods. 行，看到这里我们回头看一下scheduler的源码就能找到main的逻辑了。 3. Scheduler的main 我们打开文件：cmd/kube-scheduler/scheduler.go可以找到scheduler的main()函数，很简短，去掉枝干后如下： cmd/kube-scheduler/scheduler.go:34 func main() { command := app.NewSchedulerCommand() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } 看到这里猜都能猜到kube-scheduler这个二进制文件在运行的时候是调用了command.Execute()函数背后的那个Run，那个Run躲在command := app.NewSchedulerCommand()这行代码调用的NewSchedulerCommand()方法里，这个方法一定返回了一个*cobra.Command类型的对象。我们跟进去这个函数，看一下是不是这个样子： cmd/kube-scheduler/app/server.go:70 / NewSchedulerCommand creates a *cobra.Command object with default parameters func NewSchedulerCommand() *cobra.Command { cmd := &cobra.Command{ Use: \"kube-scheduler\", Long: `The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity. The scheduler needs to take into account individual and collective resource requirements, quality of service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, deadlines, and so on. Workload-specific requirements will be exposed through the API as necessary.`, Run: func(cmd *cobra.Command, args []string) { if err := runCommand(cmd, args, opts); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } return cmd } 如上，同样我先删掉了一些枝干代码，剩下的可以很清楚地看到，schduler启动时调用了runCommand(cmd, args, opts)，这个函数在哪里呢，继续跟一下： cmd/kube-scheduler/app/server.go:117 // runCommand runs the scheduler. func runCommand(cmd *cobra.Command, args []string, opts *options.Options) error { c, err := opts.Config() stopCh := make(chan struct{}) // Get the completed config cc := c.Complete() return Run(cc, stopCh) } 如上，可以看到这里是处理配置问题后调用了一个Run()函数，Run()的作用是基于给定的配置启动scheduler，它只会在出错时或者channel stopCh被关闭时才退出，代码主要部分如下： cmd/kube-scheduler/app/server.go:167 // Run executes the scheduler based on the given configuration. It only return on error or when stopCh is closed. func Run(cc schedulerserverconfig.CompletedConfig, stopCh 可以看到这里最终是要跑sched.Run()这个方法来启动scheduler，sched.Run()方法已经在pkg下，具体位置是pkg/scheduler/scheduler.go:276，也就是scheduler框架真正运行的逻辑了。于是我们已经从main出发，找到了scheduler主框架的入口，具体的scheduler逻辑我们下一讲再来仔细分析。 最后我们来看一下sched的定义，在linux里我们经常会看到一些软件叫做什么什么d，d也就是daemon，守护进程的意思，也就是一直跑在后台的一个程序。这里的sched也就是“scheduler daemon”的意思。sched的其实是*Scheduler类型，定义在： pkg/scheduler/scheduler.go:58 // Scheduler watches for new unscheduled pods. It attempts to find // nodes that they fit on and writes bindings back to the api server. type Scheduler struct { config *factory.Config } 如上，注释也很清晰，说Scheduler watch新创建的未被调度的pods，然后尝试寻找合适的node，回写一个绑定关系到api server.这里也可以体会到daemon的感觉，我们平时搭建的k8s集群中运行着一个daemon进程叫做kube-scheduler，这个一直跑着的进程做的就是上面注释里说的事情，在程序里面也就对应这样一个对象：Scheduler. Scheduler结构体中的Config对象我们再简单看一下： pkg/scheduler/factory/factory.go:96 // Config is an implementation of the Scheduler's configured input data. type Config struct { // It is expected that changes made via SchedulerCache will be observed // by NodeLister and Algorithm. SchedulerCache schedulerinternalcache.Cache // Ecache is used for optimistically invalid affected cache items after // successfully binding a pod Ecache *equivalence.Cache NodeLister algorithm.NodeLister Algorithm algorithm.ScheduleAlgorithm GetBinder func(pod *v1.Pod) Binder // PodConditionUpdater is used only in case of scheduling errors. If we succeed // with scheduling, PodScheduled condition will be updated in apiserver in /bind // handler so that binding and setting PodCondition it is atomic. PodConditionUpdater PodConditionUpdater // PodPreemptor is used to evict pods and update pod annotations. PodPreemptor PodPreemptor // NextPod should be a function that blocks until the next pod // is available. We don't use a channel for this, because scheduling // a pod may take some amount of time and we don't want pods to get // stale while they sit in a channel. NextPod func() *v1.Pod // SchedulingQueue holds pods to be scheduled SchedulingQueue internalqueue.SchedulingQueue } 如上，同样我只保留了一些好理解的字段，我们随便扫一下可以看到譬如：SchedulingQueue、NextPod、NodeLister这些很容易从字面上理解的字段，也就是Scheduler对象在工作（完成调度这件事）中需要用到的一些对象。 ok，下一讲我们开始聊Scheduler的工作过程！ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 11:13:36 调度程序启动前逻辑1. 概述2. cobra和main2.1. cobra是啥2.2. 使用cobra3. Scheduler的main"},"core/scheduler/scheduler-framework.html":{"url":"core/scheduler/scheduler-framework.html","title":"调度器框架","keywords":"","body":"调度器框架 写在前面 调度器启动运行 一个pod的调度流程 潜入第三层前的一点逻辑 1. 写在前面 今天我们从pkg/scheduler/scheduler.go出发，分析Scheduler的整体框架。前面讲Scheduler设计的时候有提到过源码的3层结构，pkg/scheduler/scheduler.go也就是中间这一层，负责Scheduler除了具体node过滤算法外的工作逻辑～ 这一层我们先尽可能找主线，顺着主线走通一遍，就像走一个迷宫，一条通路走出去后心里就有地了，但是迷宫中的很多角落是未曾涉足的。我们尽快走通主流程后，再就一些主要知识点专题攻破，比如k8s里面的List-Watch，Informer等好玩的东西。 2. 调度器启动运行 从goland的Structure中可以看到这个源文件(pkg/scheduler/scheduler.go)主要有这些对象： 大概浏览一下可以很快找到我们的第一个关注点应该是Scheduler这个struct和Scheduler的Run()方法： pkg/scheduler/scheduler.go:58 // Scheduler watches for new unscheduled pods. It attempts to find // nodes that they fit on and writes bindings back to the api server. type Scheduler struct { config *factory.Config } 这个struct在上一讲有跟到过，代码注释说的是： Scheduler watch新创建的未被调度的pods，然后尝试寻找合适的node，回写一个绑定关系到api server. 这个注释有个小问题就是用了复数形式，其实最后过滤出来的只有一个node；当然这种小问题知道就好，提到github上人家会觉得你在刷commit.接着往下看，Scheduler绑定了一个Run()方法，如下： pkg/scheduler/scheduler.go:276 // Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } 注释说这个函数开始watching and scheduling，也就是调度器主要逻辑了！注释后半段说到Run()方法起了一个goroutine后马上返回了，这个怎么理解呢？我们先看一下调用Run的地方： cmd/kube-scheduler/app/server.go:240 // Prepare a reusable runCommand function. run := func(ctx context.Context) { sched.Run() 可以发现调用了sched.Run()之后就在等待ctx.Done()了，所以Run中启动的goroutine自己不退出就ok. wait.Until这个函数做的事情是：每隔n时间调用f一次，除非channel c被关闭。这里的n就是0，也就是一直调用，前一次调用返回下一次调用就开始了。这里的f当然就是sched.scheduleOne，c就是sched.config.StopEverything. 3. 一个pod的调度流程 于是我们的关注点就转到了sched.scheduleOne这个方法上，看一下： scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. 注释里说scheduleOne实现1个pod的完整调度工作流，这个过程是顺序执行的，也就是非并发的。结合前面的wait.Until逻辑，也就是说前一个pod的scheduleOne一完成，一个return，下一个pod的scheduleOne立马接着执行！ 这里的串行逻辑也好理解，如果是同时调度N个pod，计算的时候觉得一个node很空闲，实际调度过去启动的时候发现别人的一群pod先起来了，端口啊，内存啊，全给你抢走了！所以这里的调度算法执行过程用串行逻辑很好理解。注意哦，调度过程跑完不是说要等pod起来，最后一步是写一个binding到apiserver，所以不会太慢。下面我们看一下scheduleOne的主要逻辑： pkg/scheduler/scheduler.go:513 func (sched *Scheduler) scheduleOne() { pod := sched.config.NextPod() suggestedHost, err := sched.schedule(pod) if err != nil { if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) } return } assumedPod := pod.DeepCopy() allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) err = sched.assume(assumedPod, suggestedHost) go func() { err := sched.bind(assumedPod, &v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) }() } 上面几行代码只保留了主干，对于我们理解scheduleOne的过程足够了，这里来个流程图吧： 不考虑scheduleOne的所有细节和各种异常情况，基本是上图的流程了，主流程的核心步骤当然是suggestedHost, err := sched.schedule(pod)这一行，这里完成了不需要抢占的场景下node的计算，我们耳熟能详的预选过程，优选过程等就是在这里面。 4. 潜入第三层前的一点逻辑 ok，这时候重点就转移到了suggestedHost, err := sched.schedule(pod)这个过程，强调一下这个过程是“同步”执行的。 pkg/scheduler/scheduler.go:290 // schedule implements the scheduling algorithm and returns the suggested host. func (sched *Scheduler) schedule(pod *v1.Pod) (string, error) { host, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) if err != nil { pod = pod.DeepCopy() sched.config.Error(pod, err) sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"%v\", err) sched.config.PodConditionUpdater.Update(pod, &v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, LastProbeTime: metav1.Now(), Reason: v1.PodReasonUnschedulable, Message: err.Error(), }) return \"\", err } return host, err } schedule方法很简短，我们关注一下第一行，调用sched.config.Algorithm.Schedule()方法，入参是pod和nodes，返回一个host，继续看一下这个Schedule方法： pkg/scheduler/algorithm/scheduler_interface.go:78 type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) Predicates() map[string]FitPredicate Prioritizers() []PriorityConfig } 发现是个接口，这个接口有4个方法，实现ScheduleAlgorithm接口的对象意味着知道如何调度pods到nodes上。默认的实现是pkg/scheduler/core/generic_scheduler.go:98 genericScheduler这个struct.我们先继续看一下ScheduleAlgorithm接口定义的4个方法： Schedule() //给定pod和nodes，计算出一个适合跑pod的node并返回； Preempt() //抢占 Predicates() //预选 Prioritizers() //优选 前面流程里讲到的sched.config.Algorithm.Schedule()也就是genericScheduler.Schedule()方法了，这个方法位于：pkg/scheduler/core/generic_scheduler.go:139一句话概括这个方法就是：尝试将指定的pod调度到给定的node列表中的一个，如果成功就返回这个node的名字。最后看一眼签名： func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) 从如参和返回值其实可以猜到很多东西，行，今天就到这里，具体的逻辑下回我们再分析～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-01 14:00:16 调度器框架1. 写在前面2. 调度器启动运行3. 一个pod的调度流程4. 潜入第三层前的一点逻辑"},"core/scheduler/generic-scheduler.html":{"url":"core/scheduler/generic-scheduler.html","title":"一般调度过程","keywords":"","body":"一般调度过程 进入Scheduler的第三层逻辑 Computing predicates Prioritizing Selecting host 1. 进入Scheduler的第三层逻辑 今天分析的代码，就已经算kube-scheduler的第三层逻辑了，我们要找到预选和优选的入口，讲完太长，干脆后面单独分2节讲预选和优选过程。所以本小节会比较简短哦～ 今天我们从pkg/scheduler/core/generic_scheduler.go:139开始，也就是从这个generic scheduler的Schedule()方法下手！ 我们依旧关心主干先，这个方法主要涉及的是预选过程+优选过程，看下主要代码： pkg/scheduler/core/generic_scheduler.go:139 func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { nodes, err := nodeLister.List() trace.Step(\"Computing predicates\") filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) trace.Step(\"Prioritizing\") priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) trace.Step(\"Selecting host\") return g.selectHost(priorityList) } 如上，我手一抖就删的只剩下这几行了，大伙应该从这不到十行的代码里找到3个步骤： \"Computing predicates\"：调用findNodesThatFit()方法； \"Prioritizing\"：调用PrioritizeNodes()方法； \"Selecting host\"：调用selectHost()方法。 接着当然是先浏览一下这3步分别完成了哪些工作咯～ 1.1. Computing predicates 这个过程的入口是： filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) 从变量命名上其实就可以猜到一大半，filteredNodes肯定就是过滤出来的nodes，也就是经受住了预选算法考验的node集合，我们从findNodesThatFit方法的函数签名中可以得到准确一些的信息： pkg/scheduler/core/generic_scheduler.go:389 func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) 入参是1个pod + 一堆node，返回值是一堆node（这个堆堆当然 1.2. Prioritizing Prioritizing的入口看着复杂一点： priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 注意到这里的返回值叫做priorityList，什么什么List也就是不止一个了，优选过程不是选出1个最佳节点吗？我们继续看： pkg/scheduler/core/generic_scheduler.go:624 func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) 首选关注返回值是什么意思： pkg/scheduler/api/types.go:305 type HostPriority struct { // Name of the host Host string // Score associated with the host Score int } // HostPriorityList declares a []HostPriority type. type HostPriorityList []HostPriority 看到这里就清晰了，原来有个HostPriority类型记录一个Host的名字和分值，HostPriorityList类型也就是HostPriority类型的集合，意味着记录了多个Host的名字和分值，于是我们可以判断PrioritizeNodes()方法的作用是计算前面的predicates过程筛选出来的nodes各自的Score.所以肯定还有一个根据Score决定哪个node胜出的逻辑咯～，继续往下看吧～ 1.3. Selecting host 这个过程比较明显了，我们直接看代码： pkg/scheduler/core/generic_scheduler.go:227 func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) 这个selectHost()方法大家应该都已经猜到了，就是从上一步的优选过程的结果集中选出一个Score最高的Host，并且返回这个Host的name. genericScheduler的Schedule()方法主要就是这3个过程，下一讲我们开始分析predicates过程。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-04 12:45:17 一般调度过程1. 进入Scheduler的第三层逻辑1.1. Computing predicates1.2. Prioritizing1.3. Selecting host"},"core/scheduler/predicate.html":{"url":"core/scheduler/predicate.html","title":"预选过程","keywords":"","body":"预选过程 预选流程 predicate的并发 一个node的predicate predicates的顺序 单个predicate执行过程 具体的predicate函数 1. 预选流程 predicate过程从pkg/scheduler/core/generic_scheduler.go:389 findNodesThatFit()方法就算正式开始了，这个方法根据给定的predicate functions过滤所有的nodes来寻找一堆可以跑pod的node集。老规矩，我们来看主干代码： pkg/scheduler/core/generic_scheduler.go:389 func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) { checkNode := func(i int) { fits, failedPredicates, err := podFitsOnNode( //…… ) if fits { length := atomic.AddInt32(&filteredLen, 1) filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) if len(filtered) > 0 && len(g.extenders) != 0 { for _, extender := range g.extenders { // Logic of extenders } } return filtered, failedPredicateMap, nil } 如上，删的有点多，大家也可以看一下原函数然后对比一下，看看我为什么只保留这一点。从上面代码中我们可以发现，最重要的是一个子函数调用过程fits, failedPredicates, err := podFitsOnNode()，这个函数的参数我没有贴出来，下面会详细讲；下半部分是一个extender过程，extender不影响对predicate过程的理解，我们后面专门当作一个主题讲。所以这里的关注点是podFitsOnNode()函数。 2. predicate的并发 进入podFitsOnNode()函数逻辑之前，我们先看一下调用到podFitsOnNode()函数的匿名函数变量checkNode是怎么被调用的： pkg/scheduler/core/generic_scheduler.go:458 workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) ParallelizeUntil()函数是用于并行执行N个独立的工作过程的，这个逻辑写的挺有意思，我们看一下完整的代码(这段的分析思路写到注释里哦)： vendor/k8s.io/client-go/util/workqueue/parallelizer.go:38 func ParallelizeUntil(ctx context.Context, workers, pieces int, doWorkPiece DoWorkPieceFunc) { // 从形参列表看，需要关注的有workers和pieces两个数字类型的参数，doworkPiece这个函数类型的参数 // DoWorkPieceFunc类型也就是func(piece int)类型 // 注意到上面调用的时候workers的实参是16，pieces是allNodes，也就是node数量 var stop 回想一下前面的checkNode := func(i int){……}，上面的doWorkPiece(piece)也就是调用到了这里的这个匿名函数func(i int){……}；到这里就清楚如何实现并发执行多个node的predicate过程了。 3. 一个node的predicate checkNode的主要逻辑就是上面介绍的并发加上下面这个podFitsOnNode()函数逻辑： pkg/scheduler/core/generic_scheduler.go:425 fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) 我们从podFitsOnNode()的函数定义入手： pkg/scheduler/core/generic_scheduler.go:537 func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, nodeCache *equivalence.NodeCache, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) 关于这个函数的逻辑，注释里的描述翻译过来大概是这个意思： podFitsOnNode()函数检查一个通过NodeInfo形式给定的node是否满足指定的predicate functions. 对于给定的一个Pod，podFitsOnNode()函数会检查是否有某个“等价的pod”存在，然后重用那个等价pod缓存的predicate结果。 这个函数的调用入口有2处: Schedule and Preempt. 当从Schedule进入时：这个函数想要测试node上所有已经存在的pod外加被指定将要调度到这个node上的其他所有高优先级（优先级不比自己低，也就是>=）的pod后，当前pod是否可以被调度到这个node上。 当从Preempt进入时：后面讲preempt时再详细分析。 podFitsOnNode()函数的参数有点多，每个跟进去就是一堆知识点。这里建议大家从字面先过一边，然后跟进去看一下类型定义，类型的注释等，了解一下功能，先不深究。整体看完一边调度器代码后回过头深入细节。 我们一起看一下其中这个参数：predicateFuncs map[string]algorithm.FitPredicate；这里的predicateFuncs是一个map，表示所有的predicate函数。这个map的key是个字符串，也就是某种形式的name了；value类型跟进去看一下： pkg/scheduler/algorithm/types.go:36 // FitPredicate is a function that indicates if a pod fits into an existing node. // The failure information is given by the error. type FitPredicate func(pod *v1.Pod, meta PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []PredicateFailureReason, error) FitPredicate是一个函数类型，3个参数，pod和node都很好理解，meta跟进去简单看一下可以发现定义的是一些和predicate相关的一些元数据，这些数据是根据pod和node信息获取到的，类似pod的端口有哪些，pod亲和的pod列表等。返回值是一个表示是否fit的bool值，predicate失败的原因列表，一个错误类型。 也就是说，FitPredicate这个函数类型也就是前面一直说的predicate functions的真面目了。下面看podFitsOnNode()函数的具体逻辑吧： pkg/scheduler/core/generic_scheduler.go:537 func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, nodeCache *equivalence.NodeCache, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) { podsAdded := false for i := 0; i 这里的逻辑是从一个for循环开始的，关于这个2次循环的含义代码里有很长的一段注释，我们先看一下注释里怎么说的（这里可以多看几遍体会一下）： 出于某些原因考虑我们需要运行两次predicate. 如果node上有更高或者相同优先级的“指定pods”（这里的“指定pods”指的是通过schedule计算后指定要跑在一个node上但是还未真正运行到那个node上的pods），我们将这些pods加入到meta和nodeInfo后执行一次计算过程。 如果这个过程所有的predicates都成功了，我们再假设这些“指定pods”不会跑到node上再运行一次。第二次计算是必须的，因为有一些predicates比如pod亲和性，也许在“指定pods”没有成功跑到node的情况下会不满足。 如果没有“指定pods”或者第一次计算过程失败了，那么第二次计算不会进行。 我们在第一次调度的时候只考虑相等或者更高优先级的pods，因为这些pod是当前pod必须“臣服”的，也就是说不能够从这些pod中抢到资源，这些pod不会被当前pod“抢占”；这样当前pod也就能够安心从低优先级的pod手里抢资源了。 新pod在上述2种情况下都可调度基于一个保守的假设：资源和pod反亲和性等的predicate在“指定pods”被处理为Running时更容易失败；pod亲和性在“指定pods”被处理为Not Running时更加容易失败。 我们不能假设“指定pods”是Running的因为它们当前还没有运行，而且事实上，它们确实有可能最终又被调度到其他node上了。 看了这个注释后，上面代码里的前几行就很好理解了，在第一次进入循环体和第二次进入时做了不同的处理，具体怎么做的处理我们暂时不关注。下面看省略的这个for循环做了啥： pkg/scheduler/core/generic_scheduler.go:583 // predicates.Ordering()得到的是一个[]string，predicate名字集合 for predicateID, predicateKey := range predicates.Ordering() { var ( fit bool reasons []algorithm.PredicateFailureReason err error ) // 如果predicateFuncs有这个key，则调用这个predicate；也就是说predicateFuncs如果定义了一堆乱七八遭的名字，会被忽略调，因为predicateKey是内置的。 if predicate, exist := predicateFuncs[predicateKey]; exist { // 降低难度，先不看缓存情况。 if eCacheAvailable { fit, reasons, err = nodeCache.RunPredicate(predicate, predicateKey, predicateID, pod, metaToUse, nodeInfoToUse, equivClass) } else { // 真正调用predicate函数了！！！！！！！！！ fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) } if err != nil { return false, []algorithm.PredicateFailureReason{}, err } if !fit { // …… } } } 如上，我们看一下2个地方： predicates.Ordering() fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) 分两个小节吧～ 3.1. predicates的顺序 pkg/scheduler/algorithm/predicates/predicates.go:130 var ( predicatesOrdering = []string{ CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred} ) 如上，这里定义了一个次序，前面的for循环遍历的是这个[]string，这样也就实现了不管predicateFuncs里定义了怎样的顺序，影响不了predicate的实际调用顺序。官网对于这个顺序有这样一个表格解释： Position Predicate comments (note, justification...) 1 CheckNodeConditionPredicate we really don’t want to check predicates against unschedulable nodes. 2 PodFitsHost we check the pod.spec.nodeName. 3 PodFitsHostPorts we check ports asked on the spec. 4 PodMatchNodeSelector check node label after narrowing search. 5 PodFitsResources this one comes here since it’s not restrictive enough as we do not try to match values but ranges. 6 NoDiskConflict Following the resource predicate, we check disk 7 PodToleratesNodeTaints check toleration here, as node might have toleration 8 PodToleratesNodeNoExecuteTaints check toleration here, as node might have toleration 9 CheckNodeLabelPresence labels are easy to check, so this one goes before 10 checkServiceAffinity - 11 MaxPDVolumeCountPredicate - 12 VolumeNodePredicate - 13 VolumeZonePredicate - 14 CheckNodeMemoryPressurePredicate doesn’t happen often 15 CheckNodeDiskPressurePredicate doesn’t happen often 16 InterPodAffinityMatches Most expensive predicate to compute 这个表格大家对着字面意思体会一下吧，基本还是可以联想到意义的。 当然这个顺序是可以被配置文件覆盖的，用户可以使用类似这样的配置： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\", \"order\": 2}, {\"name\" : \"PodFitsResources\", \"order\": 3}, {\"name\" : \"NoDiskConflict\", \"order\": 5}, {\"name\" : \"PodToleratesNodeTaints\", \"order\": 4}, {\"name\" : \"MatchNodeSelector\", \"order\": 6}, {\"name\" : \"PodFitsHost\", \"order\": 1} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1}, {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10 } 整体过完源码后我们再实际尝试一下这些特性，这一边先知道有这回事吧，ok，继续～ 3.2. 单个predicate执行过程 fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) 这行代码其实没有啥复杂逻辑，不过我们还是重复讲一下，清晰理解这一行很有必要。这里的predicate()来自前几行的if语句predicate, exist := predicateFuncs[predicateKey]，往前跟也就是FitPredicate类型，我们前面提过，类型定义在pkg/scheduler/algorithm/types.go:36，这个类型表示的是一个具体的predicate函数，这里使用predicate()也就是一个函数调用的语法，很和谐了。 3.3. 具体的predicate函数 一直在讲predicate，那么predicate函数到底长什么样子呢，我们从具体的实现函数找一个看一下。开始讲design的时候提到过predicate的实现在pkg/scheduler/algorithm/predicates/predicates.go文件中，先看一眼Structure吧： 这个文件中predicate函数有点多，这样看眼花，我们具体点开一个观察一下： pkg/scheduler/algorithm/predicates/predicates.go:277 func NoDiskConflict(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { for _, v := range pod.Spec.Volumes { for _, ev := range nodeInfo.Pods() { if isVolumeConflict(v, ev) { return false, []algorithm.PredicateFailureReason{ErrDiskConflict}, nil } } } return true, nil, nil } 我们知道predicate函数的特点，这样就很好在这个一千六百多行go文件中寻找predicate函数了。像上面这个NoDiskConflict()函数，参数是pod、meta和nodeinfo，很明显是FitPredicate类型的，标准的predicate函数。 这个函数的实现也特别简单，遍历pod的Volumes，然后对于pod的每一个Volume，遍历node上的每个pod，看是否和当前podVolume冲突。如果不fit就返回false加原因；如果fit就返回true，很清晰。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-06 18:50:29 预选过程1. 预选流程2. predicate的并发3. 一个node的predicate3.1. predicates的顺序3.2. 单个predicate执行过程3.3. 具体的predicate函数"},"core/scheduler/priority.html":{"url":"core/scheduler/priority.html","title":"优选过程","keywords":"","body":"优选过程 走近priority过程 PrioritizeNodes整体流程 Results Old Priority Function Map-Reduce Combine Scores Fun和Map-Reduce实例分析 InterPodAffinityPriority(Function) CalculateNodeAffinityPriorityMap(Map) CalculateNodeAffinityPriorityReduce(Reduce) 小结 1. 走近priority过程 pkg/scheduler/core/generic_scheduler.go:186 priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 今天的分析从这行代码开始。 PrioritizeNodes要做的事情是给已经通过predicate的nodes赋上一个分值，从而抉出一个最优node用于运行当前pod. 第一次看priority可能会一脸蒙，和predicate中的逻辑不太一样；大伙得耐下性子多思考，实在有障碍也可以先不求甚解，整体过完后再二刷代码，再不行三刷，总会大彻大悟的！ 从注释中可以找到关于PrioritizeNodes的原理(pkg/scheduler/core/generic_scheduler.go:624)： PrioritizeNodes通过并发调用一个个priority函数来给node排优先级。每一个priority函数会给一个1-10之间的分值，0最低10最高。 每一个priority函数可以有自己的权重，单个函数返回的分值*权重后得到一个加权分值，最终所有的加权分值加在一起就是这个node的最终分值。 然后我们先函数签名入手： pkg/scheduler/core/generic_scheduler.go:624 func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) 形参定义和返回值： pod *v1.Pod* // pod就不用说了； *nodeNameToInfo map[string]*schedulercache.NodeInfo // 这个也不需要讲，字面意思代表一切； meta interface{} // 和predicate里的meta不太一样，下面会贴个debug的图先，具体后面再看； priorityConfigs []algorithm.PriorityConfig // 包含优选算法各种信息，比较重要； nodes []*v1.Node // node集合，不需要解释了； extenders []algorithm.SchedulerExtender // extender逻辑放到后面单独讲。 meta实参长这个样子： 返回值只需要看一下schedulerapi.HostPriorityList类型的含义了，这个类型之前也提过，后面频繁涉及到操作这个结构，所以这里再贴一次，大伙得烂熟于心才行！ pkg/scheduler/api/types.go:305 type HostPriority struct { Host string Score int } type HostPriorityList []HostPriority 着重分析一下这2个type，虽然很简单，还是有必要啰嗦一下，必须记在心里。HostPriority这个struct的属性是Host和Score，一个是string一个是int，所以很明显HostPriority所能够保存的信息是一个节点的名字和分值，再仔细一点说就是这个结构保存的是一个node在一个priority算法计算后所得到的结果；然后看HostPriorityList类型，这个类型是上一个类型的“集合”，集合表达的是一个node多个算法还是多个node一个算法呢？稍微思考一下可以知道HostPriorityList中存的是多个Host和Score的组合，所以HostPriorityList这个结构是要保存一个算法作用于所有node之后，得到的所有node的Score信息的。（这里我们先理解成一个算法的结果，作为函数返回值这里肯定是要保留所有算法作用后的最终node的Score，所以函数后半部分肯定有combine分值的步骤。） 2. PrioritizeNodes整体流程 前面说到PrioritizeNodes()函数也就是node优选的具体逻辑，这个函数略长，我们分段讲解。 2.1. Results PrioritizeNodes()函数开头的逻辑很简单，我们先从第一行看到results定义的这一行。 pkg/scheduler/core/generic_scheduler.go:634 if len(priorityConfigs) == 0 && len(extenders) == 0 { // 这个if很明显是处理特殊场景的，就是优选算法一个都没有配置（extenders同样没有配置）的时候怎么做； // 这个result是要当作返回值的，HostPriorityList类型前面唠叨了很多了，大家得心里有数； result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { // 这一行代码是唯一的“逻辑了”，下面直到for结束都是简单代码；所以我们看一下EqualPriorityMap // 函数的作用就行了。这里我不贴代码，这个函数很短，作用就是设置每个node的Score相同（都为1） // hostPriority的类型也就是schedulerapi.HostPriority类型，再次强调这个类型是要烂熟于心的； hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } // 最终的result也就是设置了每个node的Score为1的schedulerapi.HostPriorityList类型数据； result = append(result, hostPriority) } return result, nil } // 这里只是简单定义3个变量，一把锁，一个并发等待相关的wg，一个错误集合errs； var ( mu = sync.Mutex{} wg = sync.WaitGroup{} errs []error ) // 这里定义了一个appendError小函数，逻辑很简单，并发场景下将错误信息收集到errs中； appendError := func(err error) { mu.Lock() defer mu.Unlock() errs = append(errs, err) } // 最后一个变量results也不难理解，类型是[]schedulerapi.HostPriorityList，这里需要注意这个类型 // 的作用，它保存的是所有算法作用所有node之后得到的结果集，相当于一个二维数组，每个格子是1个算法 // 作用于1个节点的结果，一行也就是1个算法作用于所有节点的结果；一行展成一个二维就是所有算法作用于所有节点； results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs)) 到这里要求大家心中能够想象上面提到的results是什么样的，可以借助纸笔画一画。下面的代码会往这个二维结构里面存储数据。 2.2. Old Priority Function 我们既然讲到“老式”，后面肯定有对应的“新式”。虽然这种函数已经DEPRECATED了，不过对于我们学习掌握优选流程还是很有帮助的。默认的优选算法里其实也只有1个是这在old形式的了： 贴这块代码之前我们先关注一下多次出现的priorityConfigs这个变量的类型： 函数形参中有写到：priorityConfigs []algorithm.PriorityConfig，所以我们直接看PriorityConfig是什么类型： pkg/scheduler/algorithm/types.go:62 // PriorityConfig is a config used for a priority function. type PriorityConfig struct { Name string Map PriorityMapFunction Reduce PriorityReduceFunction // TODO: Remove it after migrating all functions to // Map-Reduce pattern. Function PriorityFunction Weight int } PriorityConfig中有一个Name，一个Weight，很好猜到意思，名字和权重嘛。剩下的Map、Reduce和Function目测代表的就是优选函数的新旧两种表达方式了。我们先看旧的Function属性的类型PriorityFunction是什么： pkg/scheduler/algorithm/types.go:59 type PriorityFunction func(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) 很明显这个类型代表了一个priority函数，入参是pod、nodeNameToInfo和nodes，返回值是HostPriorityList，也就是我们前面提到的1个priority函数作用于每个node后得到了Score信息，存结果的结构就是这个HostPriorityList；看起来很和谐～ 然后讲回PrioritizeNodes过程： pkg/scheduler/core/generic_scheduler.go:661 for i := range priorityConfigs { // 如果第i个优选配置(priorityConfig)定义了老函数，则调用之； if priorityConfigs[i].Function != nil { wg.Add(1) // 注意这里的参数index，这里传入的实参是上面的i； go func(index int) { defer wg.Done() var err error // 所以这里的results[index]就好理解了；后面priorityConfigs[index]的索引也是index， // 这里表达的是第N个优选配置里有Function，那么这个Function的计算结果保存在 // results的第N个格子里； results[index], err = priorityConfigs[index].Function(pod, nodeNameToInfo, nodes) if err != nil { appendError(err) } }(i) } else { // 如果没有定义Function，其实也就是使用了Map-Reduce方式的，这里先存个空的结构占位； results[i] = make(schedulerapi.HostPriorityList, len(nodes)) } } 上面这段代码逻辑还算好理解，唯一有点小绕的还是前面强调的HostPriorityList相关类型的操作上。 2.3. Map-Reduce 关于map-reduce思想我就不在这里赘述了，大数据行业很流行的一个词汇，百度一下（如果你能够google，自然更好咯）可以找到一大堆介绍的文章。 简单说map-reduce就是：Map是映射，Reduce是归约；map是统计一本书中的一页出现了多少次k8s这个词，reduce是将这些map结果汇总在一起得到最终结果。（map一般都是将一个算法作用于一堆数据集的每一个元素，得到一个结果集，reduce有各种形式，可以是累加这些结果，或者是对这个结果集做其他复杂的f(x)操作。 看看在Scheduler里面是怎么用Map-Reduce的吧： // 这个并发逻辑之前介绍过了，我们直接看ParallelizeUntil的最后一个参数就行，这里直接写了一个匿名函数； workqueue.ParallelizeUntil(context.TODO(), 16, len(nodes), func(index int) { // 这里的index是[0，len(nodes)-1]，相当于遍历所有的nodes； nodeInfo := nodeNameToInfo[nodes[index].Name] // 这个for循环遍历的是所有的优选配置，如果有老Fun就跳过，新逻辑就继续； for i := range priorityConfigs { if priorityConfigs[i].Function != nil { // 因为前面old已经运行过了 continue } var err error // 这里的i和前面老Fun的互补，老Fun中没有赋值的results中的元素就在这里赋值了； // 注意到这里调用了一个Map函数就直接赋值给了results[i][index]，这里的index是第一行这个 // 匿名函数的形参，通过ParallelizeUntil这个并发实现所有node对应一个优选算法的分值计算； results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) results[i][index].Host = nodes[index].Name } } }) for i := range priorityConfigs { // 没有定义Reduce函数就不处理； if priorityConfigs[i].Reduce == nil { continue } wg.Add(1) go func(index int) { defer wg.Done() // 调用Reduce函数 if err := priorityConfigs[index].Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if klog.V(10) { for _, hostPriority := range results[index] { klog.Infof(\"%v -> %v: %v, Score: (%d)\", util.GetPodFullName(pod), hostPriority.Host, priorityConfigs[index].Name, hostPriority.Score) } } }(i) } // Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } 看到这里我们可以发现老Fun和Map的区别不大，都是优选函数的执行过程。那为什么会存在两种形式呢？我们看完PrioritizeNodes整体流程后通过具体的Fun和Map-Reduce实现来看二者的区别。 2.4. Combine Scores 这块的代码很简单，我们先抛开extender的逻辑，剩下的代码如下： // Summarize all scores. // 这个result和前面的results类似，result用于存储每个node的Score，到这里已经没有必要区分算法了； result := make(schedulerapi.HostPriorityList, 0, len(nodes)) // 循环执行len(nodes)次 for i := range nodes { // 先在result中塞满所有node的Name，Score初始化为0； result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0}) // 执行了多少个priorityConfig就有多少个Score，所以这里遍历len(priorityConfigs)次； for j := range priorityConfigs { // 每个算法对应第i个node的结果分值加权后累加； result[i].Score += results[j][i].Score * priorityConfigs[j].Weight } } return result, nil 这块逻辑很清晰，要将前面得到的二维结果results压缩成一维的加权分值集合result，最终返回这个result. 从这里我们还可以得到一个结论，不管是Fun还是Map-Reduce，处理的结果都是填充results这个二维结构，所以Map-Reduce也没有什么神秘的，下面通过具体的算法来看二者有何异同。 3. Fun和Map-Reduce实例分析 3.1. InterPodAffinityPriority(Function) 这个算法做的是Pod间亲和性优选，也就是亲和pod越多的节点分值越高，反亲和pod越多分值越低。 我们撇开具体的亲和性计算规则，从优选函数的形式上看一下这段代码的逻辑： pkg/scheduler/algorithm/priorities/interpod_affinity.go:119 func (ipa *InterPodAffinity) CalculateInterPodAffinityPriority(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) { affinity := pod.Spec.Affinity // 是否有亲和性约束； hasAffinityConstraints := affinity != nil && affinity.PodAffinity != nil // 是否有反亲和性约束； hasAntiAffinityConstraints := affinity != nil && affinity.PodAntiAffinity != nil // 这里有一段根据亲和性和反亲和性来计算一个node上匹配的pod数量的逻辑，我们先跳过这些逻辑，从优选算法实现的角度看这个算法的架子； // 当遍历完所有的node之后，可以得到1个最高分和1个最低分，分别记为maxCount和minCount； for _, node := range nodes { if pm.counts[node.Name] > maxCount { maxCount = pm.counts[node.Name] } if pm.counts[node.Name] 0 { // MaxPriority定义的是优选最高分10，第二个因数是当前node的count-最小count， // 然后除以(maxCount - minCount)；举个例子，当前node的计算结果是5，最大count是20，最小 // count是-3，那么这里就是10*[5-(-3)/20-(-3)] // 这个计算的结果显然会在[0-10]之间； fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) } // 如果分差不大于0，这时候int(fScore)也就是0，对于各个node的结果都是0； result = append(result, schedulerapi.HostPriority{Host: node.Name, Score: int(fScore)}) } return result, nil } 如上，我们可以发现最终这个函数计算出了每个node的分值，这个分值在[0-10]之间。所以说到底Function做的事情就是根据一定的规则给每个node赋一个分值，这个分值要求在[0-10]之间，然后把这个HostPriorityList返回就行。 3.2. CalculateNodeAffinityPriorityMap(Map) 这个算法和上一个类似，上一个是Pod的Affinity，这个是Node的Affinity；我们来看代码： pkg/scheduler/algorithm/priorities/node_affinity.go:34 func CalculateNodeAffinityPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } // default is the podspec. affinity := pod.Spec.Affinity if priorityMeta, ok := meta.(*priorityMetadata); ok { // We were able to parse metadata, use affinity from there. affinity = priorityMeta.affinity } var count int32 if affinity != nil && affinity.NodeAffinity != nil && affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution != nil { // Match PreferredDuringSchedulingIgnoredDuringExecution term by term. for i := range affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution { preferredSchedulingTerm := &affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution[i] if preferredSchedulingTerm.Weight == 0 { continue } nodeSelector, err := v1helper.NodeSelectorRequirementsAsSelector(preferredSchedulingTerm.Preference.MatchExpressions) if err != nil { return schedulerapi.HostPriority{}, err } if nodeSelector.Matches(labels.Set(node.Labels)) { count += preferredSchedulingTerm.Weight } } } return schedulerapi.HostPriority{ Host: node.Name, Score: int(count), }, nil } 撇开具体的亲和性计算细节，我们可以发现这个的count没有特定的规则，可能会加到10以上；另外这里的返回值是HostPriority类型，前面的Function返回了HostPriorityList类型。 map函数 pkg/scheduler/algorithm/priorities/selector_spreading.go:221 func (s *ServiceAntiAffinity) CalculateAntiAffinityPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { var firstServiceSelector labels.Selector node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } priorityMeta, ok := meta.(*priorityMetadata) if ok { firstServiceSelector = priorityMeta.podFirstServiceSelector } else { firstServiceSelector = getFirstServiceSelector(pod, s.serviceLister) } // 查找给定node在给定namespace下符合selector的pod，返回值是[]*v1.Pod matchedPodsOfNode := filteredPod(pod.Namespace, firstServiceSelector, nodeInfo) return schedulerapi.HostPriority{ Host: node.Name, // 返回值中Score设置成上面找到的pod的数量 Score: int(len(matchedPodsOfNode)), }, nil } 这个函数比较短，可以看到在指定node上查询到匹配selector的pod越多，分值就越高。假设找到了20个，那么这里的分值就是20；假设找到的是2，那这里的分值就是2. 3.3. CalculateNodeAffinityPriorityReduce(Reduce) 和上面这个Map对应的Reduce函数其实没有单独实现，通过NormalizeReduce函数做了一个通用的Reduce处理： pkg/scheduler/algorithm/priorities/node_affinity.go:77 var CalculateNodeAffinityPriorityReduce = NormalizeReduce(schedulerapi.MaxPriority, false) pkg/scheduler/algorithm/priorities/reduce.go:29 func NormalizeReduce(maxPriority int, reverse bool) algorithm.PriorityReduceFunction { return func( _ *v1.Pod, _ interface{}, _ map[string]*schedulercache.NodeInfo, // 注意到这个result是HostPriorityList，对应1个算法N个node的结果集 result schedulerapi.HostPriorityList) error { var maxCount int // 遍历result将最高的Score赋值给maxCount； for i := range result { if result[i].Score > maxCount { maxCount = result[i].Score } } if maxCount == 0 { if reverse { for i := range result { result[i].Score = maxPriority } } return nil } for i := range result { score := result[i].Score // 举个例子：10*(5/20) score = maxPriority * score / maxCount if reverse { // 如果score是3，得到7；如果score是4，得到6，结果反转； score = maxPriority - score } result[i].Score = score } return nil } } 3.4. 小结 Function：一个算法一次性计算出所有node的Score，这个Score的范围是规定的[0-10]； Map-Reduce：一个Map算法计算1个node的Score，这个Score可以灵活处理，可能是20，可能是-3；Map过程并发进行；最终得到的结果result通过Reduce归约，将这个算法对应的所有node的分值归约为[0-10]； 本节有几张图是goland debug的截图，我们目前还没有提到如何debug；不过本节内容的阅读基本是不影响的。下一节源码分析内容发出来前我会在“环境准备”这一章中增加如何开始debug的内容，大家可以选择开始debug的时机。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-19 20:21:31 优选过程1. 走近priority过程2. PrioritizeNodes整体流程2.1. Results2.2. Old Priority Function2.3. Map-Reduce2.4. Combine Scores3. Fun和Map-Reduce实例分析3.1. InterPodAffinityPriority(Function)3.2. CalculateNodeAffinityPriorityMap(Map)3.3. CalculateNodeAffinityPriorityReduce(Reduce)3.4. 小结"},"core/scheduler/preempt.html":{"url":"core/scheduler/preempt.html","title":"抢占调度","keywords":"","body":"抢占调度 Pod priority preempt 入口 preempt 实现 SchedulingQueue FIFO PriorityQueue PodPreemptor xx.Algorithm.Preempt 接口定义 整体流程 podEligibleToPreemptOthers nodesWherePreemptionMightHelp selectNodesForPreemption pickOneNodeForPreemption 小结 1. Pod priority Pod 有了 priority(优先级) 后才有优先级调度、抢占调度的说法，高优先级的 pod 可以在调度队列中排到前面，优先选择 node；另外当高优先级的 pod 找不到合适的 node 时，就会看 node 上低优先级的 pod 驱逐之后是否能够 run 起来，如果可以，那么 node 上的一个或多个低优先级的 pod 会被驱逐，然后高优先级的 pod 得以成功运行1个 node 上。 今天我们分析 pod 抢占相关的代码。开始之前我们看一下和 priority 相关的2个示例配置文件： PriorityClass 例子 apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false description: \"This priority class should be used for XYZ service pods only.\" 使用上述 PriorityClass apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent priorityClassName: high-priority 这两个文件的内容这里不解释，Pod priority 相关知识点不熟悉的小伙伴请先查阅官方文档，我们下面看调度器中和 preempt 相关的代码逻辑。 2. preempt 入口 在pkg/scheduler/scheduler.go:513 scheduleOne()方法中我们上一次关注的是suggestedHost, err := sched.schedule(pod)这行代码，也就是关注通常情况下调度器如何给一个 pod 匹配一个最合适的 node. 今天我们来看如果这一行代码返回的 err != nil 情况下，如何开始 preempt 过程。 pkg/scheduler/scheduler.go:529 suggestedHost, err := sched.schedule(pod) if err != nil { if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() } else { klog.Errorf(\"error selecting node for pod: %v\", err) metrics.PodScheduleErrors.Inc() } return } 当schedule()函数没有返回 host，也就是没有找到合适的 node 的时候，就会出发 preempt 过程。这时候代码逻辑进入sched.preempt(pod, fitError)这一行。我们先看一下这个函数的整体逻辑，然后深入其中涉及的子过程： pkg/scheduler/scheduler.go:311 func (sched *Scheduler) preempt(preemptor *v1.Pod, scheduleErr error) (string, error) { // 特性没有开启就返回 \"\" if !util.PodPriorityEnabled() || sched.config.DisablePreemption { return \"\", nil } // 更新 pod 信息；入参和返回值都是 *v1.Pod 类型 preemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) // preempt 过程，下文分析 node, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) var nodeName = \"\" if node != nil { nodeName = node.Name // 更新队列中“任命pod”队列 sched.config.SchedulingQueue.UpdateNominatedPodForNode(preemptor, nodeName) // 设置pod的Status.NominatedNodeName err = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) if err != nil { // 如果出错就从 queue 中移除 sched.config.SchedulingQueue.DeleteNominatedPodIfExists(preemptor) return \"\", err } for _, victim := range victims { // 将要驱逐的 pod 驱逐 if err := sched.config.PodPreemptor.DeletePod(victim); err != nil { return \"\", err } sched.config.Recorder.Eventf(victim, v1.EventTypeNormal, \"Preempted\", \"by %v/%v on node %v\", preemptor.Namespace, preemptor.Name, nodeName) } } // Clearing nominated pods should happen outside of \"if node != nil\". // 这个清理过程在上面的if外部，我们回头从 Preempt() 的实现去理解 for _, p := range nominatedPodsToClear { rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil { klog.Errorf(\"Cannot remove nominated node annotation of pod: %v\", rErr) // We do not return as this error is not critical. } } return nodeName, err } 3. preempt 实现 上面 preempt() 函数中涉及到了一些值得深入看看的对象，下面我们逐个看一下这些对象的实现。 3.1. SchedulingQueue SchedulingQueue 表示的是一个存储待调度 pod 的队列 pkg/scheduler/internal/queue/scheduling_queue.go:60 type SchedulingQueue interface { Add(pod *v1.Pod) error AddIfNotPresent(pod *v1.Pod) error AddUnschedulableIfNotPresent(pod *v1.Pod) error Pop() (*v1.Pod, error) Update(oldPod, newPod *v1.Pod) error Delete(pod *v1.Pod) error MoveAllToActiveQueue() AssignedPodAdded(pod *v1.Pod) AssignedPodUpdated(pod *v1.Pod) NominatedPodsForNode(nodeName string) []*v1.Pod WaitingPods() []*v1.Pod Close() UpdateNominatedPodForNode(pod *v1.Pod, nodeName string) DeleteNominatedPodIfExists(pod *v1.Pod) NumUnschedulablePods() int } 在 Scheduler 中 SchedulingQueue 接口对应两种实现： FIFO 先进先出队列 PriorityQueue 优先级队列 3.1.1. FIFO FIFO 结构是对 cache.FIFO 的简单包装，然后实现了 SchedulingQueue 接口。 pkg/scheduler/internal/queue/scheduling_queue.go:97 type FIFO struct { *cache.FIFO } cache.FIFO定义在vendor/k8s.io/client-go/tools/cache/fifo.go:93，这个先进先出队列的细节先不讨论。 3.1.2. PriorityQueue PriorityQueue 同样实现了 SchedulingQueue 接口，PriorityQueue 的顶是最高优先级的 pending pod. 这里的PriorityQueue 有2个子 queue，activeQ 放的是等待调度的 pod，unschedulableQ 放的是已经尝试过调度，然后失败了，被标记为 unschedulable 的 pod. 我们看一下 PriorityQueue 结构的定义： pkg/scheduler/internal/queue/scheduling_queue.go:201 type PriorityQueue struct { stop PriorityQueue 的方法比较好理解，我们看几个吧： 1、func (p *PriorityQueue) Add(pod *v1.Pod) error //在 active queue 中添加1个pod pkg/scheduler/internal/queue/scheduling_queue.go:276 func (p *PriorityQueue) Add(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() // 直接在 activeQ 中添加 pod err := p.activeQ.Add(pod) if err != nil { klog.Errorf(\"Error adding pod %v/%v to the scheduling queue: %v\", pod.Namespace, pod.Name, err) } else { // 如果在 unschedulableQ 中找到这个 pod，抛错误日志后移除队列中该 pod if p.unschedulableQ.get(pod) != nil { klog.Errorf(\"Error: pod %v/%v is already in the unschedulable queue.\", pod.Namespace, pod.Name) p.unschedulableQ.delete(pod) } // 队列的 nominatedPods 属性中标记该 pod 不指定到任何 node p.nominatedPods.add(pod, \"\") p.cond.Broadcast() } return err } 2、func (p *PriorityQueue) AddIfNotPresent(pod *v1.Pod) error//如果2个队列中都不存在该 pod，那么就添加到 active queue 中 pkg/scheduler/internal/queue/scheduling_queue.go:295 func (p *PriorityQueue) AddIfNotPresent(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() //如果队列 unschedulableQ 中有 pod，啥也不做 if p.unschedulableQ.get(pod) != nil { return nil } //如果队列 activeQ 中有 pod，啥也不做 if _, exists, _ := p.activeQ.Get(pod); exists { return nil } // 添加 pod 到 activeQ err := p.activeQ.Add(pod) if err != nil { klog.Errorf(\"Error adding pod %v/%v to the scheduling queue: %v\", pod.Namespace, pod.Name, err) } else { p.nominatedPods.add(pod, \"\") p.cond.Broadcast() } return err } 3、func (p *PriorityQueue) flushUnschedulableQLeftover()//刷新 unschedulableQ 中的 pod，如果一个 pod 的呆的时间超过了 durationStayUnschedulableQ，就移动到 activeQ 中 pkg/scheduler/internal/queue/scheduling_queue.go:346 func (p *PriorityQueue) flushUnschedulableQLeftover() { p.lock.Lock() defer p.lock.Unlock() var podsToMove []*v1.Pod currentTime := p.clock.Now() // 遍历 unschedulableQ 中的 pod for _, pod := range p.unschedulableQ.pods { lastScheduleTime := podTimestamp(pod) // 这里的默认值是 60s，所以超过 60s 的 pod 将得到进入 activeQ 的机会 if !lastScheduleTime.IsZero() && currentTime.Sub(lastScheduleTime.Time) > unschedulableQTimeInterval { podsToMove = append(podsToMove, pod) } } if len(podsToMove) > 0 { // 全部移到 activeQ 中，又有机会被调度了 p.movePodsToActiveQueue(podsToMove) } } 4、func (p *PriorityQueue) Pop() (*v1.Pod, error)//从 activeQ 中 pop 一个 pod pkg/scheduler/internal/queue/scheduling_queue.go:367 func (p *PriorityQueue) Pop() (*v1.Pod, error) { p.lock.Lock() defer p.lock.Unlock() for len(p.activeQ.data.queue) == 0 { // 当队列为空的时候会阻塞 if p.closed { return nil, fmt.Errorf(queueClosed) } p.cond.Wait() } obj, err := p.activeQ.Pop() if err != nil { return nil, err } pod := obj.(*v1.Pod) // 标记 receivedMoveRequest 为 false，表示新的一次调度开始了 p.receivedMoveRequest = false return pod, err } 再看个别 PriorityQueue.nominatedPods 属性相关操作的方法，也就是 preempt() 函数中多次调用到的方法： 5、`func (p PriorityQueue) UpdateNominatedPodForNode(pod v1.Pod, nodeName string)`//pod 抢占的时候，确定一个 node 可以用于跑这个 pod 时，通过调用这个方法将 pod nominated 到 指定的 node 上。 pkg/scheduler/internal/queue/scheduling_queue.go:567 func (p *PriorityQueue) UpdateNominatedPodForNode(pod *v1.Pod, nodeName string) { p.lock.Lock() //逻辑在这里面 p.nominatedPods.add(pod, nodeName) p.lock.Unlock() } 先看 nominatedPods 属性的类型，这个类型用于存储 pods 被 nominate 到 nodes 的信息： pkg/scheduler/internal/queue/scheduling_queue.go:822 type nominatedPodMap struct { // key 是 node name，value 是 nominated 到这个 node 上的 pods nominatedPods map[string][]*v1.Pod // 和上面结构相反，key 是 pod 信息，值是 node 信息 nominatedPodToNode map[ktypes.UID]string } 在看一下add()方法的实现： pkg/scheduler/internal/queue/scheduling_queue.go:832 func (npm *nominatedPodMap) add(p *v1.Pod, nodeName string) { // 不管有没有，先删一下，防止重了 npm.delete(p) nnn := nodeName // 如果传入的 nodeName 是 “” if len(nnn) == 0 { // 查询 pod 的 pod.Status.NominatedNodeName nnn = NominatedNodeName(p) // 如果 pod.Status.NominatedNodeName 也是 “”,return if len(nnn) == 0 { return } } // 逻辑到这里说明要么 nodeName 不为空字符串，要么 nodeName 为空字符串但是 pod 的 pod.Status.NominatedNodeName 不为空字符串，这时候开始下面的赋值 npm.nominatedPodToNode[p.UID] = nnn for _, np := range npm.nominatedPods[nnn] { if np.UID == p.UID { klog.V(4).Infof(\"Pod %v/%v already exists in the nominated map!\", p.Namespace, p.Name) return } } npm.nominatedPods[nnn] = append(npm.nominatedPods[nnn], p) } 3.2. PodPreemptor PodPreemptor 用来驱逐 pods 和更新 pod annotations. pkg/scheduler/factory/factory.go:145 type PodPreemptor interface { GetUpdatedPod(pod *v1.Pod) (*v1.Pod, error) DeletePod(pod *v1.Pod) error SetNominatedNodeName(pod *v1.Pod, nominatedNode string) error RemoveNominatedNodeName(pod *v1.Pod) error } 这个 interface 对应的实现类型是： pkg/scheduler/factory/factory.go:1620 type podPreemptor struct { Client clientset.Interface } 这个类型绑定了4个方法： pkg/scheduler/factory/factory.go:1624 // 新获取一次 pod 的信息 func (p *podPreemptor) GetUpdatedPod(pod *v1.Pod) (*v1.Pod, error) { return p.Client.CoreV1().Pods(pod.Namespace).Get(pod.Name, metav1.GetOptions{}) } // 删除一个 pod func (p *podPreemptor) DeletePod(pod *v1.Pod) error { return p.Client.CoreV1().Pods(pod.Namespace).Delete(pod.Name, &metav1.DeleteOptions{}) } // 设置pod.Status.NominatedNodeName 为指定的 node name func (p *podPreemptor) SetNominatedNodeName(pod *v1.Pod, nominatedNodeName string) error { podCopy := pod.DeepCopy() podCopy.Status.NominatedNodeName = nominatedNodeName _, err := p.Client.CoreV1().Pods(pod.Namespace).UpdateStatus(podCopy) return err } // 清空 pod.Status.NominatedNodeName func (p *podPreemptor) RemoveNominatedNodeName(pod *v1.Pod) error { if len(pod.Status.NominatedNodeName) == 0 { return nil } return p.SetNominatedNodeName(pod, \"\") } 3.3. xx.Algorithm.Preempt 3.3.1. 接口定义 我们回到挺久之前讲常规调度过程的时候提过的一个接口： pkg/scheduler/algorithm/scheduler_interface.go:78 type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) // Preempt 在 pod 调度发生失败的时候尝试抢占低优先级的 pod. // 返回发生 preemption 的 node, 被 preempt的 pods 列表, // nominated node name 需要被移除的 pods 列表，一个 error 信息. Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) Predicates() map[string]FitPredicate Prioritizers() []PriorityConfig } 这个接口上次我们讲到的时候关注了Schedule()、Predicates()和Prioritizers()，这次来看Preempt()是怎么实现的。 3.3.2. 整体流程 Preempt()同样由genericScheduler类型(pkg/scheduler/core/generic_scheduler.go:98)实现，方法前的一大串英文注释先来理解一下： Preempt 寻找一个在发生抢占之后能够成功调度“pod”的node. Preempt 选择一个 node 然后抢占上面的 pods 资源，返回： 这个 node 信息 被抢占的 pods 信息 nominated node name 需要被清理的 node 列表 可能有的 error Preempt 过程不涉及快照更新（快照的逻辑以后再讲） 避免出现这种情况：preempt 发现一个不需要驱逐任何 pods 就能够跑“pod”的 node. 当有很多 pending pods 在调度队列中的时候，a nominated pod 会排到队列中相同优先级的 pod 后面. The nominated pod 会阻止其他 pods 使用“指定”的资源，哪怕花费了很多时间来等待其他 pending 的 pod. 我们先过整体流程，然后逐个分析子流程调用： pkg/scheduler/core/generic_scheduler.go:251 func (g *genericScheduler) Preempt(pod *v1.Pod, nodeLister algorithm.NodeLister, scheduleErr error) (*v1.Node, []*v1.Pod, []*v1.Pod, error) { // 省略几行 // 判断执行驱逐操作是否合适 if !podEligibleToPreemptOthers(pod, g.cachedNodeInfoMap) { klog.V(5).Infof(\"Pod %v/%v is not eligible for more preemption.\", pod.Namespace, pod.Name) return nil, nil, nil, nil } // 所有的 nodes allNodes, err := nodeLister.List() if err != nil { return nil, nil, nil, err } if len(allNodes) == 0 { return nil, nil, nil, ErrNoNodesAvailable } // 计算潜在的执行驱逐后能够用于跑 pod 的 nodes potentialNodes := nodesWherePreemptionMightHelp(allNodes, fitError.FailedPredicates) if len(potentialNodes) == 0 { klog.V(3).Infof(\"Preemption will not help schedule pod %v/%v on any node.\", pod.Namespace, pod.Name) // In this case, we should clean-up any existing nominated node name of the pod. return nil, nil, []*v1.Pod{pod}, nil } // 列出 pdb 对象 pdbs, err := g.pdbLister.List(labels.Everything()) if err != nil { return nil, nil, nil, err } // 计算所有 node 需要驱逐的 pods 有哪些等，后面细讲 nodeToVictims, err := selectNodesForPreemption(pod, g.cachedNodeInfoMap, potentialNodes, g.predicates, g.predicateMetaProducer, g.schedulingQueue, pdbs) if err != nil { return nil, nil, nil, err } // 拓展调度的逻辑 nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil { return nil, nil, nil, err } // 选择1个 node 用于 schedule candidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil { return nil, nil, nil, err } // 低优先级的被 nominate 到这个 node 的 pod 很可能已经不再 fit 这个 node 了，所以 // 需要移除这些 pod 的 nomination，更新这些 pod，挪动到 activeQ 中，让调度器 // 得以寻找另外一个 node 给这些 pod nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.cachedNodeInfoMap[candidateNode.Name]; ok { return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, err } return nil, nil, nil, fmt.Errorf( \"preemption failed: the target node %s has been deleted from scheduler cache\", candidateNode.Name) } 上面涉及到一些子过程调用，我们逐个来看～ podEligibleToPreemptOthers() // 如何判断是否适合抢占？ nodesWherePreemptionMightHelp() // 怎么寻找能够用于 preempt 的 nodes？ selectNodesForPreemption() // 这个过程计算的是什么？ pickOneNodeForPreemption() // 怎么从选择最合适被抢占的 node？ 3.3.3. podEligibleToPreemptOthers podEligibleToPreemptOthers 做的事情是判断一个 pod 是否应该去抢占其他 pods. 如果这个 pod 已经抢占过其他 pods，那些 pods 还在 graceful termination period 中，那就不应该再次发生抢占。 如果一个 node 已经被这个 pod nominated，并且这个 node 上有处于 terminating 状态的 pods，那么就不考虑驱逐更多的 pods. 这个函数逻辑很简单，我们直接看源码： pkg/scheduler/core/generic_scheduler.go:1110 func podEligibleToPreemptOthers(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo) bool { nomNodeName := pod.Status.NominatedNodeName // 如果 pod.Status.NominatedNodeName 不是空字符串 if len(nomNodeName) > 0 { // 被 nominate 的 node if nodeInfo, found := nodeNameToInfo[nomNodeName]; found { for _, p := range nodeInfo.Pods() { // 有低优先级的 pod 处于删除中状态，就返回 false if p.DeletionTimestamp != nil && util.GetPodPriority(p) 3.3.4. nodesWherePreemptionMightHelp nodesWherePreemptionMightHelp 要做的事情是寻找 predicates 阶段失败但是通过抢占也许能够调度成功的 nodes. 这个函数也不怎么长，看下代码： pkg/scheduler/core/generic_scheduler.go:1060 func nodesWherePreemptionMightHelp(nodes []*v1.Node, failedPredicatesMap FailedPredicateMap) []*v1.Node { // 潜力 node， 用于存储返回值的 slice potentialNodes := []*v1.Node{} for _, node := range nodes { // 这个为 true 表示一个 node 驱逐 pod 也不一定能适合当前 pod 运行 unresolvableReasonExist := false // 一个 node 对应的所有失败的 predicates failedPredicates, _ := failedPredicatesMap[node.Name] // 遍历，看是不是再下面指定的这些原因中，如果在，就标记 unresolvableReasonExist = true for _, failedPredicate := range failedPredicates { switch failedPredicate { case predicates.ErrNodeSelectorNotMatch, predicates.ErrPodAffinityRulesNotMatch, predicates.ErrPodNotMatchHostName, predicates.ErrTaintsTolerationsNotMatch, predicates.ErrNodeLabelPresenceViolated, predicates.ErrNodeNotReady, predicates.ErrNodeNetworkUnavailable, predicates.ErrNodeUnderDiskPressure, predicates.ErrNodeUnderPIDPressure, predicates.ErrNodeUnderMemoryPressure, predicates.ErrNodeOutOfDisk, predicates.ErrNodeUnschedulable, predicates.ErrNodeUnknownCondition, predicates.ErrVolumeZoneConflict, predicates.ErrVolumeNodeConflict, predicates.ErrVolumeBindConflict: unresolvableReasonExist = true // 如果找到一个上述失败原因，说明这个 node 已经可以排除了，break 后继续下一个 node 的计算 break } } // false 的时候，也就是这个 node 也许驱逐 pods 后有用，那就添加到 potentialNodes 中 if !unresolvableReasonExist { klog.V(3).Infof(\"Node %v is a potential node for preemption.\", node.Name) potentialNodes = append(potentialNodes, node) } } return potentialNodes } 3.3.5. selectNodesForPreemption 这个函数会并发计算所有的 nodes 是否通过驱逐实现 pod 抢占。 看这个函数内容之前我们先看一下返回值的类型： map[*v1.Node]*schedulerapi.Victims 的 key 很好理解，value 是啥呢： type Victims struct { Pods []*v1.Pod NumPDBViolations int } 这里的 Pods 是被选中准备要驱逐的；NumPDBViolations 表示的是要破坏多少个 PDB 限制。这里肯定也就是要尽量符合 PDB 要求，能不和 PDB 冲突就不冲突。 然后看一下这个函数的整体过程： pkg/scheduler/core/generic_scheduler.go:895 func selectNodesForPreemption(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, potentialNodes []*v1.Node, // 上一个函数计算出来的 nodes predicates map[string]algorithm.FitPredicate, metadataProducer algorithm.PredicateMetadataProducer, queue internalqueue.SchedulingQueue, // 这里其实是前面讲的优先级队列 PriorityQueue pdbs []*policy.PodDisruptionBudget, // pdb 列表 ) (map[*v1.Node]*schedulerapi.Victims, error) { nodeToVictims := map[*v1.Node]*schedulerapi.Victims{} var resultLock sync.Mutex // We can use the same metadata producer for all nodes. meta := metadataProducer(pod, nodeNameToInfo) // 这种形式的并发已经不陌生了，前面遇到过几次了 checkNode := func(i int) { nodeName := potentialNodes[i].Name var metaCopy algorithm.PredicateMetadata if meta != nil { metaCopy = meta.ShallowCopy() } // 这里有一个子过程调用，下面单独介绍 pods, numPDBViolations, fits := selectVictimsOnNode(pod, metaCopy, nodeNameToInfo[nodeName], predicates, queue, pdbs) if fits { resultLock.Lock() victims := schedulerapi.Victims{ Pods: pods, NumPDBViolations: numPDBViolations, } // 如果 fit，就添加到 nodeToVictims 中，也就是最后的返回值 nodeToVictims[potentialNodes[i]] = &victims resultLock.Unlock() } } workqueue.ParallelizeUntil(context.TODO(), 16, len(potentialNodes), checkNode) return nodeToVictims, nil } 上面这个函数的核心逻辑在 selectVictimsOnNode 中，这个函数尝试在给定的 node 中寻找最少数量的需要被驱逐的 pods，同时需要保证驱逐了这些 pods 之后，这个 noode 能够满足“pod”运行需求。 这些被驱逐的 pods 计算同时需要满足一个约束，就是能够删除低优先级的 pod 绝不先删高优先级的 pod. 这个算法首选计算当这个 node 上所有的低优先级 pods 被驱逐之后能否调度“pod”. 如果可以，那就按照优先级排序，根据 PDB 是否破坏分成两组，一组是影响 PDB 限制的，另外一组是不影响 PDB. 两组各自按照优先级排序。然后开始逐渐释放影响 PDB 的 group 中的 pod，然后逐渐释放不影响 PDB 的 group 中的 pod，在这个过程中要保持“pod”能够 fit 这个 node. 也就是说一旦放过某一个 pod 导致“pod”不 fit 这个 node 了，那就说明这个 pod 不能放过，也就是意味着已经找到了最少 pods 集。 看一下具体的实现吧： FILENAME pkg/scheduler/core/generic_scheduler.go:983 func selectVictimsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo, fitPredicates map[string]algorithm.FitPredicate, queue internalqueue.SchedulingQueue, pdbs []*policy.PodDisruptionBudget, ) ([]*v1.Pod, int, bool) { if nodeInfo == nil { return nil, 0, false } // 排个序 potentialVictims := util.SortableList{CompFunc: util.HigherPriorityPod} nodeInfoCopy := nodeInfo.Clone() // 定义删除 pod 函数 removePod := func(rp *v1.Pod) { nodeInfoCopy.RemovePod(rp) if meta != nil { meta.RemovePod(rp) } } // 定义添加 pod 函数 addPod := func(ap *v1.Pod) { nodeInfoCopy.AddPod(ap) if meta != nil { meta.AddPod(ap, nodeInfoCopy) } } // 删除所有的低优先级 pod 看是不是能够满足调度需求了 podPriority := util.GetPodPriority(pod) for _, p := range nodeInfoCopy.Pods() { if util.GetPodPriority(p) 3.3.6. pickOneNodeForPreemption pickOneNodeForPreemption 要从给定的 nodes 中选择一个 node，这个函数假设给定的 map 中 value 部分是以 priority 降序排列的。这里选择 node 的标准是： 最少的 PDB violations 最少的高优先级 victim 优先级总数字最小 victim 总数最小 直接返回第一个 pkg/scheduler/core/generic_scheduler.go:788 func pickOneNodeForPreemption(nodesToVictims map[*v1.Node]*schedulerapi.Victims) *v1.Node { if len(nodesToVictims) == 0 { return nil } // 初始化为最大值 minNumPDBViolatingPods := math.MaxInt32 var minNodes1 []*v1.Node lenNodes1 := 0 // 这个循环要找到 PDBViolatingPods 最少的 node，如果有多个，就全部存在 minNodes1 中 for node, victims := range nodesToVictims { if len(victims.Pods) == 0 { // 如果发现一个不需要驱逐 pod 的 node，马上返回 return node } numPDBViolatingPods := victims.NumPDBViolations if numPDBViolatingPods -6，有2个 pod 的 node 反而被认为总优先级更低！ sumPriorities += int64(util.GetPodPriority(pod)) + int64(math.MaxInt32+1) } if sumPriorities 0 { return minNodes2[0] } klog.Errorf(\"Error in logic of node scoring for preemption. We should never reach here!\") return nil } 4. 小结 咋个说呢，此处应该有总结的，抢占过程的逻辑比我想象中的复杂，设计很巧妙，行云流水，大快人心！preemption 可以简单说成再预选->再优选吧；还是不多说了，一天写这么多有点坐不住了，下回再继续聊调度器～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 抢占调度1. Pod priority2. preempt 入口3. preempt 实现3.1. SchedulingQueue3.1.1. FIFO3.1.2. PriorityQueue3.2. PodPreemptor3.3. xx.Algorithm.Preempt3.3.1. 接口定义3.3.2. 整体流程3.3.3. podEligibleToPreemptOthers3.3.4. nodesWherePreemptionMightHelp3.3.5. selectNodesForPreemption3.3.6. pickOneNodeForPreemption4. 小结"},"core/scheduler/init.html":{"url":"core/scheduler/init.html","title":"调度器初始化","keywords":"","body":"调度器初始化 概述 从 --config 开始 options.Option 对象 config.Config对象 runCommand ApplyFeatureGates 默认算法注册 特性开关 Scheduler 的创建 调度算法源 policy / provider 如何生效 默认生效的算法 1. 概述 今天我们要做一些琐碎的知识点分析，比如调度器启动的时候默认配置是怎么来的？默认生效了哪些调度算法？自定义的算法是如何注入的？诸如这些问题，我们顺带会看一下调度器相关的一些数据结构的含义。看完前面这些节的分析后再看完本篇文章你可能会有一种醍醐灌顶的感觉哦～ 2. 从 --config 开始 如果我们编译出来一个 kube-scheduler 二进制文件，运行./kube-scheduler -h后会看到很多的帮助信息，这些信息是分组的，比如第一组 Misc，差不多是“大杂烩”的意思，不好分类的几个 flag，其实也是最重要的几个 flag，如下： 很好理解，第一个红框框圈出来的--config用于指定配置文件，老版本的各种参数基本都不建议使用了，所以这个 config flag 指定的 config 文件中基本包含了所有可配置项，我们看一下代码中获取这个 flag 的相关代码： cmd/kube-scheduler/app/options/options.go:143 func (o *Options) Flags() (nfs apiserverflag.NamedFlagSets) { fs := nfs.FlagSet(\"misc\") // 关注 --config fs.StringVar(&o.ConfigFile, \"config\", o.ConfigFile, \"The path to the configuration file. Flags override values in this file.\") fs.StringVar(&o.WriteConfigTo, \"write-config-to\", o.WriteConfigTo, \"If set, write the configuration values to this file and exit.\") fs.StringVar(&o.Master, \"master\", o.Master, \"The address of the Kubernetes API server (overrides any value in kubeconfig)\") o.SecureServing.AddFlags(nfs.FlagSet(\"secure serving\")) o.CombinedInsecureServing.AddFlags(nfs.FlagSet(\"insecure serving\")) o.Authentication.AddFlags(nfs.FlagSet(\"authentication\")) o.Authorization.AddFlags(nfs.FlagSet(\"authorization\")) o.Deprecated.AddFlags(nfs.FlagSet(\"deprecated\"), &o.ComponentConfig) leaderelectionconfig.BindFlags(&o.ComponentConfig.LeaderElection.LeaderElectionConfiguration, nfs.FlagSet(\"leader election\")) utilfeature.DefaultFeatureGate.AddFlag(nfs.FlagSet(\"feature gate\")) return nfs } 上述代码中有几个点可以关注到： FlagSet 的含义，命令行输出的分组和这里的分组是对应的； 除了认证授权、选举等“非关键”配置外，其他配置基本 Deprecated 了，也就意味着建议使用 config file； 上面代码中可以看到o.ConfigFile接收了config配置，我们看看Option类型是什么样子的~ 2.1. options.Option 对象 Options对象包含运行一个 Scheduler 所需要的所有参数 cmd/kube-scheduler/app/options/options.go:55 type Options struct { // 和命令行帮助信息的分组是一致的 ComponentConfig kubeschedulerconfig.KubeSchedulerConfiguration SecureServing *apiserveroptions.SecureServingOptionsWithLoopback CombinedInsecureServing *CombinedInsecureServingOptions Authentication *apiserveroptions.DelegatingAuthenticationOptions Authorization *apiserveroptions.DelegatingAuthorizationOptions Deprecated *DeprecatedOptions // config 文件的路径 ConfigFile string // 如果指定了，会输出 config 的默认配置到这个文件 WriteConfigTo string Master string } 前面的 flag 相关代码中写到配置文件的内容给了o.ConfigFile，也就是Options.ConfigFile，那这个属性怎么使用呢？ 我们来看下面这个 ApplyTo() 函数，这个函数要做的事情是把 options 配置 apply 给 scheduler app configuration(这个对象后面会讲到)： cmd/kube-scheduler/app/options/options.go:162 // 把 Options apply 给 Config func (o *Options) ApplyTo(c *schedulerappconfig.Config) error { // --config 没有使用的情况 if len(o.ConfigFile) == 0 { c.ComponentConfig = o.ComponentConfig // 使用 Deprecated 的配置 if err := o.Deprecated.ApplyTo(&c.ComponentConfig); err != nil { return err } if err := o.CombinedInsecureServing.ApplyTo(c, &c.ComponentConfig); err != nil { return err } } else { // 加载 config 文件中的内容 cfg, err := loadConfigFromFile(o.ConfigFile) if err != nil { return err } // 上面加载到的配置赋值给 Config中的 ComponentConfig c.ComponentConfig = *cfg if err := o.CombinedInsecureServing.ApplyToFromLoadedConfig(c, &c.ComponentConfig); err != nil { return err } } // …… return nil } 这个函数中可以看到用 --config 和不用 --config 两种情况下 options 是如何应用到schedulerappconfig.Config中的。那么这里提到的 Config 对象又是什么呢？ 2.2. config.Config对象 Config 对象包含运行一个 Scheduler 所需要的所有 context cmd/kube-scheduler/app/config/config.go:32 type Config struct { // 调度器配置对象 ComponentConfig kubeschedulerconfig.KubeSchedulerConfiguration LoopbackClientConfig *restclient.Config InsecureServing *apiserver.DeprecatedInsecureServingInfo InsecureMetricsServing *apiserver.DeprecatedInsecureServingInfo Authentication apiserver.AuthenticationInfo Authorization apiserver.AuthorizationInfo SecureServing *apiserver.SecureServingInfo Client clientset.Interface InformerFactory informers.SharedInformerFactory PodInformer coreinformers.PodInformer EventClient v1core.EventsGetter Recorder record.EventRecorder Broadcaster record.EventBroadcaster LeaderElection *leaderelection.LeaderElectionConfig } 所以前面的c.ComponentConfig = o.ComponentConfig这行代码也就是把 Options 中的 ComponentConfig 赋值给了 Config 中的 ComponentConfig；是哪里的逻辑让 Options 和 Config 对象产生了关联呢？(也就是说前面提到的 ApplyTo() 方法是再哪里被调用的？) 继续跟下去可以找到Config()函数，从这个函数的返回值*schedulerappconfig.Config可以看到它的目的，是需要得到一个 schedulerappconfig.Config，代码不长： cmd/kube-scheduler/app/options/options.go:221 func (o *Options) Config() (*schedulerappconfig.Config, error) { // …… c := &schedulerappconfig.Config{} // 前面我们看到的 ApplyTo() 函数 if err := o.ApplyTo(c); err != nil { return nil, err } // Prepare kube clients. // …… // Prepare event clients. eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, corev1.EventSource{Component: c.ComponentConfig.SchedulerName}) // Set up leader election if enabled. // …… c.Client = client c.InformerFactory = informers.NewSharedInformerFactory(client, 0) c.PodInformer = factory.NewPodInformer(client, 0) c.EventClient = eventClient c.Recorder = recorder c.Broadcaster = eventBroadcaster c.LeaderElection = leaderElectionConfig return c, nil } 那调用这个Config()函数的地方又在哪里呢？继续跟就到 runCommand 里面了～ 2.3. runCommand runCommand 这个函数我们不陌生： cmd/kube-scheduler/app/server.go:117 func runCommand(cmd *cobra.Command, args []string, opts *options.Options) error { // …… // 这个地方完成了前面说到的配置文件和命令行参数等读取和应用工作 c, err := opts.Config() if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } stopCh := make(chan struct{}) // Get the completed config cc := c.Complete() // To help debugging, immediately log version klog.Infof(\"Version: %+v\", version.Get()) // 这里有一堆逻辑 algorithmprovider.ApplyFeatureGates() // Configz registration. // …… return Run(cc, stopCh) } runCommand 在最开始的时候我们有见到过，已经到 cobra 入口的 Run 中了： cmd/kube-scheduler/app/server.go:85 Run: func(cmd *cobra.Command, args []string) { if err := runCommand(cmd, args, opts); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, 上面涉及到2个知识点： ApplyFeatureGates Run 中的逻辑 我们下面分别来看看～ 3. ApplyFeatureGates 这个函数跟进去可以看到如下几行简单的代码，这里很自然我们能够想到继续跟defaults.ApplyFeatureGates()，但是不能只看到这个函数哦，具体来看： pkg/scheduler/algorithmprovider/plugins.go:17 package algorithmprovider import ( \"k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults\" ) // ApplyFeatureGates applies algorithm by feature gates. func ApplyFeatureGates() { defaults.ApplyFeatureGates() } 到这里分2条路： import defaults 这个 package 的时候有一个init()函数调用的逻辑 defaults.ApplyFeatureGates() 函数调用本身。 3.1. 默认算法注册 pkg/scheduler/algorithmprovider/defaults/defaults.go:38 func init() { // …… registerAlgorithmProvider(defaultPredicates(), defaultPriorities()) // …… } init()函数中我们先关注 registerAlgorithmProvider() 函数，这里从字面上可以得到不少信息，大胆猜一下：是不是注册了默认的预选算法和优选算法？ pkg/scheduler/algorithmprovider/defaults/defaults.go:222 func registerAlgorithmProvider(predSet, priSet sets.String) { // 注册 algorithm provider. 默认使用 DefaultProvider factory.RegisterAlgorithmProvider(factory.DefaultProvider, predSet, priSet) factory.RegisterAlgorithmProvider(ClusterAutoscalerProvider, predSet, copyAndReplace(priSet, \"LeastRequestedPriority\", \"MostRequestedPriority\")) } 看到这里可以关注到 AlgorithmProvider 这个概念，后面会讲到。 先看一下里面调用的注册函数是怎么实现的： pkg/scheduler/factory/plugins.go:387 func RegisterAlgorithmProvider(name string, predicateKeys, priorityKeys sets.String) string { schedulerFactoryMutex.Lock() defer schedulerFactoryMutex.Unlock() validateAlgorithmNameOrDie(name) // 很明显，关键逻辑在这里 algorithmProviderMap[name] = AlgorithmProviderConfig{ FitPredicateKeys: predicateKeys, PriorityFunctionKeys: priorityKeys, } return name } 首先，algorithmProviderMap 这个变量是一个包级变量，在86行做的定义：algorithmProviderMap = make(map[string]AlgorithmProviderConfig) 这里的 key 有2种情况： \"DefaultProvider\" \"ClusterAutoscalerProvider\" 混合云场景用得到 ClusterAutoscalerProvider，大家感兴趣可以研究一下 ClusterAutoscaler 特性，这块我们先不说。默认的情况是生效的 DefaultProvider，这块逻辑后面还会提到。 然后这个 map 的 value 的类型是一个简单的 struct： pkg/scheduler/factory/plugins.go:99 type AlgorithmProviderConfig struct { FitPredicateKeys sets.String PriorityFunctionKeys sets.String } 接着看一下defaultPredicates()函数 pkg/scheduler/algorithmprovider/defaults/defaults.go:106 func defaultPredicates() sets.String { return sets.NewString( // Fit is determined by volume zone requirements. factory.RegisterFitPredicateFactory( predicates.NoVolumeZoneConflictPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewVolumeZonePredicate(args.PVInfo, args.PVCInfo, args.StorageClassInfo) }, ), // …… factory.RegisterFitPredicate(predicates.NoDiskConflictPred, predicates.NoDiskConflict), // …… ) } 这个函数里面就2中类型的玩法，简化一些可以理解成上面这个样子，我们一个个来看。 先认识一下 sets.NewString()函数要干嘛： vendor/k8s.io/apimachinery/pkg/util/sets/string.go:27 type String map[string]Empty // NewString creates a String from a list of values. func NewString(items ...string) String { ss := String{} ss.Insert(items...) return ss } // …… // Insert adds items to the set. func (s String) Insert(items ...string) { for _, item := range items { s[item] = Empty{} } } 如上，很简单的类型封装。里面的Empty是：type Empty struct{}，所以本质上就是要用map[string]struct{}这个类型罢了。 因此上面defaultPredicates()函数中sets.NewString()内每一个参数本质上就是一个 string 类型了，我们来看这一个个 string 是怎么来的。 pkg/scheduler/factory/plugins.go:195 func RegisterFitPredicateFactory(name string, predicateFactory FitPredicateFactory) string { schedulerFactoryMutex.Lock() defer schedulerFactoryMutex.Unlock() validateAlgorithmNameOrDie(name) // 唯一值的关注的逻辑 fitPredicateMap[name] = predicateFactory // 返回 name return name } 这个函数要返回一个 string 我们已经知道了，里面的逻辑也只有这一行需要我们关注：fitPredicateMap[name] = predicateFactory，这个 map 类型也是一个包级变量：fitPredicateMap = make(map[string]FitPredicateFactory)，所以前面讲的注册本质也就是在填充这个变量而已。理解fitPredicateMap[name] = predicateFactory中 fitPredicateMap 的 key 和 value，也就知道了这里的 Register 要做什么。 defaultPredicates()中的第二种注册方式 RegisterFitPredicate 区别不大，函数体也是调用的 RegisterFitPredicateFactory()： pkg/scheduler/factory/plugins.go:106 func RegisterFitPredicate(name string, predicate algorithm.FitPredicate) string { return RegisterFitPredicateFactory(name, func(PluginFactoryArgs) algorithm.FitPredicate { return predicate }) } 3.2. 特性开关 pkg/scheduler/algorithmprovider/defaults/defaults.go:183 func ApplyFeatureGates() { if utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition) { factory.RemoveFitPredicate(predicates.CheckNodeConditionPred) factory.RemoveFitPredicate(predicates.CheckNodeMemoryPressurePred) factory.RemoveFitPredicate(predicates.CheckNodeDiskPressurePred) factory.RemoveFitPredicate(predicates.CheckNodePIDPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeConditionPred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeMemoryPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeDiskPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodePIDPressurePred) factory.RegisterMandatoryFitPredicate(predicates.PodToleratesNodeTaintsPred, predicates.PodToleratesNodeTaints) factory.RegisterMandatoryFitPredicate(predicates.CheckNodeUnschedulablePred, predicates.CheckNodeUnschedulablePredicate) factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.PodToleratesNodeTaintsPred) factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.CheckNodeUnschedulablePred) } if utilfeature.DefaultFeatureGate.Enabled(features.ResourceLimitsPriorityFunction) { factory.RegisterPriorityFunction2(\"ResourceLimitsPriority\", priorities.ResourceLimitsPriorityMap, nil, 1) factory.InsertPriorityKeyToAlgorithmProviderMap(factory.RegisterPriorityFunction2(\"ResourceLimitsPriority\", priorities.ResourceLimitsPriorityMap, nil, 1)) } } 这个函数看着几十行，实际上只在重复一件事情，增加或删除一些预选和优选算法。我们看一下这里的一些逻辑： utilfeature.DefaultFeatureGate.Enabled() 函数要做的事情是判断一个 feature 是否开启；函数参数本质只是一个字符串： pkg/features/kube_features.go:25 const ( AppArmor utilfeature.Feature = \"AppArmor\" DynamicKubeletConfig utilfeature.Feature = \"DynamicKubeletConfig\" // …… ) 这里定义了很多的 feature，然后定义了哪些 feature 是开启的，处在 alpha 还是 beta 或者 GA 等： pkg/features/kube_features.go:405 var defaultKubernetesFeatureGates = map[utilfeature.Feature]utilfeature.FeatureSpec{ AppArmor: {Default: true, PreRelease: utilfeature.Beta}, DynamicKubeletConfig: {Default: true, PreRelease: utilfeature.Beta}, ExperimentalHostUserNamespaceDefaultingGate: {Default: false, PreRelease: utilfeature.Beta}, ExperimentalCriticalPodAnnotation: {Default: false, PreRelease: utilfeature.Alpha}, DevicePlugins: {Default: true, PreRelease: utilfeature.Beta}, TaintBasedEvictions: {Default: true, PreRelease: utilfeature.Beta}, RotateKubeletServerCertificate: {Default: true, PreRelease: utilfeature.Beta}, // …… } 所以回到前面ApplyFeatureGates()的逻辑，utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition)要判断的是 TaintNodesByCondition 这个特性是否开启了，如果开启了就把 predicates 中 \"CheckNodeCondition\", \"CheckNodeMemoryPressure\", \"CheckNodePIDPressurePred\", \"CheckNodeDiskPressure\" 这几个算法去掉，把 \"PodToleratesNodeTaints\", \"CheckNodeUnschedulable\" 加上。接着对于特性 \"ResourceLimitsPriorityFunction\" 的处理也是同一个逻辑。 4. Scheduler 的创建 我们换一条线，从 Scheduler 对象的创建再来看另外几个知识点。 前面分析到runCommand()函数的时候我们说到了需要关注最后一行return Run(cc, stopCh)的逻辑，在Run()函数中主要的逻辑就是创建 Scheduler 和启动 Scheduler；现在我们来看创建逻辑： cmd/kube-scheduler/app/server.go:174 sched, err := scheduler.New(cc.Client, cc.InformerFactory.Core().V1().Nodes(), cc.PodInformer, cc.InformerFactory.Core().V1().PersistentVolumes(), cc.InformerFactory.Core().V1().PersistentVolumeClaims(), cc.InformerFactory.Core().V1().ReplicationControllers(), cc.InformerFactory.Apps().V1().ReplicaSets(), cc.InformerFactory.Apps().V1().StatefulSets(), cc.InformerFactory.Core().V1().Services(), cc.InformerFactory.Policy().V1beta1().PodDisruptionBudgets(), storageClassInformer, cc.Recorder, cc.ComponentConfig.AlgorithmSource, stopCh, scheduler.WithName(cc.ComponentConfig.SchedulerName), scheduler.WithHardPodAffinitySymmetricWeight(cc.ComponentConfig.HardPodAffinitySymmetricWeight), scheduler.WithEquivalenceClassCacheEnabled(cc.ComponentConfig.EnableContentionProfiling), scheduler.WithPreemptionDisabled(cc.ComponentConfig.DisablePreemption), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithBindTimeoutSeconds(*cc.ComponentConfig.BindTimeoutSeconds)) 这里调用了一个New()函数，传了很多参数进去。New()函数的定义如下： pkg/scheduler/scheduler.go:131 func New(client clientset.Interface, nodeInformer coreinformers.NodeInformer, podInformer coreinformers.PodInformer, pvInformer coreinformers.PersistentVolumeInformer, pvcInformer coreinformers.PersistentVolumeClaimInformer, replicationControllerInformer coreinformers.ReplicationControllerInformer, replicaSetInformer appsinformers.ReplicaSetInformer, statefulSetInformer appsinformers.StatefulSetInformer, serviceInformer coreinformers.ServiceInformer, pdbInformer policyinformers.PodDisruptionBudgetInformer, storageClassInformer storageinformers.StorageClassInformer, recorder record.EventRecorder, schedulerAlgorithmSource kubeschedulerconfig.SchedulerAlgorithmSource, stopCh 这里涉及到的东西有点小多，我们一点点看： options := defaultSchedulerOptions 这行代码的 defaultSchedulerOptions 是一个 schedulerOptions 对象： pkg/scheduler/scheduler.go:121 // LINE 67 type schedulerOptions struct { schedulerName string hardPodAffinitySymmetricWeight int32 enableEquivalenceClassCache bool disablePreemption bool percentageOfNodesToScore int32 bindTimeoutSeconds int64 } // …… // LINE 121 var defaultSchedulerOptions = schedulerOptions{ // \"default-scheduler\" schedulerName: v1.DefaultSchedulerName, // 1 hardPodAffinitySymmetricWeight: v1.DefaultHardPodAffinitySymmetricWeight, enableEquivalenceClassCache: false, disablePreemption: false, // 50 percentageOfNodesToScore: schedulerapi.DefaultPercentageOfNodesToScore, // 100 bindTimeoutSeconds: BindTimeoutSeconds, } 回到New()函数的逻辑： pkg/scheduler/scheduler.go:148 for _, opt := range opts { opt(&options) } 这里的 opts 定义在参数里：opts ...func(o *schedulerOptions)，我们看一个实参来理解一下：scheduler.WithName(cc.ComponentConfig.SchedulerName)： pkg/scheduler/scheduler.go:80 // 这个函数能够把给定的 schedulerName 赋值给 schedulerOptions.schedulerName func WithName(schedulerName string) Option { return func(o *schedulerOptions) { o.schedulerName = schedulerName } } 这种方式设置一个对象的属性还是挺有意思的。 4.1. 调度算法源 我们继续往后面看New()函数的其他逻辑： source := schedulerAlgorithmSource 这行代码里的 schedulerAlgorithmSource 代表了什么？ 形参中有这个变量的定义：schedulerAlgorithmSource kubeschedulerconfig.SchedulerAlgorithmSource，跟进去可以看到： pkg/scheduler/apis/config/types.go:97 // 表示调度算法源，两个属性是互相排斥的，也就是二选一的意思 type SchedulerAlgorithmSource struct { // Policy is a policy based algorithm source. Policy *SchedulerPolicySource // Provider is the name of a scheduling algorithm provider to use. Provider *string } 这两个属性肯定得理解一下了，目测挺重要的样子： Policy pkg/scheduler/apis/config/types.go:106 type SchedulerPolicySource struct { // 文件方式配置生效的调度算法 File *SchedulerPolicyFileSource // ConfigMap 方式配置生效的调度算法 ConfigMap *SchedulerPolicyConfigMapSource } // 下面分别是2个属性的结构定义： // …… type SchedulerPolicyFileSource struct { // Path is the location of a serialized policy. Path string } // …… type SchedulerPolicyConfigMapSource struct { // Namespace is the namespace of the policy config map. Namespace string // Name is the name of hte policy config map. Name string } 大家还记得我们在讲调度器设计的时候提到的 Policy 文件不？大概长这个样子： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\"}, {\"name\" : \"HostName\"} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10, \"alwaysCheckAllPredicates\" : false } 所以啊，这个 Policy原来是通过代码里的 SchedulerPolicySource 去配置的～ 4.2. policy / provider 如何生效 前面讲到调度算法从何而来(源头)，现在我们看一下这些算法配置如何生效的： pkg/scheduler/scheduler.go:173 source := schedulerAlgorithmSource switch { // 如果 Provider 配置了，就不用 policy 了 case source.Provider != nil: // 根据给定的 Provider 创建 scheduler config sc, err := configurator.CreateFromProvider(*source.Provider) if err != nil { return nil, fmt.Errorf(\"couldn't create scheduler using provider %q: %v\", *source.Provider, err) } config = sc // 如果 Policy 提供了，就没有上面的 provider 的事情了 case source.Policy != nil: // 根据给定的 Policy 创建 scheduler config policy := &schedulerapi.Policy{} switch { // 是 File 的情况 case source.Policy.File != nil: if err := initPolicyFromFile(source.Policy.File.Path, policy); err != nil { return nil, err } // 是 ConfigMap 的情况 case source.Policy.ConfigMap != nil: if err := initPolicyFromConfigMap(client, source.Policy.ConfigMap, policy); err != nil { return nil, err } } sc, err := configurator.CreateFromConfig(*policy) if err != nil { return nil, fmt.Errorf(\"couldn't create scheduler from policy: %v\", err) } config = sc default: return nil, fmt.Errorf(\"unsupported algorithm source: %v\", source) } 上面代码涉及到的2个类型我们再来关注一下： schedulerapi.Policy factory.Config 这个 Policy 就是具体用于存放我们配置的 policy 的载体，对照着这个结构我们可以判断自己在配置 policy 的时候应该按照什么格式： pkg/scheduler/api/types.go:47 type Policy struct { metav1.TypeMeta Predicates []PredicatePolicy Priorities []PriorityPolicy ExtenderConfigs []ExtenderConfig HardPodAffinitySymmetricWeight int32 AlwaysCheckAllPredicates bool } 这个结构内部封装的一层层结构我就不继续贴了，大家感兴趣可以点开看一下，跟到底的落点都是基础类型的，string啊，int啊，bool啊这些～ 关于 factory.Config 可能大家有印象，这个结构就是 Scheduler 对象的唯一属性： pkg/scheduler/scheduler.go:58 type Scheduler struct { config *factory.Config } Config 结构体的属性不外乎 Scheduler 在落实调度、抢占等动作时所需要的一系列方法(或对象)；在New()函数的最后有一行sched := NewFromConfig(config)，实现是简单地实例化 Scheduler，然后将 config 赋值给 Scheduler 的 config 属性，然后返回 Scheduler 对象的地址。 5. 默认生效的算法 我们最后还是单独拎出来强调一下生效了哪些算法的具体逻辑吧，前面有提到一些了，我相信肯定有人很关注这个知识点。 前面提到 Scheduler 创建的时候使用的 New()函数，函数中 switch 判断 schedulerAlgorithmSource 是 Provider 还是 Policy，然后做了具体的初始化逻辑，我们具体看其中一个初始化， 串一下这些点： sc, err := configurator.CreateFromProvider(*source.Provider) 如果我们配置的是 Provider，这时候代码逻辑调用的是这样一行，这个函数的实现如下： pkg/scheduler/factory/factory.go:1156 func (c *configFactory) CreateFromProvider(providerName string) (*Config, error) { // 比如说我们配置的 name 是 DefaultProvider，这个函数要获取一个 AlgorithmProviderConfig 类型的对象 provider, err := GetAlgorithmProvider(providerName) if err != nil { return nil, err } // 下面详细看 return c.CreateFromKeys(provider.FitPredicateKeys, provider.PriorityFunctionKeys, []algorithm.SchedulerExtender{}) } 这个函数里有2个点需要关注，第一个是GetAlgorithmProvider()函数返回了什么： pkg/scheduler/factory/plugins.go:99 type AlgorithmProviderConfig struct { FitPredicateKeys sets.String PriorityFunctionKeys sets.String } 看到这个返回值类型，心里就明朗了。 我们继续看比较重要的CreateFromKeys()方法调用的具体逻辑，这个函数的实参中 provider.FitPredicateKeys, provider.PriorityFunctionKeys 很明显和具体的 provider 相关，不同 provider 定义的预置算法不同。继续来看函数实现： pkg/scheduler/factory/factory.go:1255 func (c *configFactory) CreateFromKeys(predicateKeys, priorityKeys sets.String, extenders []algorithm.SchedulerExtender) (*Config, error) { // …… // 根据 predicateKeys 得到 predicateFuncs predicateFuncs, err := c.GetPredicates(predicateKeys) // 根据 priorityKeys 得到 priorityConfigs priorityConfigs, err := c.GetPriorityFunctionConfigs(priorityKeys) // …… // 创建一个 genericScheduler，这个对象我们很熟悉。algo 也就是 Algorithm 的简写； algo := core.NewGenericScheduler( c.schedulerCache, c.equivalencePodCache, c.podQueue, predicateFuncs, // 和 predicateKeys 对应 predicateMetaProducer, priorityConfigs, // 和 priorityKeys 对应 priorityMetaProducer, // …… ) podBackoff := util.CreateDefaultPodBackoff() return &Config{ // …… Algorithm: algo, // 很清晰了 // …… }, nil } 上面的NewGenericScheduler()函数接收了这些参数之后丢给了 genericScheduler 对象，这个对象中 predicates 属性对应参数 predicateFuncs，prioritizers 属性对应参数 priorityConfigs； 从这里的代码可以看出来我们配置的算法源可以影响到 Scheduler 的初始化，最终体现在改变了 Scheduler 对象的 config 属性的 Algorithm 属性的 prioritizers 和 prioritizers 上。我们最后回顾一下这2个属性的类型，就和以前的预选、优选过程分析的时候关注的点对上了： predicates --> map[string]algorithm.FitPredicate prioritizers --> []algorithm.PriorityConfig 是不是很熟悉呢？ 行，今天就讲到这里～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 调度器初始化1. 概述2. 从 --config 开始2.1. options.Option 对象2.2. config.Config对象2.3. runCommand3. ApplyFeatureGates3.1. 默认算法注册3.2. 特性开关4. Scheduler 的创建4.1. 调度算法源4.2. policy / provider 如何生效5. 默认生效的算法"},"core/scheduler/affinity.html":{"url":"core/scheduler/affinity.html","title":"专题-亲和性调度","keywords":"","body":"专题-亲和性调度(Author - XiaoYang) 简介 约束调度 Labels.selector标签选择器 Node亲和性 Node亲和性预选策略MatchNodeSelectorPred Node亲和性优选策略NodeAffinityPriority Pod亲和性 Pod亲和性预选策略MatchInterPodAffinityPred Pod亲和性优选策略InterPodAffinityPriority Service亲和性 Serice亲和性预选策略checkServiceAffinity 1. 简介 在未分析和深入理解scheduler源码逻辑之前，本人在操作配置亲和性上，由于官方和第三方文档者说明不清楚等原因，在亲和性理解上有遇到过一些困惑，如： 亲和性的operator的 “In”底层是什么匹配操作？正则匹配吗？“Gt/Lt”底层又是什么操作实现的？ 所有能查到的文档描述pod亲和性的topoloykey有三个： kubernetes.io/hostname failure-domain.beta.kubernetes.io/zone failure-domain.beta.kubernetes.io/region 为什么？真的只支持这三个key？不能自定义？ Pod与Node亲和性两种类型的差异是什么？而Pod亲和性正真要去匹配的是什么，其内在逻辑是？ 不知道你们是否有同样类似的问题或困惑呢？当你清晰的理解了代码逻辑实现后，那么你会觉得一切是那么的 清楚明确了，不再有“隐性知识”问题存在。所以我希望本文所述内容能给大家在kubernetes亲和性的解惑上有所帮助。 1.1. 约束调度 在展开源码分析之前为更好的理解亲和性代码逻辑，补充一些kubernetes调度相关的基础知识： 亲和性目的是为了实现用户可以按需将pod调度到指定Node上，我称之为“约束调度”。 约束调度操作上常用以下三类： NodeSelector / NodeName node标签选择器 和 \"nodeName\"匹配 Affinity (Node/Pod/Service） 亲和性 Taint / Toleration 污点和容忍 本文所述主题是亲和性，亲和性分为三种类型Node、Pod、Service亲和，以下是亲和性预选和优选阶段代码实现的策略对应表（后面有详细分析）： 预选阶段策略 Pod.Spec配置 类别 次序 MatchNodeSelecotorPred NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution Node 6 MatchInterPodAffinityPred PodAffinity.RequiredDuringSchedulingIgnoredDuringExecution**PodAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution Pod 22 CheckServiceAffinityPred Service 12 优选阶段策略 Pod.Spec配置 默认权重 InterPodAffinityPriority PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution 1 NodeAffinityPriority NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution 1 1.2. Labels.selector标签选择器 labels selector是亲和性代码底层使用最基础的代码工具，不论是nodeAffinity还是podAffinity都是需要用到它。在使用yml类型deployment定义一个pod，配置其亲和性时须指定匹配表达式，其根本的匹配都是要对Node或pod的labels标签进行条件匹配。而这些labels标签匹配计算就必须要用到labels.selector工具(公共使用部分)。 所以在将此块最底层的匹配计算分析部分放在最前面，以便于后面源码分析部分更容易理解。 labels.selector接口定义，关键的方法是Matchs() vendor/k8s.io/apimachinery/pkg/labels/selector.go:36 type Selector interface { Matches(Labels) bool Empty() bool String() string Add(r ...Requirement) Selector Requirements() (requirements Requirements, selectable bool) DeepCopySelector() Selector } 看一下调用端，如下面的几个实例的func，调用labels.NewSelector()实例化一个labels.selector对象返回. func LabelSelectorAsSelector(ps *LabelSelector) (labels.Selector, error) { ... selector := labels.NewSelector() ... } func NodeSelectorRequirementsAsSelector(nsm []v1.NodeSelectorRequirement) (labels.Selector, error) { ... selector := labels.NewSelector() ... } func TopologySelectorRequirementsAsSelector(tsm []v1.TopologySelectorLabelRequirement) (labels.Selector, error) { ... selector := labels.NewSelector() ... } NewSelector返回的是一个InternelSelector类型，而InternelSelector类型是一个Requirement（必要条件） 类型的列表。 vendor/k8s.io/apimachinery/pkg/labels/selector.go:79 func NewSelector() Selector { return internalSelector(nil) } type internalSelector []Requirement InternelSelector类的Matches()底层实现是遍历调用requirement.Matches() vendor/k8s.io/apimachinery/pkg/labels/selector.go:340 func (lsel internalSelector) Matches(l Labels) bool { for ix := range lsel { // internalSelector[ix]为Requirement if matches := lsel[ix].Matches(l); !matches { return false } } return true } 再来看下requirment结构定义(key、操作符、值 ) \"这就是配置的亲和匹配条件表达式\" vendor/k8s.io/apimachinery/pkg/labels/selector.go:114 type Requirement struct { key string operator selection.Operator // In huge majority of cases we have at most one value here. // It is generally faster to operate on a single-element slice // than on a single-element map, so we have a slice here. strValues []string } requirment.matchs() 真正的条件表达式操作实现,基于表达式operator,计算key/value,返回匹配与否 vendor/k8s.io/apimachinery/pkg/labels/selector.go:192 func (r *Requirement) Matches(ls Labels) bool { switch r.operator { case selection.In, selection.Equals, selection.DoubleEquals: if !ls.Has(r.key) { //IN return false } return r.hasValue(ls.Get(r.key)) case selection.NotIn, selection.NotEquals: //NotIn if !ls.Has(r.key) { return true } return !r.hasValue(ls.Get(r.key)) case selection.Exists: //Exists return ls.Has(r.key) case selection.DoesNotExist: //NotExists return !ls.Has(r.key) case selection.GreaterThan, selection.LessThan: // GT、LT if !ls.Has(r.key) { return false } lsValue, err := strconv.ParseInt(ls.Get(r.key), 10, 64) //能转化为数值的”字符数值“ if err != nil { klog.V(10).Infof(\"ParseInt failed for value %+v in label %+v, %+v\", ls.Get(r.key), ls, err) return false } // There should be only one strValue in r.strValues, and can be converted to a integer. if len(r.strValues) != 1 { klog.V(10).Infof(\"Invalid values count %+v of requirement %#v, for 'Gt', 'Lt' operators, exactly one value is required\", len(r.strValues), r) return false } var rValue int64 for i := range r.strValues { rValue, err = strconv.ParseInt(r.strValues[i], 10, 64) if err != nil { klog.V(10).Infof(\"ParseInt failed for value %+v in requirement %#v, for 'Gt', 'Lt' operators, the value must be an integer\", r.strValues[i], r) return false } } return (r.operator == selection.GreaterThan && lsValue > rValue) || (r.operator == selection.LessThan && lsValue 注： 除了LabelsSelector外还有NodeSelector 、FieldsSelector、PropertySelector等，但基本都是类似的Selector接口实现，逻辑上都基本一致，后在源码分析过程有相应的说明。 2. Node亲和性 Node亲和性基础描述: yml配置实例sample： --- apiVersion:v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: #pod实例部署在prd-zone-A 或 prd-zone-B requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/prd-zone-name operator: In values: - prd-zone-A - prd-zone-B preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: securityZone operator: In values: - BussinssZone containers: - name: with-node-affinity image: gcr.io/google_containers/pause:2.0 2.1. Node亲和性预选策略MatchNodeSelectorPred 策略说明： 基于NodeSelector和NodeAffinity定义为被调度的pod选择相匹配的Node（Nodes Labels） 适用NodeAffinity配置项： NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution 预选策略源码分析： 策略注册： defaults.init()注册了一条名为“MatchNodeSelectorPred”预选策略项,策略Func是PodMatchNodeSelector() pkg/scheduler/algorithmprovider/defaults/defaults.go:78 func init() { ... factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) ... } 策略Func: PodMatchNodeSelector() 获取目标Node信息,调用podMatchesNodeSelectorAndAffinityTerms()对被调度pod和目标node进行亲和性匹配。 如果符合则返回true,反之false并记录错误信息。 pkg/scheduler/algorithm/predicates/predicates.go:853 func PodMatchNodeSelector(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { // 获取node信息 node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } // 关键子逻辑func // 输入参数:被调度的pod和前面获取的node(被检测的node) if podMatchesNodeSelectorAndAffinityTerms(pod, node) { return true, nil, nil } return false, []algorithm.PredicateFailureReason{ErrNodeSelectorNotMatch}, nil } ​ podMatchesNodeSelectorAndAffinityTerms() ​ NodeSelector和NodeAffinity定义的\"必要条件\"配置匹配检测 pkg/scheduler/algorithm/predicates/predicates.go:807 func podMatchesNodeSelectorAndAffinityTerms(pod *v1.Pod, node *v1.Node) bool { // 如果设置了NodeSelector,则检测Node labels是否满足NodeSelector所定义的所有terms项. if len(pod.Spec.NodeSelector) > 0 { selector := labels.SelectorFromSet(pod.Spec.NodeSelector) if !selector.Matches(labels.Set(node.Labels)) { return false } } //如果设置了NodeAffinity，则进行Node亲和性匹配 nodeMatchesNodeSelectorTerms() *[后面有详细分析]* nodeAffinityMatches := true affinity := pod.Spec.Affinity if affinity != nil && affinity.NodeAffinity != nil { nodeAffinity := affinity.NodeAffinity if nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution == nil { return true } if nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution != nil { nodeSelectorTerms := nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms klog.V(10).Infof(\"Match for RequiredDuringSchedulingIgnoredDuringExecution node selector terms %+v\", nodeSelectorTerms) // 关键处理func: nodeMatchesNodeSelectorTerms() nodeAffinityMatches = nodeAffinityMatches && nodeMatchesNodeSelectorTerms(node, nodeSelectorTerms) } } return nodeAffinityMatches } 注： NodeSelector和NodeAffinity.Require... 都存在配置则都需True； 如果NodeSelector失败则直接false,不处理NodeAffinity; 如果指定了多个 NodeSelectorTerms，那 node只要满足其中一个条件; 如果指定了多个 MatchExpressions，那必须要满足所有条件. nodeMatchesNodeSelectorTerms() 调用v1helper.MatchNodeSelectorTerms()进行NodeSelectorTerm定义的必要条件进行检测是否符合。 关键的配置定义分为两类(matchExpressions/matchFileds)： -“requiredDuringSchedulingIgnoredDuringExecution.matchExpressions”定义检测(匹配key与value) -“requiredDuringSchedulingIgnoredDuringExecution.matchFileds”定义检测(不匹配key，只value) pkg/scheduler/algorithm/predicates/predicates.go:797 func nodeMatchesNodeSelectorTerms(node *v1.Node, nodeSelectorTerms []v1.NodeSelectorTerm) bool { nodeFields := map[string]string{} // 获取检测目标node的Filelds for k, f := range algorithm.NodeFieldSelectorKeys { nodeFields[k] = f(node) } // 调用v1helper.MatchNodeSelectorTerms() // 参数：nodeSelectorTerms 亲和性配置的必要条件Terms // labels 被检测的目标node的label列表 // fields 被检测的目标node filed列表 return v1helper.MatchNodeSelectorTerms(nodeSelectorTerms, labels.Set(node.Labels), fields.Set(nodeFields)) } // pkg/apis/core/v1/helper/helpers.go:302 func MatchNodeSelectorTerms( nodeSelectorTerms []v1.NodeSelectorTerm, nodeLabels labels.Set, nodeFields fields.Set,) bool { for _, req := range nodeSelectorTerms { // nil or empty term selects no objects if len(req.MatchExpressions) == 0 && len(req.MatchFields) == 0 { continue } // MatchExpressions条件表达式匹配 ① if len(req.MatchExpressions) != 0 { labelSelector, err := NodeSelectorRequirementsAsSelector(req.MatchExpressions) if err != nil || !labelSelector.Matches(nodeLabels) { continue } } // MatchFields条件表达式匹配 ② if len(req.MatchFields) != 0 { fieldSelector, err := NodeSelectorRequirementsAsFieldSelector(req.MatchFields) if err != nil || !fieldSelector.Matches(nodeFields) { continue } } return true } return false } ① NodeSelectorRequirementAsSelector() 是对“requiredDuringSchedulingIgnoredDuringExecution.matchExpressions\"所配置的表达式进行Selector表达式进行格式化加工，返回一个labels.Selector实例化对象. [本文开头1.2章节有分析] pkg/apis/core/v1/helper/helpers.go:222 func NodeSelectorRequirementsAsSelector(nsm []v1.NodeSelectorRequirement) (labels.Selector, error) { if len(nsm) == 0 { return labels.Nothing(), nil } selector := labels.NewSelector() for _, expr := range nsm { var op selection.Operator switch expr.Operator { case v1.NodeSelectorOpIn: op = selection.In case v1.NodeSelectorOpNotIn: op = selection.NotIn case v1.NodeSelectorOpExists: op = selection.Exists case v1.NodeSelectorOpDoesNotExist: op = selection.DoesNotExist case v1.NodeSelectorOpGt: op = selection.GreaterThan case v1.NodeSelectorOpLt: op = selection.LessThan default: return nil, fmt.Errorf(\"%q is not a valid node selector operator\", expr.Operator) } // 表达式的三个关键要素： expr.Key, op, expr.Values r, err := labels.NewRequirement(expr.Key, op, expr.Values) if err != nil { return nil, err } selector = selector.Add(*r) } return selector, nil } ② NodeSelectorRequirementAsFieldSelector() 是对“requiredDuringSchedulingIgnoredDuringExecution.matchFields\"所配置的表达式进行Selector表达式进行格式化加工，返回一个Fields.Selector实例化对象. pkg/apis/core/v1/helper/helpers.go:256 func NodeSelectorRequirementsAsFieldSelector(nsm []v1.NodeSelectorRequirement) (fields.Selector, error) { if len(nsm) == 0 { return fields.Nothing(), nil } selectors := []fields.Selector{} for _, expr := range nsm { switch expr.Operator { case v1.NodeSelectorOpIn: if len(expr.Values) != 1 { return nil, fmt.Errorf(\"unexpected number of value (%d) for node field selector operator %q\", len(expr.Values), expr.Operator) } selectors = append(selectors, fields.OneTermEqualSelector(expr.Key, expr.Values[0])) case v1.NodeSelectorOpNotIn: if len(expr.Values) != 1 { return nil, fmt.Errorf(\"unexpected number of value (%d) for node field selector operator %q\", len(expr.Values), expr.Operator) } selectors = append(selectors, fields.OneTermNotEqualSelector(expr.Key, expr.Values[0])) default: return nil, fmt.Errorf(\"%q is not a valid node field selector operator\", expr.Operator) } } return fields.AndSelectors(selectors...), nil } 关键数据结构 NodeSelector相关结构的定义 vendor/k8s.io/api/core/v1/types.go:2436 type NodeSelector struct { NodeSelectorTerms []NodeSelectorTerm `json:\"nodeSelectorTerms\" protobuf:\"bytes,1,rep,name=nodeSelectorTerms\"` } type NodeSelectorTerm struct { MatchExpressions []NodeSelectorRequirement `json:\"matchExpressions,omitempty\" protobuf:\"bytes,1,rep,name=matchExpressions\"` MatchFields []NodeSelectorRequirement `json:\"matchFields,omitempty\" protobuf:\"bytes,2,rep,name=matchFields\"` } type NodeSelectorRequirement struct { Key string `json:\"key\" protobuf:\"bytes,1,opt,name=key\"` Operator NodeSelectorOperator `json:\"operator\" protobuf:\"bytes,2,opt,name=operator,casttype=NodeSelectorOperator\"` Values []string `json:\"values,omitempty\" protobuf:\"bytes,3,rep,name=values\"` } type NodeSelectorOperator string const ( NodeSelectorOpIn NodeSelectorOperator = \"In\" NodeSelectorOpNotIn NodeSelectorOperator = \"NotIn\" NodeSelectorOpExists NodeSelectorOperator = \"Exists\" NodeSelectorOpDoesNotExist NodeSelectorOperator = \"DoesNotExist\" NodeSelectorOpGt NodeSelectorOperator = \"Gt\" NodeSelectorOpLt NodeSelectorOperator = \"Lt\" ) FieldsSelector实现类的结构定义（Match value) vendor/k8s.io/apimachinery/pkg/fields/selector.go:78 type hasTerm struct { field, value string } func (t *hasTerm) Matches(ls Fields) bool { return ls.Get(t.field) == t.value } type notHasTerm struct { field, value string } func (t *notHasTerm) Matches(ls Fields) bool { return ls.Get(t.field) != t.value } 2.2. Node亲和性优选策略NodeAffinityPriority 策略说明： 通过被调度的pod亲和性配置定义条件，对潜在可被调度运行的Nodes进行亲和性匹配并评分. 适用NodeAffinity配置项： NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution 预选策略源码分析： 策略注册：defaultPriorities()注册了一条名为“NodeAffinityPriority”优选策略项.并注册了策略的两个方法Map/Reduce： CalculateNodeAffinityPriorityMap() map计算， 对潜在被调度Node进行亲和匹配，并为其计权重得分. CalculateNodeAffinityPriorityReduce() reduce计算，重新统计得分,取值区间0~10. pkg/scheduler/algorithmprovider/defaults/defaults.go:266 //k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go/algorithmprovider/defaults.go func defaultPriorities() sets.String { ... factory.RegisterPriorityFunction2(\"NodeAffinityPriority\", priorities.CalculateNodeAffinityPriorityMap, priorities.CalculateNodeAffinityPriorityReduce, 1), ... } 策略Func: map计算 CalculateNodeAffinityPriorityMap() 遍历affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution所 定义的Terms解NodeSelector对象(labels.selector)后，对潜在被调度Node的labels进行Match匹配检测，如果匹配则将条件所给定的Weight权重值累计。 最后将返回各潜在的被调度Node最后分值。 pkg/scheduler/algorithm/priorities/node_affinity.go:34 func CalculateNodeAffinityPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { // 获取被检测的Node信息 node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } // 默认为Spec配置的Affinity affinity := pod.Spec.Affinity if priorityMeta, ok := meta.(*priorityMetadata); ok { // We were able to parse metadata, use affinity from there. affinity = priorityMeta.affinity } var count int32 if affinity != nil && affinity.NodeAffinity != nil && affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution != nil { // 遍历PreferredDuringSchedulingIgnoredDuringExecution定义的`必要条件项`(Terms) for i := range affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution { preferredSchedulingTerm := &affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution[i] if preferredSchedulingTerm.Weight == 0 { //注意前端的配置，如果weight为0则不做任何处理 continue } // TODO: Avoid computing it for all nodes if this becomes a performance problem. // 获取node亲和MatchExpression表达式条件，实例化label.Selector对象. nodeSelector, err := v1helper.NodeSelectorRequirementsAsSelector(preferredSchedulingTerm.Preference.MatchExpressions) if err != nil { return schedulerapi.HostPriority{}, err } if nodeSelector.Matches(labels.Set(node.Labels)) { count += preferredSchedulingTerm.Weight } } } // 返回Node得分 return schedulerapi.HostPriority{ Host: node.Name, Score: int(count), }, nil } 再次看到前面(预选策略分析时)分析过的NodeSelectorRequirementAsSelector()返回一个labels.Selector实例对象 使用selector.Matches对node.Labels进行匹配是否符合条件. reduce计算 CalculateNodeAffinityPriorityReduce() 将各个node的最后得分重新计算分布区间在0〜10. 代码内给定一个NormalizeReduce()方法，MaxPriority值为10,reverse取反false关闭 pkg/scheduler/algorithm/priorities/node_affinity.go:77 const MaxPriority = 10 var CalculateNodeAffinityPriorityReduce = NormalizeReduce(schedulerapi.MaxPriority, false) NormalizeReduce() 结果评分取值0〜MaxPriority reverse取反为true时，最终评分=(MaxPriority-其原评分值） pkg/scheduler/algorithm/priorities/reduce.go:29 func NormalizeReduce(maxPriority int, reverse bool) algorithm.PriorityReduceFunction { return func( _ *v1.Pod, _ interface{}, _ map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error { var maxCount int // 取出最大的值 for i := range result { if result[i].Score > maxCount { maxCount = result[i].Score } } // 如果最大的值为0，且取反设为真，则将所有的评分设置为MaxPriority if maxCount == 0 { if reverse { for i := range result { result[i].Score = maxPriority } } return nil } // 计算后得分 = maxPrority * 原分值 / 最大值 // 如果取反为真则 maxPrority - 计算后得分 for i := range result { score := result[i].Score score = maxPriority * score / maxCount if reverse { score = maxPriority - score } result[i].Score = score } return nil } } 3. Pod亲和性 Pod亲和性基础描述: yml配置实例sample： --- apiVersion: apps/v1beta1 kind: Deployment metadata: name: affinity labels: app: affinity spec: replicas: 3 template: metadata: labels: app: affinity role: lab-web spec: containers: - name: nginx image: nginx:1.9.0 ports: - containerPort: 80 name: nginx_web_Lab affinity: #为实现高可用，三个pod应该分布在不同Node上 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - prod-pod topologyKey: kubernetes.io/hostname 3.1. Pod亲和性预选策略MatchInterPodAffinityPred 策略说明： 对需被调度的Pod进行亲和/反亲和配置匹配检测目标Pods，然后获取满足亲和条件的Pods所运行的Nodes ​的 TopologyKey的值(亲和性pod定义topologyKey)与目标 Nodes进行一一匹配是否符合条件. 适用NodeAffinity配置项： PodAffinity.RequiredDuringSchedulingIgnoredDuringExecutionPodAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution 预选策略源码分析： 策略注册：defaultPredicates()注册了一条名为“MatchInterPodAffinity”预选策略项. pkg/scheduler/algorithmprovider/defaults/defaults.go:143 func defaultPredicates() sets.String { ... factory.RegisterFitPredicateFactory( predicates.MatchInterPodAffinityPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewPodAffinityPredicate(args.NodeInfo, args.PodLister) }, ... } 策略Func: checker.InterPodAffinityMatches() Func是通过NewPodAffinityProdicate()实例化PodAffinityChecker类对象后返回。 pkg/scheduler/algorithm/predicates/predicates.go:1138 type PodAffinityChecker struct { info NodeInfo podLister algorithm.PodLister } func NewPodAffinityPredicate(info NodeInfo, podLister algorithm.PodLister) algorithm.FitPredicate { checker := &PodAffinityChecker{ info: info, podLister: podLister, } return checker.InterPodAffinityMatches //返回策略func } InterPodAffinityMatches() 检测一个pod是否满足调度到特定的（符合pod亲和或反亲和配置）Node上。 satisfiesExistingPodsAntiAffinity() 满足存在的Pods反亲和配置. satisfiesPodsAffinityAntiAffinity() 满足Pods亲和与反亲和配置. pkg/scheduler/algorithm/predicates/predicates.go:1155 func (c *PodAffinityChecker) InterPodAffinityMatches(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } //① if failedPredicates, error := c.satisfiesExistingPodsAntiAffinity(pod, meta, nodeInfo); failedPredicates != nil { failedPredicates := append([]algorithm.PredicateFailureReason{ErrPodAffinityNotMatch}, failedPredicates) return false, failedPredicates, error } // Now check if requirements will be satisfied on this node. affinity := pod.Spec.Affinity if affinity == nil || (affinity.PodAffinity == nil && affinity.PodAntiAffinity == nil) { return true, nil, nil } //② if failedPredicates, error := c.satisfiesPodsAffinityAntiAffinity(pod, meta, nodeInfo, affinity); failedPredicates != nil { failedPredicates := append([]algorithm.PredicateFailureReason{ErrPodAffinityNotMatch}, failedPredicates) return false, failedPredicates, error } return true, nil, nil } ① satisfiesExistingPodsAntiAffinity() 检测当pod被调度到目标node上是否触犯了其它pods所定义的反亲和配置. 即：当调度一个pod到目标Node上，而某个或某些Pod定义了反亲和配置与被 调度的Pod相匹配(触犯)，那么就不应该将此Node加入到可选的潜在调度Nodes列表内. pkg/scheduler/algorithm/predicates/predicates.go:1293 func (c *PodAffinityChecker) satisfiesExistingPodsAntiAffinity(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return ErrExistingPodsAntiAffinityRulesNotMatch, fmt.Errorf(\"Node is nil\") } var topologyMaps *topologyPairsMaps //如果存在预处理的MetaData则直接获取topologyPairsAntiAffinityPodsMap if predicateMeta, ok := meta.(*predicateMetadata); ok { topologyMaps = predicateMeta.topologyPairsAntiAffinityPodsMap } else { // 不存在预处理的MetaData处理逻辑. // 过滤掉pod的nodeName等于NodeInfo.Node.Name,且不存在于nodeinfo中. // 即运行在其它Nodes上的Pods filteredPods, err := c.podLister.FilteredList(nodeInfo.Filter, labels.Everything()) if err != nil { errMessage := fmt.Sprintf(\"Failed to get all pods, %+v\", err) klog.Error(errMessage) return ErrExistingPodsAntiAffinityRulesNotMatch, errors.New(errMessage) } // 获取被调度Pod与其它存在反亲和配置的Pods匹配的topologyMaps if topologyMaps, err = c.getMatchingAntiAffinityTopologyPairsOfPods(pod, filteredPods); err != nil { errMessage := fmt.Sprintf(\"Failed to get all terms that pod %+v matches, err: %+v\", podName(pod), err) klog.Error(errMessage) return ErrExistingPodsAntiAffinityRulesNotMatch, errors.New(errMessage) } } // 遍历所有topology pairs(所有反亲和topologyKey/Value)，检测Node是否有影响. for topologyKey, topologyValue := range node.Labels { if topologyMaps.topologyPairToPods[topologyPair{key: topologyKey, value: topologyValue}] != nil { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v\", podName(pod), node.Name) return ErrExistingPodsAntiAffinityRulesNotMatch, nil } } return nil, nil } getMatchingAntiAffinityTopologyPairsOfPods() 获取被调度Pod与其它存在反亲和配置的Pods匹配的topologyMaps pkg/scheduler/algorithm/predicates/predicates.go:1270 func (c *PodAffinityChecker) getMatchingAntiAffinityTopologyPairsOfPods(pod *v1.Pod, existingPods []*v1.Pod) (*topologyPairsMaps, error) { topologyMaps := newTopologyPairsMaps() // 遍历所有存在Pods,获取pod所运行的Node信息 for _, existingPod := range existingPods { existingPodNode, err := c.info.GetNodeInfo(existingPod.Spec.NodeName) if err != nil { if apierrors.IsNotFound(err) { klog.Errorf(\"Node not found, %v\", existingPod.Spec.NodeName) continue } return nil, err } // 依据被调度的pod、目标pod、目标Node信息(上面获取得到)获取TopologyPairs。 // getMatchingAntiAffinityTopologyPairsOfPod()下面详述 existingPodTopologyMaps, err := getMatchingAntiAffinityTopologyPairsOfPod(pod, existingPod, existingPodNode) if err != nil { return nil, err } topologyMaps.appendMaps(existingPodTopologyMaps) } return topologyMaps, nil } //1)是否ExistingPod定义了反亲和配置，如果没有直接返回 //2)如果有定义，是否有任务一个反亲和Term匹配需被调度的pod. // 如果配置则将返回term定义的TopologyKey和Node的topologyValue. func getMatchingAntiAffinityTopologyPairsOfPod(newPod *v1.Pod, existingPod *v1.Pod, node *v1.Node) (*topologyPairsMaps, error) { affinity := existingPod.Spec.Affinity if affinity == nil || affinity.PodAntiAffinity == nil { return nil, nil } topologyMaps := newTopologyPairsMaps() for _, term := range GetPodAntiAffinityTerms(affinity.PodAntiAffinity) { namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(existingPod, &term) selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector) if err != nil { return nil, err } if priorityutil.PodMatchesTermsNamespaceAndSelector(newPod, namespaces, selector) { if topologyValue, ok := node.Labels[term.TopologyKey]; ok { pair := topologyPair{key: term.TopologyKey, value: topologyValue} topologyMaps.addTopologyPair(pair, existingPod) } } } return topologyMaps, nil } ② satisfiesPodsAffinityAntiAffinity() 满足Pods亲和与反亲和配置. 我们先看一下代码结构，我将共分为两个部分if{}部分,else{}部分,依赖于是否指定了预处理的预选metadata. pkg/scheduler/algorithm/predicates/predicates.go:1367 func (c *PodAffinityChecker) satisfiesPodsAffinityAntiAffinity(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo, affinity *v1.Affinity) (algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return ErrPodAffinityRulesNotMatch, fmt.Errorf(\"Node is nil\") } if predicateMeta, ok := meta.(*predicateMetadata); ok { ... //partI } else { ... //partII } return nil, nil } partI if{...} 如果指定了预处理metadata，则使用此逻辑，否则跳至else{...} 获取所有pod亲和性定义AffinityTerms，如果存在亲和性定义，基于指定的metadata判断AffinityTerms所定义的nodeTopoloykey与值是否所有都存在于metadata.topologyPairsPotentialAffinityPods之内（潜在匹配亲和定义的pod list）。 获取所有pod亲和性定义AntiAffinityTerms，如果存在反亲和定义，基于指定的metadata判断AntiAffinityTerms所定义的nodeTopoloykey与值 是否有一个存在于 metadata.topologyPairsPotentialAntiAffinityPods之内的情况（潜在匹配anti反亲和定义的pod list）。 if predicateMeta, ok := meta.(*predicateMetadata); ok { // 检测所有affinity terms. topologyPairsPotentialAffinityPods := predicateMeta.topologyPairsPotentialAffinityPods if affinityTerms := GetPodAffinityTerms(affinity.PodAffinity); len(affinityTerms) > 0 { matchExists := c.nodeMatchesAllTopologyTerms(pod, topologyPairsPotentialAffinityPods, nodeInfo, affinityTerms) if !matchExists { if !(len(topologyPairsPotentialAffinityPods.topologyPairToPods) == 0 && targetPodMatchesAffinityOfPod(pod, pod)) { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAffinity\", podName(pod), node.Name) return ErrPodAffinityRulesNotMatch, nil } } } // 检测所有anti-affinity terms. topologyPairsPotentialAntiAffinityPods := predicateMeta.topologyPairsPotentialAntiAffinityPods if antiAffinityTerms := GetPodAntiAffinityTerms(affinity.PodAntiAffinity); len(antiAffinityTerms) > 0 { matchExists := c.nodeMatchesAnyTopologyTerm(pod, topologyPairsPotentialAntiAffinityPods, nodeInfo, antiAffinityTerms) if matchExists { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAntiAffinity\", podName(pod), node.Name) return ErrPodAntiAffinityRulesNotMatch, nil } } } 以下说明继续if{…}内所用的各个子逻辑函数分析(按代码位置的先后顺序)： GetPodAffinityTerms() 如果存在podAffinity硬件配置，获取所有\"匹配必要条件”Terms pkg/scheduler/algorithm/predicates/predicates.go:1217 func GetPodAffinityTerms(podAffinity *v1.PodAffinity) (terms []v1.PodAffinityTerm) { if podAffinity != nil { if len(podAffinity.RequiredDuringSchedulingIgnoredDuringExecution) != 0 { terms = podAffinity.RequiredDuringSchedulingIgnoredDuringExecution } } return terms } nodeMatchesAllTopologyTerms() 判断目标Node是否匹配所有亲和性配置的定义Terms的topology值. pkg/scheduler/algorithm/predicates/predicates.go:1336 // 目标Node须匹配所有Affinity terms所定义的TopologyKey，且值须与nodes(运行被亲和匹配表达式匹配的Pods) // 的TopologyKey和值相匹配。 // 注：此逻辑内metadata预计算了topologyPairs func (c *PodAffinityChecker) nodeMatchesAllTopologyTerms(pod *v1.Pod, topologyPairs *topologyPairsMaps, nodeInfo *schedulercache.NodeInfo, terms []v1.PodAffinityTerm) bool { node := nodeInfo.Node() for _, term := range terms { // 判断目标node上是否存在亲和配置定义的TopologyKey的key，取出其topologykey值 // 根据key与值创建topologyPair // 基于metadata.topologyPairsPotentialAffinityPods(潜在亲和pods的topologyPairs)判断\\ //目标node上的ToplogyKey与value是否相互匹配. if topologyValue, ok := node.Labels[term.TopologyKey]; ok { pair := topologyPair{key: term.TopologyKey, value: topologyValue} if _, ok := topologyPairs.topologyPairToPods[pair]; !ok { return false // 一项不满足则为false } } else { return false } } return true } // topologyPairsMaps结构定义 type topologyPairsMaps struct { topologyPairToPods map[topologyPair]podSet podToTopologyPairs map[string]topologyPairSet } targetPodMatchesAffinityOfPod() 根据pod的亲和定义检测目标pod的NameSpace是否符合条件以及 Labels.selector条件表达式是否匹配. pkg/scheduler/algorithm/predicates/metadata.go:498 func targetPodMatchesAffinityOfPod(pod, targetPod *v1.Pod) bool { affinity := pod.Spec.Affinity if affinity == nil || affinity.PodAffinity == nil { return false } affinityProperties, err := getAffinityTermProperties(pod, GetPodAffinityTerms(affinity.PodAffinity)) // ① if err != nil { klog.Errorf(\"error in getting affinity properties of Pod %v\", pod.Name) return false } // ② return podMatchesAllAffinityTermProperties(targetPod, affinityProperties) } // ① 获取affinityTerms所定义所有的namespaces 和 selector 列表， // 返回affinityTermProperites数组. 数组的每项定义{namesapces,selector}. func getAffinityTermProperties(pod *v1.Pod, terms []v1.PodAffinityTerm) (properties []*affinityTermProperties, err error) { if terms == nil { return properties, nil } for _, term := range terms { namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(pod, &term) // 基于定义的亲和性term，创建labels.selector selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector) if err != nil { return nil, err } // 返回 namespaces 和 selector properties = append(properties, &affinityTermProperties{namespaces: namespaces, selector: selector}) } return properties, nil } // 返回Namespace列表（如果term未指定Namespace则使用被调度pod的Namespace）. func GetNamespacesFromPodAffinityTerm(pod *v1.Pod, podAffinityTerm *v1.PodAffinityTerm) sets.String { names := sets.String{} if len(podAffinityTerm.Namespaces) == 0 { names.Insert(pod.Namespace) } else { names.Insert(podAffinityTerm.Namespaces...) } return names } // ② 遍历properties所有定义的namespaces 和 selector 列表，调用PodMatchesTermsNamespaceAndSelector()进行一一匹配. func podMatchesAllAffinityTermProperties(pod *v1.Pod, properties []*affinityTermProperties) bool { if len(properties) == 0 { return false } for _, property := range properties { if !priorityutil.PodMatchesTermsNamespaceAndSelector(pod, property.namespaces, property.selector) { return false } } return true } // 检测NameSpaces一致性和Labels.selector是否匹配. // - 如果pod.Namespaces不相等于指定的NameSpace值则返回false，如果true则继续labels match. // - 如果pod.labels不能Match Labels.selector选择器，则返回false,反之true func PodMatchesTermsNamespaceAndSelector(pod *v1.Pod, namespaces sets.String, selector labels.Selector) bool { if !namespaces.Has(pod.Namespace) { return false } if !selector.Matches(labels.Set(pod.Labels)) { return false } return true } GetPodAntiAffinityTerms() 获取pod反亲和配置所有的必要条件Terms pkg/scheduler/algorithm/predicates/predicates.go:1231 func GetPodAntiAffinityTerms(podAntiAffinity *v1.PodAntiAffinity) (terms []v1.PodAffinityTerm) { if podAntiAffinity != nil { if len(podAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution) != 0 { terms = podAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution } } return terms } nodeMatchesAnyTopologyTerm() 判断目标Node是否有匹配了反亲和的定义Terms的topology值*. pkg/scheduler/algorithm/predicates/predicates.go:1353 // Node只须匹配任何一条AnitAffinity terms所定义的TopologyKey则为True // 逻辑等同于nodeMatchesAllTopologyTerms(),只是匹配一条则返回为true. func (c *PodAffinityChecker) nodeMatchesAnyTopologyTerm(pod *v1.Pod, topologyPairs *topologyPairsMaps, nodeInfo *schedulercache.NodeInfo, terms []v1.PodAffinityTerm) bool { node := nodeInfo.Node() for _, term := range terms { if topologyValue, ok := node.Labels[term.TopologyKey]; ok { pair := topologyPair{key: term.TopologyKey, value: topologyValue} if _, ok := topologyPairs.topologyPairToPods[pair]; ok { return true // 一项满足则为true } } } return false } partII else{...} 如果没有预处理的Metadata，则通过指定podFilter过滤器获取满足条件的pod列表 获取所有亲和配置定义，如果存在则，通过获取PodAffinity所定义的所有namespaces和标签条件表达式进行匹配”目标pod\",完全符合则获取此目标pod的运行node的topologykey（此为affinity指定的topologykey）的 值和\"潜在Node\"的topologykey的值比对是否一致。 与上类似，获取所有anti反亲和配置定义，如果存在则，通过获取PodAntiAffinity所定义的所有namespaces和标签条件表达式进行匹配”目标pod\",完全符合则获取此目标pod的运行node的topologykey（此为AntiAffinity指定的topologykey）的值和\"潜在Node\"的topologykey的值比对是否一致。 else { // We don't have precomputed metadata. We have to follow a slow path to check affinity terms. filteredPods, err := c.podLister.FilteredList(nodeInfo.Filter, labels.Everything()) if err != nil { return ErrPodAffinityRulesNotMatch, err } //获取亲和、反亲和配置定义的\"匹配条件\"Terms affinityTerms := GetPodAffinityTerms(affinity.PodAffinity) antiAffinityTerms := GetPodAntiAffinityTerms(affinity.PodAntiAffinity) matchFound, termsSelectorMatchFound := false, false for _, targetPod := range filteredPods { // 遍历所有目标Pod,检测所有亲和性配置\"匹配条件\"Terms if !matchFound && len(affinityTerms) > 0 { // podMatchesPodAffinityTerms()对namespaces和标签条件表达式进行匹配目标pod【详解后述】 affTermsMatch, termsSelectorMatch, err := c.podMatchesPodAffinityTerms(pod, targetPod, nodeInfo, affinityTerms) if err != nil { errMessage := fmt.Sprintf(\"Cannot schedule pod %+v onto node %v, because of PodAffinity, err: %v\", podName(pod), node.Name, err) klog.Error(errMessage) return ErrPodAffinityRulesNotMatch, errors.New(errMessage) } if termsSelectorMatch { termsSelectorMatchFound = true } if affTermsMatch { matchFound = true } } // 同上，遍历所有目标Pod,检测所有Anti反亲和配置\"匹配条件\"Terms. if len(antiAffinityTerms) > 0 { antiAffTermsMatch, _, err := c.podMatchesPodAffinityTerms(pod, targetPod, nodeInfo, antiAffinityTerms) if err != nil || antiAffTermsMatch { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAntiAffinityTerm, err: %v\", podName(pod), node.Name, err) return ErrPodAntiAffinityRulesNotMatch, nil } } } if !matchFound && len(affinityTerms) > 0 { if termsSelectorMatchFound { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAffinity\", podName(pod), node.Name) return ErrPodAffinityRulesNotMatch, nil } // Check if pod matches its own affinity properties (namespace and label selector). if !targetPodMatchesAffinityOfPod(pod, pod) { klog.V(10).Infof(\"Cannot schedule pod %+v onto node %v, because of PodAffinity\", podName(pod), node.Name) return ErrPodAffinityRulesNotMatch, nil } } } 以下说明继续else{…}内所用的子逻辑函数分析： podMatchesPodAffinityTerms() 通过获取亲和配置定义的所有namespaces和标签条件表达式进行匹配目标pod,完全符合则获取此目标pod的运行node的topologykey（此为affinity指定的topologykey）的值和潜在Node的topologykey的值比对是否一致. pkg/scheduler/algorithm/predicates/predicates.go:1189 func (c *PodAffinityChecker) podMatchesPodAffinityTerms(pod, targetPod *v1.Pod, nodeInfo *schedulercache.NodeInfo, terms []v1.PodAffinityTerm) (bool, bool, error) { if len(terms) == 0 { return false, false, fmt.Errorf(\"terms array is empty\") } // 获取{namespaces,selector}列表 props, err := getAffinityTermProperties(pod, terms) if err != nil { return false, false, err } // 匹配目标pod是否在affinityTerm定义的{namespaces,selector}列表内所有项，如果不匹配则返回false, // 如果匹配则获取此pod的运行node信息(称为目标Node)， // 通过“目标Node”所定义的topologykey（此为affinity指定的topologykey）的值来匹配“潜在被调度的Node”的topologykey是否一致。 if !podMatchesAllAffinityTermProperties(targetPod, props) { return false, false, nil } // Namespace and selector of the terms have matched. Now we check topology of the terms. targetPodNode, err := c.info.GetNodeInfo(targetPod.Spec.NodeName) if err != nil { return false, false, err } for _, term := range terms { if len(term.TopologyKey) == 0 { return false, false, fmt.Errorf(\"empty topologyKey is not allowed except for PreferredDuringScheduling pod anti-affinity\") } if !priorityutil.NodesHaveSameTopologyKey(nodeInfo.Node(), targetPodNode, term.TopologyKey) { return false, true, nil } } return true, true, nil } priorityutil.NodesHaveSameTopologyKey() 正真的toplogykey比较实现的逻辑代码块。 *从此代码可以看出deployment的yml对topologykey设定的可以支持自定义的 pkg/scheduler/algorithm/priorities/util/topologies.go:53 // 判断两者的topologyKey定义的值是否一致。 func NodesHaveSameTopologyKey(nodeA, nodeB *v1.Node, topologyKey string) bool { if len(topologyKey) == 0 { return false } if nodeA.Labels == nil || nodeB.Labels == nil { return false } nodeALabel, okA := nodeA.Labels[topologyKey] //取Node一个被意义化的“Label”的值value nodeBLabel, okB := nodeB.Labels[topologyKey] // If found label in both nodes, check the label if okB && okA { return nodeALabel == nodeBLabel //比对 } return false } 3.2. Pod亲和性优选策略InterPodAffinityPriority 策略说明： 并发遍历所有潜在的目标Nodes，对Pods与需被调度Pod的亲和和反亲性检测，对亲性匹配则增，对反亲性 匹配则减， 最终对每个Node进行统计分数。 适用NodeAffinity配置项： PodAffinity.PreferredDuringSchedulingIgnoredDuringExecutionPodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution 预选策略源码分析： 策略注册：defaultPriorities()注册了一条名为“InterPodAffinityPriority”优选策略项. pkg/scheduler/algorithmprovider/defaults/defaults.go:145 // k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go func defaultPriorities() sets.String { ... factory.RegisterPriorityConfigFactory( \"InterPodAffinityPriority\", factory.PriorityConfigFactory{ Function: func(args factory.PluginFactoryArgs) algorithm.PriorityFunction { return priorities.NewInterPodAffinityPriority(args.NodeInfo, args.NodeLister, args.PodLister, args.HardPodAffinitySymmetricWeight) }, Weight: 1, }, ), ... } 策略Func: interPodAffinity.CalculateInterPodAffinityPriority() 通过NewPodAffinityPriority()实例化interPodAffinityod类对象及CalculateInterPodAffinityPriority()策略Func返回。 pkg/scheduler/algorithm/priorities/interpod_affinity.go:45 func NewInterPodAffinityPriority( info predicates.NodeInfo, nodeLister algorithm.NodeLister, podLister algorithm.PodLister, hardPodAffinityWeight int32) algorithm.PriorityFunction { interPodAffinity := &InterPodAffinity{ info: info, nodeLister: nodeLister, podLister: podLister, hardPodAffinityWeight: hardPodAffinityWeight, } return interPodAffinity.CalculateInterPodAffinityPriority } CalculateInterPodAffinityPriority() 基于pod亲和性配置匹配\"必要条件项”Terms,并发处理所有目标nodes,为其目标node统计亲和weight得分. 我们先来看一下它的代码结构： processPod := func(existingPod *v1.Pod) error {… pm.processTerms()} processNode := func(i int) {…} workqueue.ParallelizeUntil(context.TODO(), 16, len(allNodeNames), processNode) fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) 此代码逻辑需理解几个定义： pod 一个\"需被调度的Pod\" hasAffinityConstraints \"被调度的pod\"是否有定义亲和配置 hasAntiAffinityConstraints \"被调度的pod\"是否有定义亲和配置 existingPod 一个待处理的\"亲和目标pod\" existingPodNode 运行此“亲和目标pod”的节点--“目标Node” existingHasAffinityConstraints \"亲和目标pod\"是否存在亲和约束existingHasAntiAffinityConstraints \"亲和目标pod\"是否存在反亲和约束 pkg/scheduler/algorithm/priorities/interpod_affinity.go:119 func (ipa *InterPodAffinity) CalculateInterPodAffinityPriority(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) { affinity := pod.Spec.Affinity //\"需被调度Pod\"是否存在亲和、反亲和约束配置 hasAffinityConstraints := affinity != nil && affinity.PodAffinity != nil hasAntiAffinityConstraints := affinity != nil && affinity.PodAntiAffinity != nil allNodeNames := make([]string, 0, len(nodeNameToInfo)) for name := range nodeNameToInfo { allNodeNames = append(allNodeNames, name) } var maxCount float64 var minCount float64 pm := newPodAffinityPriorityMap(nodes) // processPod()主要处理pod亲和和反亲和weight累计的逻辑代码。 ② // 调用了Terms处理方法：processTerms() processPod := func(existingPod *v1.Pod) error { ... // 亲和性检测逻辑代码 ① pm.processTerms(terms, pod, existingPod, existingPodNode, 1) ... } //ProcessNode()通过一个判断是否存在亲和性配置选择调用processPod() ③ processNode := func(i int) { ... if err := processPod(existingPod); err != nil { pm.setError(err) } ... } // 并发多线程处理调用ProcessNode() workqueue.ParallelizeUntil(context.TODO(), 16, len(allNodeNames), processNode) ... for _, node := range nodes { if pm.counts[node.Name] > maxCount { maxCount = pm.counts[node.Name] } if pm.counts[node.Name] 0 { //reduce计算fScore分 ④ fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) } result = append(result, schedulerapi.HostPriority{ Host: node.Name, Score: int(fScore) }) } } return result, nil } ① ProcessTerms() 给定Pod和此Pod的定义的亲和性配置(podAffinityTerm)、被测目标pod、运行被测目标pod的Node信息，对所有潜在可被调度的Nodes列表进行一一检测,并对根据检测结果为node进行weight累计。 流程如下： “被测Pod”的namespaces是否与“给定的pod”的namespaces是否一致； “被测Pod”的labels是否与“给定的pod”的podAffinityTerm定义匹配; 如果前两条件都为True，则对运行“被测的pod”的node的TopologyKey的值与所有潜在可被调度的Node进行遍历检测 TopologyKey的值是否一致，true则累计weight值. 逻辑理解： 1与2实现了找出在同一个namespace下满足被调pod所配置podAffinityTerm的pods; 3则实现获取topologyKey的值与潜在被调度的Node进行匹配检测” . 此处则可清楚的理解pod亲和性配置匹配的内在含义与逻辑。 pkg/scheduler/algorithm/priorities/interpod_affinity.go:107 func (p *podAffinityPriorityMap) processTerms(terms []v1.WeightedPodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, multiplier int) { for i := range terms { term := &terms[i] p.processTerm(&term.PodAffinityTerm, podDefiningAffinityTerm, podToCheck, fixedNode, float64(term.Weight*int32(multiplier))) } } func (p *podAffinityPriorityMap) processTerm(term *v1.PodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, weight float64) { // 获取namesapce信息(affinityTerm.Namespaces或pod.Namesapce) // 根据podAffinityTerm定义生成selector对象（参看本文开头的述labelSelector） namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(podDefiningAffinityTerm, term) selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector) //labeSelector if err != nil { p.setError(err) return } //判断“被检测的Pod”的Namespace和Selector Labels是否匹配 match := priorityutil.PodMatchesTermsNamespaceAndSelector(podToCheck, namespaces, selector) if match { func() { p.Lock() defer p.Unlock() for _, node := range p.nodes { //对\"运行被检测亲和Pod的Node节点\" 与被考虑的所有Nodes进行一一匹配TopologyKey检查,如相等则进行累加权值 if priorityutil.NodesHaveSameTopologyKey(node, fixedNode, term.TopologyKey) { p.counts[node.Name] += weight } } }() } } GetNamespaceFromPodAffinitTerm() 返回Namespaces列表（如果term未指定Namespace则使用被调度pod的Namespace） pkg/scheduler/algorithm/priorities/util/topologies.go:28 func GetNamespacesFromPodAffinityTerm(pod *v1.Pod, podAffinityTerm *v1.PodAffinityTerm) sets.String { names := sets.String{} if len(podAffinityTerm.Namespaces) == 0 { names.Insert(pod.Namespace) } else { names.Insert(podAffinityTerm.Namespaces...) } return names } PodMatchesTermsNamespaceAndSelector() 检测NameSpace一致性和Labels.selector是否匹配. pkg/scheduler/algorithm/priorities/util/topologies.go:40 func PodMatchesTermsNamespaceAndSelector(pod *v1.Pod, namespaces sets.String, selector labels.Selector) bool { if !namespaces.Has(pod.Namespace) { return false } if !selector.Matches(labels.Set(pod.Labels)) { return false } return true } ② processPod() 处理亲和和反亲和逻辑层，调用processTerms()进行检测与统计权重值。 pkg/scheduler/algorithm/priorities/interpod_affinity.go:136 processPod := func(existingPod *v1.Pod) error { existingPodNode, err := ipa.info.GetNodeInfo(existingPod.Spec.NodeName) if err != nil { if apierrors.IsNotFound(err) { klog.Errorf(\"Node not found, %v\", existingPod.Spec.NodeName) return nil } return err } existingPodAffinity := existingPod.Spec.Affinity existingHasAffinityConstraints := existingPodAffinity != nil && existingPodAffinity.PodAffinity != nil existingHasAntiAffinityConstraints := existingPodAffinity != nil && existingPodAffinity.PodAntiAffinity != nil //如果\"需被调度的Pod\"存在亲和约束，则与\"亲和目标Pod\"和\"亲和目标Node\"进行一次ProcessTerms()检测，如果成立则wieght权重值加1倍. if hasAffinityConstraints { terms := affinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, pod, existingPod, existingPodNode, 1) } // 如果\"需被调度的Pod\"存在反亲和约束，则与\"亲和目标Pod\"和\"亲和目标Node\"进行一次ProcessTerms()检测，如果成立则wieght权重值减1倍. if hasAntiAffinityConstraints { terms := affinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, pod, existingPod, existingPodNode, -1) } //如果\"亲和目标Pod\"存在亲和约束，则反过来与\"需被调度的Pod\"和\"亲和目标Node\"进行一次ProcessTerms()检测，如果成立则wieght权重值加1倍. if existingHasAffinityConstraints { if ipa.hardPodAffinityWeight > 0 { terms := existingPodAffinity.PodAffinity.RequiredDuringSchedulingIgnoredDuringExecution for _, term := range terms { pm.processTerm(&term, existingPod, pod, existingPodNode, float64(ipa.hardPodAffinityWeight)) } } terms := existingPodAffinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, existingPod, pod, existingPodNode, 1) } // 如果\"亲和目标Pod\"存在反亲和约束，则反过来与\"需被调度的Pod\"和\"亲和目标Node\"进行一次ProcessTerms()检测，如果成立则wieght权重值减1倍. if existingHasAntiAffinityConstraints { terms := existingPodAffinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, existingPod, pod, existingPodNode, -1) } return nil } ③ processNode 如果\"被调度pod\"未定义亲和配置，则检测潜在Nodes的亲和性定义. pkg/scheduler/algorithm/priorities/interpod_affinity.go:193 processNode := func(i int) { nodeInfo := nodeNameToInfo[allNodeNames[i]] if nodeInfo.Node() != nil { if hasAffinityConstraints || hasAntiAffinityConstraints { // We need to process all the nodes. for _, existingPod := range nodeInfo.Pods() { if err := processPod(existingPod); err != nil { pm.setError(err) } } } else { for _, existingPod := range nodeInfo.PodsWithAffinity() { if err := processPod(existingPod); err != nil { pm.setError(err) } } } } } ④ 最后的得分fscore计算公式： // 10 * (node权重累计值 - 最小权重得分值) / (最大权重得分值 - 最小权重得分值) fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) const ( // MaxPriority defines the max priority value. MaxPriority = 10 ) 4. Service亲和性 在default调度器代码内并未注册此预选策略，仅有代码实现。连google/baidu上都无法查询到相关使用案例，配置用法不予分析，仅看下面源码详细分析。 代码场景应用注释译文： 一个服务的第一个Pod被调度到带有Label “region=foo”的Nodes（资源集群）上， 那么其服务后面的其它Pod都将调度至Label “region=foo”的Nodes。 4.1. Serice亲和性预选策略checkServiceAffinity 通过NewServiceAffinityPredicate()创建一个ServiceAffinity类对象，并返回两个预选策略所必须的处理Func: affinity.checkServiceAffinity 基于预选元数据Meta，对被调度的pod检测Node是否满足服务亲和性. affinity.serverAffinityMetadataProducer 基于预选Meta的pod信息，获取服务信息和在相同NameSpace下的的Pod列表，供亲和检测时使用。 后面将详述处理func pkg/scheduler/algorithm/predicates/predicates.go:955 func NewServiceAffinityPredicate(podLister algorithm.PodLister, serviceLister algorithm.ServiceLister, nodeInfo NodeInfo, labels []string) (algorithm.FitPredicate, PredicateMetadataProducer) { affinity := &ServiceAffinity{ podLister: podLister, serviceLister: serviceLister, nodeInfo: nodeInfo, labels: labels, } return affinity.checkServiceAffinity, affinity.serviceAffinityMetadataProducer } affinity.serverAffinityMetadataProducer() 输入：predicateMateData 返回：services 和 pods 基于预选MetaData的pod信息查询出services 基于预选MetaData的pod Lables获取所有匹配的pods,且过滤掉仅剩在同一个Namespace的pods。 pkg/scheduler/algorithm/predicates/predicates.go:934 func (s *ServiceAffinity) serviceAffinityMetadataProducer(pm *predicateMetadata) { if pm.pod == nil { klog.Errorf(\"Cannot precompute service affinity, a pod is required to calculate service affinity.\") return } pm.serviceAffinityInUse = true var errSvc, errList error // 1.基于预选MetaData的pod信息查询services pm.serviceAffinityMatchingPodServices, errSvc = s.serviceLister.GetPodServices(pm.pod) // 2.基于预选MetaData的pod Lables获取所有匹配的pods selector := CreateSelectorFromLabels(pm.pod.Labels) allMatches, errList := s.podLister.List(selector) // In the future maybe we will return them as part of the function. if errSvc != nil || errList != nil { klog.Errorf(\"Some Error were found while precomputing svc affinity: \\nservices:%v , \\npods:%v\", errSvc, errList) } // 3.过滤掉仅剩在同一个Namespace的pods pm.serviceAffinityMatchingPodList = FilterPodsByNamespace(allMatches, pm.pod.Namespace) } affinity.checkServiceAffinity() 基于预处理的MetaData，对被调度的pod检测Node是否满足服务亲和性。 最终的亲和检测Labels: ​ Final affinityLabels =（A ∩ B）+ （B ∩ C） 与 node.Labels 进行Match计算 //∩交集符号 A: 需被调度pod的NodeSelector配置 B: 需被调度pod定义的服务亲和affinityLabels配置 C: 被选定的亲和目标Node的Lables pkg/scheduler/algorithm/predicates/predicates.go:992 func (s *ServiceAffinity) checkServiceAffinity(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { var services []*v1.Service var pods []*v1.Pod if pm, ok := meta.(*predicateMetadata); ok && (pm.serviceAffinityMatchingPodList != nil || pm.serviceAffinityMatchingPodServices != nil) { services = pm.serviceAffinityMatchingPodServices pods = pm.serviceAffinityMatchingPodList } else { // Make the predicate resilient in case metadata is missing. pm = &predicateMetadata{pod: pod} s.serviceAffinityMetadataProducer(pm) pods, services = pm.serviceAffinityMatchingPodList, pm.serviceAffinityMatchingPodServices } // 筛选掉存在于Node（nodeinfo）上pods，且与之进行podKey比对不相等的pods。 ① filteredPods := nodeInfo.FilterOutPods(pods) node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } // affinityLabes交集 ==（A ∩ B） // A：被调度pod的NodeSelector定义 B：定义的亲和性Labels ② affinityLabels := FindLabelsInSet(s.labels, labels.Set(pod.Spec.NodeSelector)) // Step 1: If we don't have all constraints, introspect nodes to find the missing constraints. if len(s.labels) > len(affinityLabels) { if len(services) > 0 { if len(filteredPods) > 0 { //\"被选定的亲和Node\" //基于第一个filteredPods获取Node信息 nodeWithAffinityLabels, err := s.nodeInfo.GetNodeInfo(filteredPods[0].Spec.NodeName) if err != nil { return false, nil, err } // 输入：交集Labels、服务亲和Labels、被选出的亲和Node Lables // affinityLabels = affinityLabels + 交集（B ∩ C） // B: 服务亲和Labels C:被选出的亲和Node的Lables ③ AddUnsetLabelsToMap(affinityLabels, s.labels, labels.Set(nodeWithAffinityLabels.Labels)) } } } // 进行一次最终的匹配（affinityLabels 与 被检测亲和的node.Labels ） ④ if CreateSelectorFromLabels(affinityLabels).Matches(labels.Set(node.Labels)) { return true, nil, nil } return false, []algorithm.PredicateFailureReason{ErrServiceAffinityViolated}, nil } ① FilterOutPods() 筛选掉存在于Node（nodeinfo）上pods，且与之进行podKey比对不相等的pods filteredPods = 未在Node上的pods + 在node上但podKey相同的pods pkg/scheduler/cache/node_info.go:656 func (n *NodeInfo) FilterOutPods(pods []*v1.Pod) []*v1.Pod { //获取Node的详细信息 node := n.Node() if node == nil { return pods } filtered := make([]*v1.Pod, 0, len(pods)) for _, p := range pods { //如果pod（亲和matched）的NodeName 不等于Spec配置的nodeNmae (即pod不在此Node上)，将pod放入filtered. if p.Spec.NodeName != node.Name { filtered = append(filtered, p) continue } //如果在此Node上，则获取podKey（pod.UID） //遍历此Node上所有的目标Pods，获取每个podKey进行与匹配pod的podkey是否相同， //相同则将pod放入filtered并返回 podKey, _ := GetPodKey(p) for _, np := range n.Pods() { npodkey, _ := GetPodKey(np) if npodkey == podKey { filtered = append(filtered, p) break } } } return filtered } ② FindLabelsInSet() 参数一： (B)定义的亲和性Labels配置 参数二： (A)被调度pod的定义NodeSelector配置Selector 检测存在的于NodeSelector的亲和性Labels配置，则取两者的交集部分. （A ∩ B） pkg/scheduler/algorithm/predicates/utils.go:26 func FindLabelsInSet(labelsToKeep []string, selector labels.Set) map[string]string { aL := make(map[string]string) for _, l := range labelsToKeep { if selector.Has(l) { aL[l] = selector.Get(l) } } return aL } ③ AddUnsetLabelsToMap() 参数一： (N)在FindLabelsInSet()计算出来的交集Labels 参数二： (B)定义的亲和性Labels配置 参数三： (C)\"被选出的亲和Node\"上的Lables 检测存在的于\"被选出的亲和Node\"上的亲和性配置Labels，则取两者的交集部分存放至N. (B ∩ C)=>N pkg/scheduler/algorithm/predicates/utils.go:37 // 输入：交集Labels、服务亲和Labels、被选出的亲和Node Lables // 填充：Labels交集 ==（B ∩ C） B: 服务亲和Labels C:被选出的亲和Node Lables func AddUnsetLabelsToMap(aL map[string]string, labelsToAdd []string, labelSet labels.Set) { for _, l := range labelsToAdd { // 如果存在则不作任何操作 if _, exists := aL[l]; exists { continue } // 反之，计算包含的交集部分 C ∩ B if labelSet.Has(l) { aL[l] = labelSet.Get(l) } } } ④ CreateSelectorFromLabels().Match() 返回labels.Selector对象 pkg/scheduler/algorithm/predicates/utils.go:62 func CreateSelectorFromLabels(aL map[string]string) labels.Selector { if aL == nil || len(aL) == 0 { return labels.Everything() } return labels.Set(aL).AsSelector() } End Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 专题-亲和性调度(Author - XiaoYang)1. 简介1.1. 约束调度1.2. Labels.selector标签选择器2. Node亲和性2.1. Node亲和性预选策略MatchNodeSelectorPred2.2. Node亲和性优选策略NodeAffinityPriority3. Pod亲和性3.1. Pod亲和性预选策略MatchInterPodAffinityPred3.2. Pod亲和性优选策略InterPodAffinityPriority4. Service亲和性4.1. Serice亲和性预选策略checkServiceAffinity"},"core/scheduler/summarize.html":{"url":"core/scheduler/summarize.html","title":"scheduler 总结","keywords":"","body":"Scheduler 总结 In Edit 1. kube-scheduler 是什么 The Kubernetes scheduler runs as a process alongside the other master components such as the API server. Its interface to the API server is to watch for Pods with an empty PodSpec.NodeName, and for each Pod, it posts a binding indicating where the Pod should be scheduled. 一句话介绍 kube-scheduler 的话，就是一个独立运行的程序，负责和 API Server 交互，获取 PodSpec.NodeName 属性为空的 Pods 列表，然后一个个计算最适合 Pod 运行的 Node，最后将 Pod 绑定到对应的 Node 上。 2. Scheduler 工作流程 宏观上看，kube-scheduler 的工作流类似下图： 对于一个给定的pod +---------------------------------------------+ | 可用于调度的nodes如下： | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 初步过滤: node 3 资源不足 +----------------------+----------------------+ | v +----------------------+----------------------+ | 剩下的nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 优先级算法计算结果: node 1: 分数=2 node 2: 分数=5 +----------------------+----------------------+ | v 选择分值最高的节点 = node 2 上述流程图涉及到两个关键步骤：Predicate 和 Priority. Predicate：对应大家平时说的预选过程。在 predicate 过程中 scheduler 需要过滤掉无法满足 pod 正常运行需求的 node，比如不满足 pod 的“硬亲和性”需求。在 predicate 过程过滤后剩下的 node 理论上都可以成功让 pod 运行起来。 Priority：对于大家平时说的优选过程。Pod 运行毕竟只需要1个 node，所以在预选结束后剩下的 node 要经过第二次计算从而得出最合适跑给定 pod 的 node. 3. 代码层级工作流程 4. Scheduler 拓展性 //TODO 5. Scheduler 高级调度 NodeSelector - 最简单的用法 NodeAffinity - node 亲和与反亲和 PodAffinity - pod 亲和与反亲和 Taint / Toleration - 污点和容忍 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 Scheduler 总结1. kube-scheduler 是什么2. Scheduler 工作流程3. 代码层级工作流程4. Scheduler 拓展性5. Scheduler 高级调度"},"core/controller-manager/":{"url":"core/controller-manager/","title":"controller-manager","keywords":"","body":"controller-manager 本章 owner：farmer-hutao Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 "},"core/controller-manager/controller.html":{"url":"core/controller-manager/controller.html","title":"控制器概述","keywords":"","body":"控制器概述 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 "},"core/controller-manager/custom-controller.html":{"url":"core/controller-manager/custom-controller.html","title":"自定义控制器","keywords":"","body":"自定义控制器 Custom controller 工作流 1. Custom controller 工作流 上半部分 client-go 的逻辑我们在放在 “PART III - 周边项目 / client-go / Informer” 中分析。 今天来关注这幅图的下半部分，也就是 Custom controller 中需要我们自己编码的部分，对应图中的黄色方块的内容。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 自定义控制器1. Custom controller 工作流"},"core/apiserver/":{"url":"core/apiserver/","title":"apiserver","keywords":"","body":"apiserver Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/kube-proxy/":{"url":"core/kube-proxy/","title":"kube-proxy","keywords":"","body":"kube-proxy 本章 owner：XiaoYang 在kubernetes中几个基础概念如pod、endpoints、 service。提供相同服务的一组pod可以抽象成一个service,通过service提供的统一入口对外提供服务。kube-proxy组件被安装在kubernetes集群的各个node节点上，实现了kubernetes service机制(实现集群内客户端pod访问service,或者外部通过NodePort、XLB或ingress等方式访问)。kube-proxy提供了三种服务负载模式: 基于用户态的userspace模式 iptables模式 ipvs模式 kube-proxy源码框架风格继承了kubernetes组件的一惯风格，而且kube-proxy源码更为简洁，基本上分为三层。第一层为标准的CLI应用App创建层，使用kubernetes通用的Cobra框架来构建App和初始化配置。第二层为主应用进程proxy服务器相关对象的创建与运行层。第三层为kubernetes的\"service层\"的实现机制层(proxyMode选择决定)。 kube-proxy源码分析以下几部分内容进行展开： kube-proxy服务框架分析，CLI应用创建与初始化以及proxyserver创建和Run主框架代码； kube-proxy框架整体逻辑分析，具体的proxy逻辑代码； kube-proxy三种模式源码分析，详细分析userspace/iptables/ipvs三种模式的实现(每种模式单文分析)； kube-proxy其它，如关键数据结构、类关系图等。 1. 本章规划 Proxy 服务框架 Iptables-mode Proxier Ipvs-mode Proxier Userspace-mode Proxier Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 kube-proxy1. 本章规划"},"core/kube-proxy/arch.html":{"url":"core/kube-proxy/arch.html","title":"Proxy 服务框架","keywords":"","body":"Proxy 服务框架 程序入口与初始化 Proxy Server创建 Proxy Server运行 同步规则机制(Informer) 1. 程序入口与初始化 kube-proxy使用通用Cobra框架构建一个标准的CLI应用.同kubernets基础组件(如:scheduler)一样的方式来创建应用和运行应用，因此应用command构建层不将对其深入分析，我们将重心聚焦在后面的proxyserver创建与proxyserver run代码分析。 cmd/kube-proxy/proxy.go:35 func main() { rand.Seed(time.Now().UnixNano()) command := app.NewProxyCommand() //Cobra命令风格应用对象创建 //...... //对前面实例化Command对象的执行 if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"error: %v\\n\", err) os.Exit(1) } } NewProxyCommand()返回命令command对象,command.Execute()则调用定义的Run: func{},执行内部的opts.Run(). cmd/kube-proxy/app/server.go:370 func NewProxyCommand() *cobra.Command { // 创建options对象(全局的应用配置对象) opts := NewOptions() cmd := &cobra.Command{ Use: \"kube-proxy\", Long: `The Kubernetes network proxy runs on each node...略`, Run: func(cmd *cobra.Command, args []string) { //Command应用RUN命令func定义 verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) if err := initForOS(opts.WindowsService); err != nil { klog.Fatalf(\"failed OS init: %v\", err) } // 完成所有需求的options配置(外置配置文件、自定义主机名处理、特征功能开关设置等) if err := opts.Complete(); err != nil { klog.Fatalf(\"failed complete: %v\", err) } // 校验kube-proxy配置的有效性 if err := opts.Validate(args); err != nil { klog.Fatalf(\"failed validate: %v\", err) } klog.Fatal(opts.Run()) //应用真正执行Run(),后面进行分析 }, } var err error opts.config, err = opts.ApplyDefaults(opts.config) //应用默认配置，完成配置的初始化工作 if err != nil { klog.Fatalf(\"unable to create flag defaults: %v\", err) } opts.AddFlags(cmd.Flags()) cmd.MarkFlagFilename(\"config\", \"yaml\", \"yml\", \"json\") return cmd //返回命令command对象 } NewOptions() 全局配置对象创建 cmd/kube-proxy/app/server.go:184 func NewOptions() *Options { return &Options{ config: new(kubeproxyconfig.KubeProxyConfiguration), //实例化配置config对象 healthzPort: ports.ProxyHealthzPort, //Healthz端口 metricsPort: ports.ProxyStatusPort, //metrics端口 scheme: scheme.Scheme, codecs: scheme.Codecs, CleanupIPVS: true, } } //完整的配置结构体与命令执行时用户传递的命令选项相对应 type KubeProxyConfiguration struct { metav1.TypeMeta FeatureGates map[string]bool //功能特征模块开关 BindAddress string //默认所有接口0.0.0.0 HealthzBindAddress string //默认0.0.0.0:10256 MetricsBindAddress string //默认127.0.0.1:10249 EnableProfiling bool //\"/debug/pprof\" ClusterCIDR string //ClusterCIDR HostnameOverride string //自定义Hostname ClientConnection apimachineryconfig.ClientConnectionConfiguration //apiserver client IPTables KubeProxyIPTablesConfiguration //IPTABLES配置项(地址伪装、同步周期等) IPVS KubeProxyIPVSConfiguration //IPVS配置项(同步周期、调度器等) OOMScoreAdj *int32 //修改OOMScoreAdj分值 Mode ProxyMode //proxy模式 PortRange string //端口range ResourceContainer string //\"Default: /kube-proxy\" UDPIdleTimeout metav1.Duration //UDP空闲超时 Conntrack KubeProxyConntrackConfiguration //Conntrack对象 ConfigSyncPeriod metav1.Duration //同步周期 NodePortAddresses []string //Node地址 } const ( ProxyHealthzPort = 10256 ProxyStatusPort = 10249 ) opts.Run() 创建proxy服务并启动 cmd/kube-proxy/app/server.go:250 func (o *Options) Run() error { if len(o.WriteConfigTo) > 0 { return o.writeConfigFile() //写配置文件 } proxyServer, err := NewProxyServer(o) //基于配置创建proxy服务 if err != nil { return err } return proxyServer.Run() //运行proxy服务，后续详细分析proxyserver执行代码逻辑 } 上面已完成对kube-proxy第一层(CLI创建、配置项初始化、启动)分析,下面进入proxy Server创建与启动运行代码分析( kube-proxy第二层 )。 2. Proxy Server创建 NewProxyServer() 非windows版本代码 cmd/kube-proxy/app/server_others.go:55 func NewProxyServer(o *Options) (*ProxyServer, error) { return newProxyServer(o.config, o.CleanupAndExit, o.CleanupIPVS, o.scheme, o.master) } func newProxyServer( config *proxyconfigapi.KubeProxyConfiguration, cleanupAndExit bool, cleanupIPVS bool, scheme *runtime.Scheme, master string) (*ProxyServer, error) { if config == nil { return nil, errors.New(\"config is required\") } if c, err := configz.New(proxyconfigapi.GroupName); err == nil { c.Set(config) } else { return nil, fmt.Errorf(\"unable to register configz: %s\", err) } //协议IPV4 or IPV6 protocol := utiliptables.ProtocolIpv4 if net.ParseIP(config.BindAddress).To4() == nil { klog.V(0).Infof(\"IPv6 bind address (%s), assume IPv6 operation\", config.BindAddress) protocol = utiliptables.ProtocolIpv6 } // 关键依赖工具实现的api接口iptables/ipvs/ipset/dbus // 从此处则可以看出kube-proxy底层技术的依赖（当然不同的proxy-mode，实现技术也不一样 ） var iptInterface utiliptables.Interface var ipvsInterface utilipvs.Interface var kernelHandler ipvs.KernelHandler var ipsetInterface utilipset.Interface var dbus utildbus.Interface // exec命令执行器对象创建 execer := exec.New() // dbus对象创建（linux实现进程间通信机制） dbus = utildbus.New() // iptables操作对象创建 iptInterface = utiliptables.New(execer, dbus, protocol) // IPVS kernelHandler = ipvs.NewLinuxKernelHandler() // ipset ipsetInterface = utilipset.New(execer) // IPVS环境检测 canUseIPVS, _ := ipvs.CanUseIPVSProxier(kernelHandler, ipsetInterface) if canUseIPVS { ipvsInterface = utilipvs.New(execer) } // We omit creation of pretty much everything if we run in cleanup mode if cleanupAndExit { return &ProxyServer{ execer: execer, IptInterface: iptInterface, IpvsInterface: ipvsInterface, IpsetInterface: ipsetInterface, CleanupAndExit: cleanupAndExit, }, nil } //api client client, eventClient, err := createClients(config.ClientConnection, master) if err != nil { return nil, err } //主机名 hostname, err := utilnode.GetHostname(config.HostnameOverride) if err != nil { return nil, err } // 事件广播器 eventBroadcaster := record.NewBroadcaster() // Create event recorder recorder := eventBroadcaster.NewRecorder(scheme, v1.EventSource{Component: \"kube-proxy\", Host: hostname}) nodeRef := &v1.ObjectReference{ Kind: \"Node\", Name: hostname, UID: types.UID(hostname), Namespace: \"\", } var healthzServer *healthcheck.HealthzServer var healthzUpdater healthcheck.HealthzUpdater //创建默认的healthzServer服务对象 if len(config.HealthzBindAddress) > 0 { healthzServer = healthcheck.NewDefaultHealthzServer(config.HealthzBindAddress, 2*config.IPTables.SyncPeriod.Duration, recorder, nodeRef) healthzUpdater = healthzServer } var proxier proxy.ProxyProvider var serviceEventHandler proxyconfig.ServiceHandler var endpointsEventHandler proxyconfig.EndpointsHandler // proxyMode模式配置获取 proxyMode := getProxyMode(string(config.Mode), iptInterface, kernelHandler, ipsetInterface, iptables.LinuxKernelCompatTester{}) // 节点绑定IP nodeIP := net.ParseIP(config.BindAddress) if nodeIP.IsUnspecified() { nodeIP = utilnode.GetNodeIP(client, hostname) } if proxyMode == proxyModeIPTables { // proxyMode为\"IPTables\" klog.V(0).Info(\"Using iptables Proxier.\") if config.IPTables.MasqueradeBit == nil { // MasqueradeBit must be specified or defaulted. return nil, fmt.Errorf(\"unable to read IPTables MasqueradeBit from config\") } //创建iptables proxier对象 proxierIPTables, err := iptables.NewProxier( iptInterface, utilsysctl.New(), execer, config.IPTables.SyncPeriod.Duration, config.IPTables.MinSyncPeriod.Duration, config.IPTables.MasqueradeAll, int(*config.IPTables.MasqueradeBit), config.ClusterCIDR, hostname, nodeIP, recorder, healthzUpdater, config.NodePortAddresses, ) if err != nil { return nil, fmt.Errorf(\"unable to create proxier: %v\", err) } metrics.RegisterMetrics() //iptables proxier对象和事件处理 proxier = proxierIPTables serviceEventHandler = proxierIPTables endpointsEventHandler = proxierIPTables // No turning back. Remove artifacts that might still exist from the userspace Proxier. klog.V(0).Info(\"Tearing down inactive rules.\") //模式转换清理userspace/ipvs模式所创建iptables规则 userspace.CleanupLeftovers(iptInterface) if canUseIPVS { ipvs.CleanupLeftovers(ipvsInterface, iptInterface, ipsetInterface, cleanupIPVS) } } else if proxyMode == proxyModeIPVS { // proxyMode为\"IPVS\" klog.V(0).Info(\"Using ipvs Proxier.\") //ipvs proxier对象创建 proxierIPVS, err := ipvs.NewProxier( iptInterface, ipvsInterface, ipsetInterface, utilsysctl.New(), execer, config.IPVS.SyncPeriod.Duration, config.IPVS.MinSyncPeriod.Duration, config.IPVS.ExcludeCIDRs, config.IPTables.MasqueradeAll, int(*config.IPTables.MasqueradeBit), config.ClusterCIDR, hostname, nodeIP, recorder, healthzServer, config.IPVS.Scheduler, config.NodePortAddresses, ) if err != nil { return nil, fmt.Errorf(\"unable to create proxier: %v\", err) } metrics.RegisterMetrics() //ipvs proxier对象和事件处理 proxier = proxierIPVS serviceEventHandler = proxierIPVS endpointsEventHandler = proxierIPVS klog.V(0).Info(\"Tearing down inactive rules.\") //模式转换清理userspace/iptables模式规则 userspace.CleanupLeftovers(iptInterface) iptables.CleanupLeftovers(iptInterface) } else { // proxyMode为\"userspace\" klog.V(0).Info(\"Using userspace Proxier.\") //创建RR模式负载均衡 loadBalancer := userspace.NewLoadBalancerRR() //设置EndpointsConfigHandler(endpoints事件处理) endpointsEventHandler = loadBalancer //创建userspace proxier对象 proxierUserspace, err := userspace.NewProxier( loadBalancer, net.ParseIP(config.BindAddress), iptInterface, execer, *utilnet.ParsePortRangeOrDie(config.PortRange), config.IPTables.SyncPeriod.Duration, config.IPTables.MinSyncPeriod.Duration, config.UDPIdleTimeout.Duration, config.NodePortAddresses, ) if err != nil { return nil, fmt.Errorf(\"unable to create proxier: %v\", err) } //userspace proxier对象和service事件处理 serviceEventHandler = proxierUserspace proxier = proxierUserspace klog.V(0).Info(\"Tearing down inactive rules.\") //模式转换清理iptables/ipvs模式所创建iptables规则 iptables.CleanupLeftovers(iptInterface) if canUseIPVS { ipvs.CleanupLeftovers(ipvsInterface, iptInterface, ipsetInterface, cleanupIPVS) } } //注册reloadfunc为proxier的sync()同步方法 iptInterface.AddReloadFunc(proxier.Sync) // 构建ProxyServer对象 return &ProxyServer{ Client: client, //apiServer client EventClient: eventClient, //事件client IptInterface: iptInterface, //iptables接口 IpvsInterface: ipvsInterface, //ipvs接口 IpsetInterface: ipsetInterface, //ipset接口 execer: execer, //exec命令执行器 Proxier: proxier, //proxier创建对象 Broadcaster: eventBroadcaster, //事件广播器 Recorder: recorder, //事件记录器 ConntrackConfiguration: config.Conntrack, //Conntrack配置 Conntracker: &realConntracker{}, //Conntrack对象 ProxyMode: proxyMode, //proxy模式 NodeRef: nodeRef, //node节点reference信息 MetricsBindAddress: config.MetricsBindAddress, //metric服务地址配置 EnableProfiling: config.EnableProfiling, //debug/pprof配置 OOMScoreAdj: config.OOMScoreAdj, //OOMScoreAdj值配置 ResourceContainer: config.ResourceContainer, //容器资源配置 ConfigSyncPeriod: config.ConfigSyncPeriod.Duration, //同步周期配置 ServiceEventHandler: serviceEventHandler, //处理service事件proxier对象 EndpointsEventHandler: endpointsEventHandler, //处理endpoints事件proxier对象 HealthzServer: healthzServer, //健康检测服务 }, nil } 3. Proxy Server运行 cmd/kube-proxy/app/server.go:481 func (s *ProxyServer) Run() error { klog.Infof(\"Version: %+v\", version.Get()) // 如果CleanupAndExit设置为true,则删除存在的所有iptables规则项，然后应用退出。 if s.CleanupAndExit { encounteredError := userspace.CleanupLeftovers(s.IptInterface) encounteredError = iptables.CleanupLeftovers(s.IptInterface) || encounteredError encounteredError = ipvs.CleanupLeftovers(s.IpvsInterface, s.IptInterface, s.IpsetInterface, s.CleanupIPVS) || encounteredError if encounteredError { return errors.New(\"encountered an error while tearing down rules.\") } return nil } // 根据启动参数配置\"oom-score-adj\"分值调整，取值区间[-1000, 1000] var oomAdjuster *oom.OOMAdjuster if s.OOMScoreAdj != nil { oomAdjuster = oom.NewOOMAdjuster() if err := oomAdjuster.ApplyOOMScoreAdj(0, int(*s.OOMScoreAdj)); err != nil { klog.V(2).Info(err) } } //\"resource-container\"设置是否运行在容器里 if len(s.ResourceContainer) != 0 { if err := resourcecontainer.RunInResourceContainer(s.ResourceContainer); err != nil { klog.Warningf(\"Failed to start in resource-only container %q: %v\", s.ResourceContainer, err) } else { klog.V(2).Infof(\"Running in resource-only container %q\", s.ResourceContainer) } } //事件广播器 if s.Broadcaster != nil && s.EventClient != nil { s.Broadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: s.EventClient.Events(\"\")}) } // 根据配置启动healthz健康检测服务 if s.HealthzServer != nil { s.HealthzServer.Run() } // 根据配置启动metrics服务。URI: \"/proxyMode\" 与 \"/metrics\" if len(s.MetricsBindAddress) > 0 { mux := mux.NewPathRecorderMux(\"kube-proxy\") healthz.InstallHandler(mux) mux.HandleFunc(\"/proxyMode\", func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"%s\", s.ProxyMode) }) mux.Handle(\"/metrics\", prometheus.Handler()) if s.EnableProfiling { routes.Profiling{}.Install(mux) } configz.InstallHandler(mux) go wait.Until(func() { err := http.ListenAndServe(s.MetricsBindAddress, mux) if err != nil { utilruntime.HandleError(fmt.Errorf(\"starting metrics server failed: %v\", err)) } }, 5*time.Second, wait.NeverStop) } // 如果需要(命令选项或配置项)调节conntrack配置值 if s.Conntracker != nil { max, err := getConntrackMax(s.ConntrackConfiguration) if err != nil { return err } if max > 0 { err := s.Conntracker.SetMax(max) if err != nil { if err != readOnlySysFSError { return err } const message = \"DOCKER RESTART NEEDED (docker issue #24000): /sys is read-only: \" + \"cannot modify conntrack limits, problems may arise later.\" s.Recorder.Eventf(s.NodeRef, api.EventTypeWarning, err.Error(), message) } } //设置conntracker的TCPEstablishedTimeout if s.ConntrackConfiguration.TCPEstablishedTimeout != nil && s.ConntrackConfiguration.TCPEstablishedTimeout.Duration > 0 { timeout := int(s.ConntrackConfiguration.TCPEstablishedTimeout.Duration / time.Second) if err := s.Conntracker.SetTCPEstablishedTimeout(timeout); err != nil { return err } } //设置conntracker的TCPCloseWaitTimeout if s.ConntrackConfiguration.TCPCloseWaitTimeout != nil && s.ConntrackConfiguration.TCPCloseWaitTimeout.Duration > 0 { timeout := int(s.ConntrackConfiguration.TCPCloseWaitTimeout.Duration / time.Second) if err := s.Conntracker.SetTCPCloseWaitTimeout(timeout); err != nil { return err } } } // informer机制获取与监听Services和Endpoints的配置与事件信息 // 注册ServiceEventHandler服务事件的处理 // 注册EndpointsEventHandler端点事件的处理 informerFactory := informers.NewSharedInformerFactory(s.Client, s.ConfigSyncPeriod) serviceConfig := config.NewServiceConfig(informerFactory.Core().V1().Services(), s.ConfigSyncPeriod) serviceConfig.RegisterEventHandler(s.ServiceEventHandler) go serviceConfig.Run(wait.NeverStop) endpointsConfig := config.NewEndpointsConfig(informerFactory.Core().V1().Endpoints(), s.ConfigSyncPeriod) endpointsConfig.RegisterEventHandler(s.EndpointsEventHandler) go endpointsConfig.Run(wait.NeverStop) go informerFactory.Start(wait.NeverStop) // \"新生儿降生的哭声\"，作者命名比较有生活情趣^_^ // 服务成功启动，将启动事件广播。 // s.Recorder.Eventf(s.NodeRef, api.EventTypeNormal, \"Starting\", \"Starting kube-proxy.\") // nodeRef := &v1.ObjectReference{ // Kind: \"Node\", // Name: hostname, // UID: types.UID(hostname), // Namespace: \"\", // } s.birthCry() // Proxier(代理服务提供者)进行循环配置同步与处理proxy逻辑（本文聚焦应用主框架，后有专篇分析Proxier） // 此时将进入了proxier运行，默认使用的iptables模式的proxier对象 s.Proxier.SyncLoop() return nil } s.Proxier.SyncLoop()的运行将进入kube-proxy第三层(service实现机制层),Proxier实例化对象是在proxy server对象创建时通过config配置文件或\"-proxy-mode\"指定（userspace / iptables / ipvs模式），而默认使用的iptables模式proxier对象。第三层的代码分析将针对三种模式设立专篇分析，此处为关键部分请记住此处，在后面proxier的分析文章重点关注。 在第二层的框架层我们还须关注kube-proxy与kubernetes集群同步信息的机制informer。kube-proxy组件在proxy server的run()运行创建service、endpoints的informer对其list同步数据和watch监听事件(add、delete、update)，调用注册的proxier handler进行处理(第三层proxier的同步处理机制调用与触发)。 4. 同步规则机制(Informer) kube-proxy同样使用client-go标准的ApiServer同步方式，创建informer,注册事件处理器handler，持续监控watch事件并调用handler处理事件add/update/delete，后端处理则由proxier(userspace/iptables/ipvs)实现。因为endpoints的同步与service同步方式一致，则下面仅说明service代码实现。 pkg/proxy/config/config.go:174 func NewServiceConfig(serviceInformer coreinformers.ServiceInformer, resyncPeriod time.Duration) *ServiceConfig { result := &ServiceConfig{ lister: serviceInformer.Lister(), //监听器 listerSynced: serviceInformer.Informer().HasSynced, //监听器同步状态值 } //在服务informer上添加了资源事件的处理器handleFunc。（Add/update/delete） serviceInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs{ AddFunc: result.handleAddService, UpdateFunc: result.handleUpdateService, DeleteFunc: result.handleDeleteService, }, resyncPeriod, ) return result } pkg/proxy/config/config.go:119 func (c *ServiceConfig) Run(stopCh HandleAddService()新增事件处理 pkg/proxy/config/config.go:217 func (c *ServiceConfig) handleAddService(obj interface{}) { service, ok := obj.(*v1.Service) if !ok { utilruntime.HandleError(fmt.Errorf(\"unexpected object type: %v\", obj)) return } for i := range c.eventHandlers { klog.V(4).Info(\"Calling handler.OnServiceAdd\") c.eventHandlers[i].OnServiceAdd(service) //服务新增事件与proxier处理同步 } } HandleUpdateService()更新事件处理 pkg/proxy/config/config.go:229 func (c *ServiceConfig) handleUpdateService(oldObj, newObj interface{}) { oldService, ok := oldObj.(*v1.Service) if !ok { utilruntime.HandleError(fmt.Errorf(\"unexpected object type: %v\", oldObj)) return } service, ok := newObj.(*v1.Service) if !ok { utilruntime.HandleError(fmt.Errorf(\"unexpected object type: %v\", newObj)) return } for i := range c.eventHandlers { klog.V(4).Info(\"Calling handler.OnServiceUpdate\") c.eventHandlers[i].OnServiceUpdate(oldService, service) //服务更新事件与proxier处理同步 } } HandleDeleteService()删除事件处理 pkg/proxy/config/config.go:246 func (c *ServiceConfig) handleDeleteService(obj interface{}) { service, ok := obj.(*v1.Service) if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf(\"unexpected object type: %v\", obj)) return } if service, ok = tombstone.Obj.(*v1.Service); !ok { utilruntime.HandleError(fmt.Errorf(\"unexpected object type: %v\", obj)) return } } for i := range c.eventHandlers { klog.V(4).Info(\"Calling handler.OnServiceDelete\") c.eventHandlers[i].OnServiceDelete(service) //服务删除事件与proxier处理同步 } } 第三层proxier分析，请参看iptables、ipvs、userspace-mode proxier分析文档。 〜本文END〜 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 Proxy 服务框架1. 程序入口与初始化2. Proxy Server创建3. Proxy Server运行4. 同步规则机制(Informer)"},"core/kube-proxy/iptables.html":{"url":"core/kube-proxy/iptables.html","title":"Iptables-mode Proxier","keywords":"","body":"Iptables-mode Proxier 概述 Proxier 数据结构与类定义 Proxier对象生成与运行 Proxier 服务与端点更新 Tracker syncProxyRule 同步配置与规则 更新 service 和 endpoints ;返回更新结果 UpdateServiceMap() SVC 服务的更新实现 UpdateEndpointsMap() 端点更新的实现 创建与联接 kube 链 创建 Iptables 基础数据 为每个 service 创建 rules 配置收尾规则数据 汇集与加载 iptables 配置规则数据 IPtables 底层的 runner 实现 iptables 执行器 iptables 执行器方法 1. 概述 kube-Proxy提供三种模式(userspace/iptables/ipvs)的proxier实现,userspace是早期的proxy模式，ipvs模式处于实验性阶段proxy模式，本文先从默认的内核级iptables proxier代码实现与逻辑分析开始，其它模式将用专文解析源码。 Iptables-mode Proxier的service配置和代码内都包含一些基础概念如clusterIP、nodeport、loadbalancer、Ingress、ClusterCIDR、onlyLocal、ExternalIP等，请在了解源码之前先熟悉其概念用途场景与类型区别，再看源码将对你理解proxy事半功倍。当然也需要对netfilter、iptables、connTrack等proxy基础依赖的工具熟悉。基础概念部分在本文将不深入介绍，有需求可自行查阅相关资料。 从kube-proxy组件整体框架层的代码来看，在ProxyServer.Run()最后走到了s.Proxier.SyncLoop()执行空间一直无限loop下去。而默认的ProxyServer配置的Proxier对象就是Iptables(if proxyMode == proxyModeIPTables)，将调用iptabls-mode的Proxier.SyncLoop()，SyncLoop()时间定时循环执行syncProxyRules()完成services、endpoints与iptables规则的同步操作。 Iptables-mode proxier的负载均衡机制是通过底层netfliter/iptables规则来实现的，通过Informer机制watch服务与端点信息的变更事件触发对iptables的规则的同步更新，如下代码逻辑示意图： 下面proxier源码分析,我们先从proxier的接口、实现类、实现类方法列表一窥究竟，从结构上看整体Proxier的框架。然后我们再详细分析proxier对象的产生时所定义的属性值、值类型和用途。有了前面的两项的了解后我们再来分析proxier类方法的实现，也就是proxier代理逻辑部分(关键逻辑部分在syncProxyRules()方法分析部分)。最后我们分析proxier底层内核iptables的runner实现，也就是proxy上层逻辑层最终会调用iptables命令去执行规则的操作部分。 2. Proxier 数据结构与类定义 ProxyProvider 代理提供者接口定义，需要实现两个proxy的关键方法Sync()和SyncLoop() pkg/proxy/types.go:27 type ProxyProvider interface { // Sync 即时同步Proxy提供者的当前状态至proxy规则 Sync() // SyncLoop 周期性运行 // 作为一个线程或应用主loop运行，无返回. SyncLoop() } Iptables-mode Proxier 为 ProxyProvider 接口实现类，proxier 类属性项比较多，我们先看一下注释用途与结构定义，在实例化proxier对象时我们再详看。 pkg/proxy/iptables/proxier.go:205 type Proxier struct { endpointsChanges *proxy.EndpointChangeTracker // 端点更新信息跟踪器 serviceChanges *proxy.ServiceChangeTracker // 服务更新信息跟踪器 mu sync.Mutex // 保护同步锁 serviceMap proxy.ServiceMap // 存放服务列表信息 ① endpointsMap proxy.EndpointsMap // 存放端点列表信息 ② portsMap map[utilproxy.LocalPort]utilproxy.Closeable //端口关闭接口map endpointsSynced bool // ep同步状态 servicesSynced bool // svc同步状态 initialized int32 // 初始化状态 syncRunner *async.BoundedFrequencyRunner // 指定频率运行器，此处用于管理对 // syncProxyRules的调用 iptables utiliptables.Interface // iptables命令执行接口 masqueradeAll bool masqueradeMark string // SNAT地址伪装Mark exec utilexec.Interface // exec命令执行工具接口 clusterCIDR string // 集群CIDR hostname string // 主机名 nodeIP net.IP // 节点IP地址 portMapper utilproxy.PortOpener // TCP/UTP端口打开与监听 recorder record.EventRecorder // 事件记录器 healthChecker healthcheck.Server // healthcheck服务器对象 healthzServer healthcheck.HealthzUpdater // Healthz更新器 precomputedProbabilities []string //预计算可能性 //iptables规则与链数据(Filter/NAT) iptablesData *bytes.Buffer existingFilterChainsData *bytes.Buffer filterChains *bytes.Buffer filterRules *bytes.Buffer natChains *bytes.Buffer natRules *bytes.Buffer endpointChainsNumber int // Node节点IP与端口信息 nodePortAddresses []string networkInterfacer utilproxy.NetworkInterfacer //网络接口 } ① ServiceMap和ServicePort定义 pkg/proxy/service.go:229 type ServiceMap map[ServicePortName]ServicePort //String() => \"NS/SvcName:PortName\" ServicePortName{NamespacedName: svcName, Port: servicePort.Name} ServiceSpec service.spec定义，在用户前端可定义service的spec配置项。 vendor/k8s.io/api/core/v1/types.go:3606 type ServiceSpec struct { Ports []ServicePort //服务端口列表 Selector map[string]string //选择器 ClusterIP string //VIP 、 portal Type ServiceType //服务类型 ExternalIPs []string //外部IP列表，如外部负载均衡 SessionAffinity ServiceAffinity //会话保持 LoadBalancerIP string //service类型为\"LoadBalancer\"时配置LB ip LoadBalancerSourceRanges []string //cloud-provider的限制client ip区间 ExternalName string ExternalTrafficPolicy ServiceExternalTrafficPolicyType HealthCheckNodePort int32 PublishNotReadyAddresses bool SessionAffinityConfig *SessionAffinityConfig //会话保持配置信息 } ServicePort类定义和ServicePort接口 vendor/k8s.io/api/core/v1/types.go:3563 type ServicePort struct { Name string Protocol Protocol Port int32 TargetPort intstr.IntOrString NodePort int32 } //ServicePort接口 type ServicePort interface { // 返回服务字串，格式如: `IP:Port/Protocol`. String() string // 返回集群IP字串 ClusterIPString() string // 返回协议 GetProtocol() v1.Protocol // 返回健康检测端口 GetHealthCheckNodePort() int } ② EndpointsMap定义与Endpoint接口 pkg/proxy/endpoints.go:181 type EndpointsMap map[ServicePortName][]Endpoint type Endpoint interface { // 返回endpoint字串，格式 `IP:Port`. String() string // 是否本地 GetIsLocal() bool // 返回IP IP() string // 返回端口 Port() (int, error) // 检测两上endpoint是否相等 Equal(Endpoint) bool } Endpoints结构与相关定义 vendor/k8s.io/api/core/v1/types.go:3710 type Endpoints struct { metav1.TypeMeta metav1.ObjectMeta Subsets []EndpointSubset } type EndpointSubset struct { Addresses []EndpointAddress // EndpointAddress地址列表 NotReadyAddresses []EndpointAddress Ports []EndpointPort // EndpointPort端口列表 } type EndpointAddress struct { IP string Hostname string NodeName *string TargetRef *ObjectReference } type EndpointPort struct { Name string Port int32 Protocol Protocol } Iptables-mode Proxier提供的方法列表，先大概从名称上来了解一下方法用途，后面我在逻辑部分对主要使用的方法再深入分析。 func (proxier *Proxier) precomputeProbabilities(numberOfPrecomputed int) {/*...*/} func (proxier *Proxier) probability(n int) string{/*...*/} func (proxier *Proxier) Sync(){/*...*/} func (proxier *Proxier) SyncLoop(){/*...*/} func (proxier *Proxier) setInitialized(value bool){/*...*/} func (proxier *Proxier) isInitialized() bool{/*...*/} func (proxier *Proxier) OnServiceAdd(service *v1.Service){/*...*/} func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service){/*...*/} func (proxier *Proxier) OnServiceDelete(service *v1.Service){/*...*/} func (proxier *Proxier) OnServiceSynced(){/*...*/} func (proxier *Proxier) OnEndpointsAdd(endpoints *v1.Endpoints){/*...*/} func (proxier *Proxier) OnEndpointsUpdate(oldEndpoints, endpoints *v1.Endpoints){/*...*/} func (proxier *Proxier) OnEndpointsDelete(endpoints *v1.Endpoints) {/*...*/} func (proxier *Proxier) OnEndpointsSynced() {/*...*/} func (proxier *Proxier) deleteEndpointConnections(connectionMap []proxy.ServiceEndpoint){/*...*/} func (proxier *Proxier) appendServiceCommentLocked(args []string, svcName string){/*...*/} func (proxier *Proxier) syncProxyRules(){/*...*/} 3. Proxier对象生成与运行 iptables Proxier 构建New方法，下面省略部分校验的代码，关注关键构造部分。 pkg/proxy/iptables/proxier.go:281 func NewProxier(ipt utiliptables.Interface, sysctl utilsysctl.Interface, exec utilexec.Interface, syncPeriod time.Duration, minSyncPeriod time.Duration, masqueradeAll bool, masqueradeBit int, clusterCIDR string, hostname string, nodeIP net.IP, recorder record.EventRecorder, healthzServer healthcheck.HealthzUpdater, nodePortAddresses []string, ) (*Proxier, error) { // ...以下为省略部分解析... // sysctl对\"net/ipv4/conf/all/route_localnet\"设置 ，内核iptables支持 // sysctl对\"net/bridge/bridge-nf-call-iptables\"设置，内核iptables支持 // 生产SNAT的IP伪装mark // 如果节点IP为空，则kube-proxy的nodeIP的初始IP为127.0.0.1 // 集群CIDR检验是否为空，IPv6验证 // ... //healthcheck服务器对象 healthChecker := healthcheck.NewServer(hostname, recorder, nil, nil) //proxier对象 proxier := &Proxier{ portsMap: make(map[utilproxy.LocalPort]utilproxy.Closeable), serviceMap: make(proxy.ServiceMap), //svc存放map serviceChanges: proxy.NewServiceChangeTracker(newServiceInfo, &isIPv6, recorder), //svc变化跟踪器 endpointsMap: make(proxy.EndpointsMap), //ep存放map endpointsChanges: proxy.NewEndpointChangeTracker(hostname, newEndpointInfo, &isIPv6, recorder), //ep变化跟踪器 iptables: ipt, masqueradeAll: masqueradeAll, masqueradeMark: masqueradeMark, exec: exec, clusterCIDR: clusterCIDR, hostname: hostname, nodeIP: nodeIP, portMapper: &listenPortOpener{}, //服务端口监听器 recorder: recorder, healthChecker: healthChecker, healthzServer: healthzServer, precomputedProbabilities: make([]string, 0, 1001), iptablesData: bytes.NewBuffer(nil), //iptables配置规则数据 existingFilterChainsData: bytes.NewBuffer(nil), filterChains: bytes.NewBuffer(nil), filterRules: bytes.NewBuffer(nil), natChains: bytes.NewBuffer(nil), natRules: bytes.NewBuffer(nil), nodePortAddresses: nodePortAddresses, networkInterfacer: utilproxy.RealNetwork{}, //网络接口 } burstSyncs := 2 klog.V(3).Infof(\"minSyncPeriod: %v, syncPeriod: %v, burstSyncs: %d\", minSyncPeriod, syncPeriod, burstSyncs) //运行器执行指定频率对syncProxyRules调用来同步规则,关键的逻辑则在syncProxyRules()方法内，后面有详述方法 proxier.syncRunner = async.NewBoundedFrequencyRunner(\"sync-runner\", proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) return proxier, nil } proxier.SyncLoop() 我们知道proxy server的运行时最后的调用就是\"s.Proxier.SyncLoop()\",此处我们来详细了解一下SyncLoop的proxier运行实现。 pkg/proxy/iptables/proxier.go:487 func (proxier *Proxier) SyncLoop() { if proxier.healthzServer != nil { proxier.healthzServer.UpdateTimestamp() } // proxier.syncRunner在proxier对象创建时指定为async.NewBoundedFrequencyRunner(...) ① proxier.syncRunner.Loop(wait.NeverStop) } ① async.BoundedFrequencyRunner时间器循环func执行器。 proxier类结构内定义的proxier.syncRunner 类型为async.BoundedFrequencyRunner pkg/util/async/bounded_frequency_runner.go:31 type BoundedFrequencyRunner struct { name string minInterval time.Duration // 两次运行的最小间隔时间 maxInterval time.Duration // 两次运行的最大间隔时间 run chan struct{} // 执行一次run mu sync.Mutex fn func() // 需要运行的func lastRun time.Time // 最近一次运行时间 timer timer // 定时器 limiter rateLimiter // 按需限制运行的QPS } BoundedFrequencyRunner实例化构建，通过传参按需来控制对func的调用。 pkg/util/async/bounded_frequency_runner.go:134 func NewBoundedFrequencyRunner(name string, fn func(), minInterval, maxInterval time.Duration, burstRuns int) *BoundedFrequencyRunner { timer := realTimer{Timer: time.NewTimer(0)} // 立即tick } //实例构建 func construct(name string, fn func(), minInterval, maxInterval time.Duration, burstRuns int, timer timer) *BoundedFrequencyRunner { if maxInterval = minInterval (%v)\", name, minInterval, maxInterval)) } if timer == nil { panic(fmt.Sprintf(\"%s: timer must be non-nil\", name)) } bfr := &BoundedFrequencyRunner{ name: name, fn: fn, //被调用处理的func minInterval: minInterval, maxInterval: maxInterval, run: make(chan struct{}, 1), timer: timer, } if minInterval == 0 { //最小间隔时间如果不指定，将不受限制 bfr.limiter = nullLimiter{} } else { // TokenBucketRateLimiter的实现流控机制，有兴趣可以再深入了解机制，此处不展开 qps := float32(time.Second) / float32(minInterval) bfr.limiter = flowcontrol.NewTokenBucketRateLimiterWithClock(qps, burstRuns, timer) } return bfr } proxier.syncRunner.Loop() 时间器循环运行的实现 func (bfr *BoundedFrequencyRunner) Loop(stop BoundedFrequencyRunner.tryRun() 按指定频率对func的运行 !FILENAME:pkg/util/async/bounded_frequency_runner.go:211 func (bfr *BoundedFrequencyRunner) tryRun() { bfr.mu.Lock() defer bfr.mu.Unlock() //限制条件允许运行func if bfr.limiter.TryAccept() { bfr.fn() // 调用func bfr.lastRun = bfr.timer.Now() // 记录运行时间 bfr.timer.Stop() bfr.timer.Reset(bfr.maxInterval) // 重设下次运行时间 klog.V(3).Infof(\"%s: ran, next possible in %v, periodic in %v\", bfr.name, bfr.minInterval, bfr.maxInterval) return } //限制条件不允许运行，计算下次运行时间 elapsed := bfr.timer.Since(bfr.lastRun) // elapsed:上次运行时间到现在已过多久 nextPossible := bfr.minInterval - elapsed // nextPossible:下次运行至少差多久（最小周期） nextScheduled := bfr.maxInterval - elapsed // nextScheduled:下次运行最迟差多久(最大周期) klog.V(4).Infof(\"%s: %v since last run, possible in %v, scheduled in %v\", bfr.name, elapsed, nextPossible, nextScheduled) if nextPossible 4. Proxier 服务与端点更新 Tracker kube-proxy需要及时同步services和endpoints的变化信息,前面我们看到proxier类对象有两个属性：serviceChanges和endpointsChanges是就是用来跟踪Service和Endpoint的更新信息，我们先来分析与之相关这两个类ServiceChangeTracker和EndpointChangeTracker。 ServiceChangeTracker 服务信息变更Tracker pkg/proxy/service.go:143 type ServiceChangeTracker struct { // 同步锁保护items lock sync.Mutex // items为service变化记录map items map[types.NamespacedName]*serviceChange // makeServiceInfo允许proxier在处理服务时定制信息 makeServiceInfo makeServicePortFunc isIPv6Mode *bool //IPv6 recorder record.EventRecorder //事件记录器 } //serviceChange类型定义， 新旧服务对. type serviceChange struct { previous ServiceMap current ServiceMap } //ServiceMap类型定义 type ServiceMap map[ServicePortName]ServicePort //ServicePortName类型定义 type ServicePortName struct { types.NamespacedName Port string } //NamespacedName类型定义 type NamespacedName struct { Namespace string Name string } //实例化ServiceChangeTracker对象 func NewServiceChangeTracker(makeServiceInfo makeServicePortFunc, isIPv6Mode *bool, recorder record.EventRecorder) *ServiceChangeTracker { return &ServiceChangeTracker{ items: make(map[types.NamespacedName]*serviceChange), makeServiceInfo: makeServiceInfo, isIPv6Mode: isIPv6Mode, recorder: recorder, } } EndpointChangeTracker.Update() pkg/proxy/service.go:173 func (ect *EndpointChangeTracker) Update(previous, current *v1.Endpoints) bool { endpoints := current if endpoints == nil { endpoints = previous } // previous == nil && current == nil is unexpected, we should return false directly. if endpoints == nil { return false } namespacedName := types.NamespacedName{Namespace: endpoints.Namespace, Name: endpoints.Name} ect.lock.Lock() defer ect.lock.Unlock() change, exists := ect.items[namespacedName] if !exists { change = &endpointsChange{} change.previous = ect.endpointsToEndpointsMap(previous) ect.items[namespacedName] = change } change.current = ect.endpointsToEndpointsMap(current) // if change.previous equal to change.current, it means no change if reflect.DeepEqual(change.previous, change.current) { delete(ect.items, namespacedName) } return len(ect.items) > 0 } EndpointChangeTracker 端点信息变更Tracker pkg/proxy/endpoints.go:83 type EndpointChangeTracker struct { lock sync.Mutex // kube-proxy运行的主机名. hostname string // items为\"endpoints\"变化记录map items map[types.NamespacedName]*endpointsChange makeEndpointInfo makeEndpointFunc isIPv6Mode *bool recorder record.EventRecorder } //endpointsChange类型定义， 新旧端点对 type endpointsChange struct { previous EndpointsMap current EndpointsMap } //EndpointsMap类型定义 type EndpointsMap map[ServicePortName][]Endpoint //Endpoint接口 type Endpoint interface { // 返回endpoint字串 如格式: `IP:Port`. String() string // 是否本地主机 GetIsLocal() bool // 返回endpoint的IP部分 IP() string // 返回endpoint的Port部分 Port() (int, error) // 计算两个endpoint是否相等 Equal(Endpoint) bool } //实例化NewEndpointChangeTracker对象 func NewEndpointChangeTracker(hostname string, makeEndpointInfo makeEndpointFunc, isIPv6Mode *bool, recorder record.EventRecorder) *EndpointChangeTracker { return &EndpointChangeTracker{ hostname: hostname, items: make(map[types.NamespacedName]*endpointsChange), makeEndpointInfo: makeEndpointInfo, isIPv6Mode: isIPv6Mode, recorder: recorder, } } EndpointChangeTracker.Update() pkg/proxy/endpoints.go:116 func (ect *EndpointChangeTracker) Update(previous, current *v1.Endpoints) bool { endpoints := current if endpoints == nil { endpoints = previous } // previous == nil && current == nil is unexpected, we should return false directly. if endpoints == nil { return false } namespacedName := types.NamespacedName{Namespace: endpoints.Namespace, Name: endpoints.Name} ect.lock.Lock() defer ect.lock.Unlock() change, exists := ect.items[namespacedName] if !exists { change = &endpointsChange{} change.previous = ect.endpointsToEndpointsMap(previous) ect.items[namespacedName] = change } change.current = ect.endpointsToEndpointsMap(current) // if change.previous equal to change.current, it means no change if reflect.DeepEqual(change.previous, change.current) { delete(ect.items, namespacedName) } return len(ect.items) > 0 } Proxier服务与端点同步（Add / delete /update）都将调用ChangeTracker的update()来执行syncProxyRules Proxier.OnServiceSynced()服务信息同步 pkg/proxy/iptables/proxier.go:528 func (proxier *Proxier) OnServiceSynced() { proxier.mu.Lock() proxier.servicesSynced = true proxier.setInitialized(proxier.servicesSynced && proxier.endpointsSynced) proxier.mu.Unlock() // 非周期同步，单次执行同步 proxier.syncProxyRules() } service服务更新(add/delete/update),都调用Proxier.OnServiceUpdate()方法 pkg/proxy/iptables/proxier.go:513 func (proxier *Proxier) OnServiceAdd(service *v1.Service) { proxier.OnServiceUpdate(nil, service) //传参一:oldService为nil } // update Service 包含Add / Delete func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service) { if proxier.serviceChanges.Update(oldService, service) && proxier.isInitialized() { proxier.syncRunner.Run() //单次执行 } } func (proxier *Proxier) OnServiceDelete(service *v1.Service) { proxier.OnServiceUpdate(service, nil) //传参二:newService为nil } Proxier.OnEndpointsSynced()端点信息同步 pkg/proxy/iptables/proxier.go:552 func (proxier *Proxier) OnEndpointsSynced() { proxier.mu.Lock() proxier.endpointsSynced = true proxier.setInitialized(proxier.servicesSynced && proxier.endpointsSynced) proxier.mu.Unlock() // Sync unconditionally - this is called once per lifetime. proxier.syncProxyRules() } 同service服务一样endpoint更新(add/delete/update),都调用Proxier.OnEndpointsUpdate()方法 pkg/proxy/iptables/proxier.go:538 func (proxier *Proxier) OnEndpointsAdd(endpoints *v1.Endpoints) { proxier.OnEndpointsUpdate(nil, endpoints) //传参一:oldEndpoints为nil } // update endpoints，包含 Add / Delete func (proxier *Proxier) OnEndpointsUpdate(oldEndpoints, endpoints *v1.Endpoints) { if proxier.endpointsChanges.Update(oldEndpoints, endpoints) && proxier.isInitialized() { proxier.syncRunner.Run() } } func (proxier *Proxier) OnEndpointsDelete(endpoints *v1.Endpoints) { proxier.OnEndpointsUpdate(endpoints, nil) //传参二:endpoints为nil } 5. syncProxyRule 同步配置与规则 proxier.syncProxyRules() 实现监听svc或ep更新配置到iptables规则的一致性同步机制功能，这也是iptables proxer最核心的逻辑代码。作者实现是利用了iptables-save/iptables-restore机制将现存的iptables配置和服务与端点同步的信息来生成相对应的iptables链与规则数据，每次同步执行写入可restore标准格式的规则数据后通过iptables-restore命令进行重设iptables规则。 这个同步规则处理代码比较长，我们后面将分解成小块来讲解。下面为syncProxyRules(部分已解析注释替代)代码框架说明，内注释的每块内容在后面都将有单独代码分析说明。 pkg/proxy/iptables/proxier.go:634 func (proxier *Proxier) syncProxyRules() { proxier.mu.Lock() defer proxier.mu.Unlock() start := time.Now() defer func() { metrics.SyncProxyRulesLatency.Observe(metrics.SinceInMicroseconds(start)) klog.V(4).Infof(\"syncProxyRules took %v\", time.Since(start)) }() // don't sync rules till we've received services and endpoints if !proxier.endpointsSynced || !proxier.servicesSynced { klog.V(2).Info(\"Not syncing iptables until Services and Endpoints have been received from master\") return } // 检测与更新变化(service/endpoints) //... // 创建与联接kube链 //... // 获取现存在的Filter/Nat表链数据 //... // 创建iptables-save/restore格式数据（表头、链） //... // 写kubernets特有的SNAT地址伪装规则 //... // Accumulate NAT chains to keep. activeNATChains := map[utiliptables.Chain]bool{} // Accumulate the set of local ports that we will be holding open once this update is complete replacementPortsMap := map[utilproxy.LocalPort]utilproxy.Closeable{} endpoints := make([]*endpointsInfo, 0) endpointChains := make([]utiliptables.Chain, 0) args := make([]string, 64) // Compute total number of endpoint chains across all services. proxier.endpointChainsNumber = 0 for svcName := range proxier.serviceMap { proxier.endpointChainsNumber += len(proxier.endpointsMap[svcName]) } // 为个\"服务\"创建rules（service portal规则的创建） for svcName, svc := range proxier.serviceMap { //... } // 删除不再使用的链 //... // nodeports链 //... // FORWARD策略 //... // 配置clusterCIDR规则 //... // 写结整标签 //... //汇集与iptables-restore加载数据 //... // 关闭过旧的本地端口，更新portmap数据 for k, v := range proxier.portsMap { if replacementPortsMap[k] == nil { v.Close() } } proxier.portsMap = replacementPortsMap // 更新healthz timestamp. if proxier.healthzServer != nil { proxier.healthzServer.UpdateTimestamp() } // 更新healthchecks. if err := proxier.healthChecker.SyncServices(serviceUpdateResult.HCServiceNodePorts); err != nil { klog.Errorf(\"Error syncing healthcheck services: %v\", err) } if err := proxier.healthChecker.SyncEndpoints(endpointUpdateResult.HCEndpointsLocalIPSize); err != nil { klog.Errorf(\"Error syncing healthcheck endpoints: %v\", err) } // 完成清理工作 for _, svcIP := range staleServices.UnsortedList() { if err := conntrack.ClearEntriesForIP(proxier.exec, svcIP, v1.ProtocolUDP); err != nil { klog.Errorf(\"Failed to delete stale service IP %s connections, error: %v\", svcIP, err) } } proxier.deleteEndpointConnections(endpointUpdateResult.StaleEndpoints) } 下面将分解详述每块代码逻辑: 5.1. 更新 service 和 endpoints ;返回更新结果 pkg/proxy/iptables/proxier.go:652 //更新SVC/EP serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxy.UpdateEndpointsMap(proxier.endpointsMap, proxier.endpointsChanges) staleServices := serviceUpdateResult.UDPStaleClusterIP // 从EndpointsMap更新结果返回中合并UDP协议废弃服务信息 for _, svcPortName := range endpointUpdateResult.StaleServiceNames { if svcInfo, ok := proxier.serviceMap[svcPortName]; ok && svcInfo != nil && svcInfo.GetProtocol() == v1.ProtocolUDP { klog.V(2).Infof(\"Stale udp service %v -> %s\", svcPortName, svcInfo.ClusterIPString()) staleServices.Insert(svcInfo.ClusterIPString()) } } 5.1.1. UpdateServiceMap() SVC 服务的更新实现 pkg/proxy/service.go:212 func UpdateServiceMap(serviceMap ServiceMap, changes *ServiceChangeTracker) (result UpdateServiceMapResult) { result.UDPStaleClusterIP = sets.NewString() // 已废弃的UDP端口 serviceMap.apply(changes, result.UDPStaleClusterIP) // 应用更新map-> result.HCServiceNodePorts = make(map[types.NamespacedName]uint16) for svcPortName, info := range serviceMap { if info.GetHealthCheckNodePort() != 0 { result.HCServiceNodePorts[svcPortName.NamespacedName] = uint16(info.GetHealthCheckNodePort()) //健康检测的node Port } } return result } serviceMap.apply() 应用更新变化事件的服务项(merge->filter->unmerge) pkg/proxy/service.go:268 func (serviceMap *ServiceMap) apply(changes *ServiceChangeTracker, UDPStaleClusterIP sets.String) { changes.lock.Lock() defer changes.lock.Unlock() for _, change := range changes.items { serviceMap.merge(change.current) //合并变更增加项到serviceMap ① change.previous.filter(change.current) //过滤掉已处理更新变化的项，跳过unmerge处理 ② serviceMap.unmerge(change.previous, UDPStaleClusterIP) //删除废弃的项 ③ } // clear changes after applying them to ServiceMap. changes.items = make(map[types.NamespacedName]*serviceChange) return } ① serviceMap.merge() 合并增加\"other“内容到当前的serviceMap内。\"即将变化的服务列表进行合并”。 pkg/proxy/service.go:301 func (sm *ServiceMap) merge(other ServiceMap) sets.String { existingPorts := sets.NewString() for svcPortName, info := range other { existingPorts.Insert(svcPortName.String()) _, exists := (*sm)[svcPortName] //... (*sm)[svcPortName] = info } return existingPorts } ② serviceMap.unmerge() 从当前map移除\"other\"存在的内容项。\"即删除废弃的项\" pkg/proxy/service.go:330 func (sm *ServiceMap) unmerge(other ServiceMap, UDPStaleClusterIP sets.String) { for svcPortName := range other { info, exists := (*sm)[svcPortName] if exists { if info.GetProtocol() == v1.ProtocolUDP { UDPStaleClusterIP.Insert(info.ClusterIPString()) //存储已废丢UDP服务的集群IP列表 } delete(*sm, svcPortName) } //... } } ③ serviceMap.filter() 基于\"other\"给定的服务端口名(key值)，过滤掉存在于serviceMap的项 pkg/proxy/service.go:319 func (sm *ServiceMap) filter(other ServiceMap) { for svcPortName := range *sm { if _, ok := other[svcPortName]; ok { delete(*sm, svcPortName) } } } 5.1.2. UpdateEndpointsMap() 端点更新的实现 pkg/proxy/endpoints.go:163 func UpdateEndpointsMap(endpointsMap EndpointsMap, changes *EndpointChangeTracker) (result UpdateEndpointMapResult) { result.StaleEndpoints = make([]ServiceEndpoint, 0) result.StaleServiceNames = make([]ServicePortName, 0) endpointsMap.apply(changes, &result.StaleEndpoints, &result.StaleServiceNames) // TODO: If this will appear to be computationally expensive, consider // computing this incrementally similarly to endpointsMap. result.HCEndpointsLocalIPSize = make(map[types.NamespacedName]int) localIPs := GetLocalEndpointIPs(endpointsMap) for nsn, ips := range localIPs { result.HCEndpointsLocalIPSize[nsn] = len(ips) } return result } EndpointsMap.apply() 应用更新变化事件的端点项(merge->unmerge) pkg/proxy/endpoints.go:242 func (endpointsMap EndpointsMap) apply(changes *EndpointChangeTracker, staleEndpoints *[]ServiceEndpoint, staleServiceNames *[]ServicePortName) { if changes == nil { return } changes.lock.Lock() defer changes.lock.Unlock() for _, change := range changes.items { endpointsMap.Unmerge(change.previous) // 删除 ① endpointsMap.Merge(change.current) // 更新 ② detectStaleConnections(change.previous, change.current, staleEndpoints, staleServiceNames) //废弃查找 ③ } changes.items = make(map[types.NamespacedName]*endpointsChange) } ① EndpointsMap.Merge() 将\"other\"内指定项的值(端点列表)更新至EndpointMap pkg/proxy/endpoints.go:259 func (em EndpointsMap) Merge(other EndpointsMap) { for svcPortName := range other { em[svcPortName] = other[svcPortName] } } ② EndpointsMap.Unmerge() 删除\"other\"内指定项 pkg/proxy/endpoints.go:266 func (em EndpointsMap) Unmerge(other EndpointsMap) { for svcPortName := range other { delete(em, svcPortName) } } ③ EndpointsMap.detectStaleConnections() 查找废弃后端连接信息项 pkg/proxy/endpoints.go:291 func detectStaleConnections(oldEndpointsMap, newEndpointsMap EndpointsMap, staleEndpoints *[]ServiceEndpoint, staleServiceNames *[]ServicePortName) { for svcPortName, epList := range oldEndpointsMap { for _, ep := range epList { stale := true for i := range newEndpointsMap[svcPortName] { if newEndpointsMap[svcPortName][i].Equal(ep) { //存在则stale为否 stale = false break } } if stale { klog.V(4).Infof(\"Stale endpoint %v -> %v\", svcPortName, ep.String()) *staleEndpoints = append(*staleEndpoints, ServiceEndpoint{Endpoint: ep.String(), ServicePortName: svcPortName}) //存储废弃的endpoint列表 } } } for svcPortName, epList := range newEndpointsMap { //对于UDP服务，如果后端变化从0至非0，可能存在conntrack项将服务的流量黑洞 if len(epList) > 0 && len(oldEndpointsMap[svcPortName]) == 0 { *staleServiceNames = append(*staleServiceNames, svcPortName) //存储废弃的服务名列表 } } } 5.2. 创建与联接 kube 链 filter表中INPUT链头部插入自定义链调转到KUBE-EXTERNAL-SERVICES链 iptables -I \"INPUT\" -t \"filter\" -m \"conntrack\" --ctstate \"NEW\" -m comment --comment \"kubernetes externally-visible service portals\" -j \"KUBE-EXTERNAL-SERVICES\" filter表中OUTPUT链头部插入自定义链调转到KUBE-SERVICE链 iptables -I \"OUTPUT\" -t \"filter\" -m \"conntrack\" --ctstate \"NEW\" -m comment --comment \"kubernetes service portals\" -j \"KUBE-SERVICES\" nat表中OUTPUT链头部插入自定义链调转到KUBE-SERVICES链 iptables -I \"OUTPUT\" -t \"nat\" -m comment --comment \"kubernetes service portals\" -j \"KUBE-SERVICES\" nat表中PREROUTING链头部插入自定义链调转到KUBE-SERVICES链 iptables -I \"PREROUTING\" -t \"nat\" -m comment --comment \"kubernetes service portals\" -j \"KUBE-SERVICES\" nat表中POSTROUTING链头部插入自定义链调转到KUBE-POSTROUTING链 iptables -I \"POSTROUTING\" -t \"nat\" -m comment --comment \"kubernetes postrouting rules\" -j \"KUBE-POSTROUTING\" filter表中FORWARD链头部插入自定义链调转到KUBE-FORWARD链 iptables -I \"FORWARD\" -t \"filter\" -m comment --comment \"kubernetes forwarding rules\" -j \"KUBE-FORWARD\" pkg/proxy/iptables/proxier.go:667 //循环iptablesJumpChains定义 for _, chain := range iptablesJumpChains { //底层命令iptables -t $tableName -N $chainName if _, err := proxier.iptables.EnsureChain(chain.table, chain.chain); err != nil { klog.Errorf(\"Failed to ensure that %s chain %s exists: %v\", chain.table, kubeServicesChain, err) return } args := append(chain.extraArgs, \"-m\", \"comment\", \"--comment\", chain.comment, \"-j\", string(chain.chain), ) //底层命令iptables -I $chainName -t $tableName -m comment --comment $comment -j $chain if _, err := proxier.iptables.EnsureRule(utiliptables.Prepend, chain.table, chain.sourceChain, args...); err != nil { klog.Errorf(\"Failed to ensure that %s chain %s jumps to %s: %v\", chain.table, chain.sourceChain, chain.chain, err) return } } 5.3. 创建 Iptables 基础数据 获取现存在的Filter/Nat表链数据 创建iptables-save/restore格式数据（表头、链） 创建SNAT地址伪装规则 pkg/proxy/iptables/proxier.go:688 //现存在的filter表链获取 existingFilterChains := make(map[utiliptables.Chain][]byte) proxier.existingFilterChainsData.Reset() err := proxier.iptables.SaveInto(utiliptables.TableFilter, proxier.existingFilterChainsData) //通过iptables-save方式来获取 if err != nil { klog.Errorf(\"Failed to execute iptables-save, syncing all rules: %v\", err) } else { existingFilterChains = utiliptables.GetChainLines(utiliptables.TableFilter, proxier.existingFilterChainsData.Bytes()) //输出结果 } //同上，现存在的nat表链获取 existingNATChains := make(map[utiliptables.Chain][]byte) proxier.iptablesData.Reset() err = proxier.iptables.SaveInto(utiliptables.TableNAT, proxier.iptablesData) if err != nil { klog.Errorf(\"Failed to execute iptables-save, syncing all rules: %v\", err) } else { existingNATChains = utiliptables.GetChainLines(utiliptables.TableNAT, proxier.iptablesData.Bytes()) } // Reset all buffers used later. // This is to avoid memory reallocations and thus improve performance. proxier.filterChains.Reset() proxier.filterRules.Reset() proxier.natChains.Reset() proxier.natRules.Reset() // 写表头 writeLine(proxier.filterChains, \"*filter\") writeLine(proxier.natChains, \"*nat\") 写链数据 fileter: \"KUBE-SERVICES\" / \"KUBE-EXTERNAL-SERVICES\"/ \"KUBE-FORWARD\" nat: \"KUBE-SERVICES\" / \"KUBE-NODEPORTS\" / \"KUBE-POSTROUTING\" / \"KUBE-MARK-MASQ\" pkg/proxy/iptables/proxier.go:720 // 写chain链数据,将filter和Nat相关链格式化存放buffer for _, chainName := range []utiliptables.Chain{kubeServicesChain, kubeExternalServicesChain, kubeForwardChain} { if chain, ok := existingFilterChains[chainName]; ok { writeBytesLine(proxier.filterChains, chain) } else { // iptables-save/restore格式的链行\":$chainName - [0:0]\" writeLine(proxier.filterChains, utiliptables.MakeChainLine(chainName)) } } for _, chainName := range []utiliptables.Chain{kubeServicesChain, kubeNodePortsChain, kubePostroutingChain, KubeMarkMasqChain} { if chain, ok := existingNATChains[chainName]; ok { writeBytesLine(proxier.natChains, chain) } else { writeLine(proxier.natChains, utiliptables.MakeChainLine(chainName)) } } 写地址伪装规则，在POSTROUTING阶段对地址进行MASQUERADE（基于接口动态IP的SNAT）处理，原始请求源IP将被丢失，被请求POD的应用看到为NodeIP或CNI设备IP(bridge/vxlan设备) pkg/proxy/iptables/proxier.go:738 // 写kubernets特有的SNAT地址伪装规则 // -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE writeLine(proxier.natRules, []string{ \"-A\", string(kubePostroutingChain), \"-m\", \"comment\", \"--comment\", `\"kubernetes service traffic requiring SNAT\"`, \"-m\", \"mark\", \"--mark\", proxier.masqueradeMark, \"-j\", \"MASQUERADE\", }...) //-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 writeLine(proxier.natRules, []string{ \"-A\", string(KubeMarkMasqChain), \"-j\", \"MARK\", \"--set-xmark\", proxier.masqueradeMark, }...) 5.4. 为每个 service 创建 rules 先了解serviceInfo的完整定义说明 pkg/proxy/iptables/proxier.go:141 type serviceInfo struct { *proxy.BaseServiceInfo // The following fields are computed and stored for performance reasons. serviceNameString string servicePortChainName utiliptables.Chain // KUBE-SVC-XXXX16BitXXXX 服务链 serviceFirewallChainName utiliptables.Chain // KUBE-FW-XXXX16BitXXXX Firewall链 serviceLBChainName utiliptables.Chain // KUBE-XLB-XXXX16BitXXXX SLB链 } type BaseServiceInfo struct { ClusterIP net.IP //PortalIP(VIP) Port int //portal端口 Protocol v1.Protocol //协议 NodePort int //node节点端口 LoadBalancerStatus v1.LoadBalancerStatus //LB Ingress SessionAffinityType v1.ServiceAffinity //会话保持 StickyMaxAgeSeconds int //保持最大时长 ExternalIPs []string //ExternalIPs（指定的node上监听端口） LoadBalancerSourceRanges []string //过滤源地址流量 HealthCheckNodePort int //HealthCheck检测端口 OnlyNodeLocalEndpoints bool } 为每个服务创建服务\"KUBE-SVC-XXX…\"和外部负载均衡\"KUBE-XLB-XXX…\"链 pkg/proxy/iptables/proxier.go:791 svcChain := svcInfo.servicePortChainName //\"KUBE-SVC-XXX...\" if hasEndpoints { // Create the per-service chain, retaining counters if possible. if chain, ok := existingNATChains[svcChain]; ok { writeBytesLine(proxier.natChains, chain) } else { writeLine(proxier.natChains, utiliptables.MakeChainLine(svcChain)) } activeNATChains[svcChain] = true } svcXlbChain := svcInfo.serviceLBChainName // \"KUBE-XLB-XXX…\" if svcInfo.OnlyNodeLocalEndpoints { // Only for services request OnlyLocal traffic // create the per-service LB chain, retaining counters if possible. if lbChain, ok := existingNATChains[svcXlbChain]; ok { writeBytesLine(proxier.natChains, lbChain) } else { writeLine(proxier.natChains, utiliptables.MakeChainLine(svcXlbChain)) } activeNATChains[svcXlbChain] = true } clusterIP流量的匹配，clusterIP为默认方式，仅资源集群内可访问。 pkg/proxy/iptables/proxier.go:815 //存在端点，写规则 if hasEndpoints { args = append(args[:0], \"-A\", string(kubeServicesChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"%s cluster IP\"`, svcNameString), \"-m\", protocol, \"-p\", protocol, \"-d\", utilproxy.ToCIDR(svcInfo.ClusterIP), \"--dport\", strconv.Itoa(svcInfo.Port), ) // proxier配置masqueradeAll // -A KUBE-SERVICES -m comment --comment \"...\" -m $prot -p $prot -d $clusterIP \\ // --dport $port -j KUBE-MARK-MASQ if proxier.masqueradeAll { writeLine(proxier.natRules, append(args, \"-j\", string(KubeMarkMasqChain))...) } else if len(proxier.clusterCIDR) > 0 { // proxier配置clusterCIDR情况： // -A KUBE-SERVICES ! -s $clusterCIDR -m comment --comment \"...\" -m $prot \\ // -p $prot -d $clusterIP --dport $port -j KUBE-MARK-MASQ writeLine(proxier.natRules, append(args, \"! -s\", proxier.clusterCIDR, \"-j\", string(KubeMarkMasqChain))...) } // -A KUBE-SERVICES -m comment --comment \"...\" -m $prot -p $prot -d $clusterIP \\ // --dport $port -j KUBE-SVC-XXXX16bitXXXX writeLine(proxier.natRules, append(args, \"-j\", string(svcChain))...) } else { // 无Endpoints的情况，则创建REJECT规则 // -A KUBE-SERVICES -m comment --comment $svcName -m $prot -p $prot -d $clusterIP \\ // --dport $port -j REJECT writeLine(proxier.filterRules, \"-A\", string(kubeServicesChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"%s has no endpoints\"`, svcNameString), \"-m\", protocol, \"-p\", protocol, \"-d\", utilproxy.ToCIDR(svcInfo.ClusterIP), \"--dport\", strconv.Itoa(svcInfo.Port), \"-j\", \"REJECT\", ) } 服务是否启用ExternalIPs(指定的node上开启监听端口) pkg/proxy/iptables/proxier.go:846 for _, externalIP := range svcInfo.ExternalIPs { // 判断externalIP是否为本node的IP以及协议为SCTP，且端口是否已开启 // 如果未开启则在本地打开监听端口 if local, err := utilproxy.IsLocalIP(externalIP); err != nil { klog.Errorf(\"can't determine if IP is local, assuming not: %v\", err) } else if local && (svcInfo.GetProtocol() != v1.ProtocolSCTP) { lp := utilproxy.LocalPort{ Description: \"externalIP for \" + svcNameString, IP: externalIP, Port: svcInfo.Port, Protocol: protocol, } if proxier.portsMap[lp] != nil { klog.V(4).Infof(\"Port %s was open before and is still needed\", lp.String()) replacementPortsMap[lp] = proxier.portsMap[lp] } else { //打开与监听本地端口 socket, err := proxier.portMapper.OpenLocalPort(&lp) if err != nil { msg := fmt.Sprintf(\"can't open %s, skipping this externalIP: %v\", lp.String(), err) proxier.recorder.Eventf( &v1.ObjectReference{ Kind: \"Node\", Name: proxier.hostname, UID: types.UID(proxier.hostname), Namespace: \"\", }, v1.EventTypeWarning, err.Error(), msg) klog.Error(msg) continue } replacementPortsMap[lp] = socket } } //存在端点，写规则 if hasEndpoints { args = append(args[:0], \"-A\", string(kubeServicesChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"%s external IP\"`, svcNameString), \"-m\", protocol, \"-p\", protocol, \"-d\", utilproxy.ToCIDR(net.ParseIP(externalIP)), \"--dport\", strconv.Itoa(svcInfo.Port), ) // -A KUBE-EXTERNAL-SERVICES -m comment --comment \"...\" -m $prot -p $prot -d \\ // $externalIP --dport $port -j KUBE-MARK-MASQ writeLine(proxier.natRules, append(args, \"-j\", string(KubeMarkMasqChain))...) // -A KUBE-EXTERNAL-SERVICES -m comment --comment \"...\" -m $prot -p $prot -d \\ // $externalIP --dport $port -m physdev ! --physdev-is-in \\ // -m addrtype ! --src-type Local -j KUBE-SVC-XXXX16bitXXXXX externalTrafficOnlyArgs := append(args, \"-m\", \"physdev\", \"!\", \"--physdev-is-in\", \"-m\", \"addrtype\", \"!\", \"--src-type\", \"LOCAL\") writeLine(proxier.natRules, append(externalTrafficOnlyArgs, \"-j\", string(svcChain))...) dstLocalOnlyArgs := append(args, \"-m\", \"addrtype\", \"--dst-type\", \"LOCAL\") // -A KUBE-EXTERNAL-SERVICES -m comment --comment \"...\" -m $prot -p $prot -d \\ // $externalIP --dport $port -m addrtype --dst-type Local // -j KUBE-SVC-XXXX16bitXXXXX writeLine(proxier.natRules, append(dstLocalOnlyArgs, \"-j\", string(svcChain))...) } else { // 不存在端点信息则reject // -A KUBE-EXTERNAL-SERVICES -m comment --comment \"...\" -m $prot -p $prot -d \\ // $externalIP --dport $port -j REJECT writeLine(proxier.filterRules, \"-A\", string(kubeExternalServicesChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"%s has no endpoints\"`, svcNameString), \"-m\", protocol, \"-p\", protocol, \"-d\", utilproxy.ToCIDR(net.ParseIP(externalIP)), \"--dport\", strconv.Itoa(svcInfo.Port), \"-j\", \"REJECT\", ) } } 服务是否启用了外部负载均衡服务load-balancer ingress pkg/proxy/iptables/proxier.go:917 //存在端点，写规则 if hasEndpoints { fwChain := svcInfo.serviceFirewallChainName //\"KUBE-FW-XXXX16bitXXXXX\" for _, ingress := range svcInfo.LoadBalancerStatus.Ingress { if ingress.IP != \"\" { // 创建服务KUBE-FW-X链 if chain, ok := existingNATChains[fwChain]; ok { writeBytesLine(proxier.natChains, chain) } else { //原来不存在则新建 writeLine(proxier.natChains, utiliptables.MakeChainLine(fwChain)) } activeNATChains[fwChain] = true // The service firewall rules are created based on ServiceSpec.loadBalancerSourceRanges field. // This currently works for loadbalancers that preserves source ips. // For loadbalancers which direct traffic to service NodePort, the firewall rules will not apply. args = append(args[:0], \"-A\", string(kubeServicesChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"%s loadbalancer IP\"`, svcNameString), \"-m\", protocol, \"-p\", protocol, \"-d\", utilproxy.ToCIDR(net.ParseIP(ingress.IP)), \"--dport\", strconv.Itoa(svcInfo.Port), ) // -A KUBE-SERVICES -m comment --comment \"...\" -m $prot -p $prot -d \\ // $ingresIP --dport $port -j KUBE-FW-XXXX16bitXXXXX writeLine(proxier.natRules, append(args, \"-j\", string(fwChain))...) args = append(args[:0], \"-A\", string(fwChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"%s loadbalancer IP\"`, svcNameString), ) // 在KUBE-FW链，每个源匹配规则可能跳转至一个SVC或XLB链 chosenChain := svcXlbChain // If we are proxying globally, we need to masquerade in case we cross nodes. // If we are proxying only locally, we can retain the source IP. if !svcInfo.OnlyNodeLocalEndpoints { // -j \"KUBE-MARK-MASQ\" 地址伪装实现跨主机访问 writeLine(proxier.natRules, append(args, \"-j\", string(KubeMarkMasqChain))...) chosenChain = svcChain // 选择为SVC链 } if len(svcInfo.LoadBalancerSourceRanges) == 0 { // 允许所有源，直接跳转 writeLine(proxier.natRules, append(args, \"-j\", string(chosenChain))...) } else { // 基于source range配置过滤 \"-s $srcRanges\" allowFromNode := false for _, src := range svcInfo.LoadBalancerSourceRanges { writeLine(proxier.natRules, append(args, \"-s\", src, \"-j\", string(chosenChain))...) _, cidr, _ := net.ParseCIDR(src) if cidr.Contains(proxier.nodeIP) { allowFromNode = true //配置CIDR包含节点IP，则允许来自节点请求 } } // 添加 \"-s $ingresIP\" 来允许LB后端主机请求 if allowFromNode { writeLine(proxier.natRules, append(args, \"-s\", utilproxy.ToCIDR(net.ParseIP(ingress.IP)), \"-j\", string(chosenChain))...) } } // 条件ingress.IP为空\"-j KUBE-MARK-DROP\" writeLine(proxier.natRules, append(args, \"-j\", string(KubeMarkDropChain))...) } } } 服务是否启用了nodeport(在每个节点上都将开启一个nodeport端口) pkg/proxy/iptables/proxier.go:989 if svcInfo.NodePort != 0 { // 获取node addresses addresses, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer) if err != nil { klog.Errorf(\"Failed to get node ip address matching nodeport cidr: %v\", err) continue } lps := make([]utilproxy.LocalPort, 0) for address := range addresses { lp := utilproxy.LocalPort{ Description: \"nodePort for \" + svcNameString, IP: address, Port: svcInfo.NodePort, Protocol: protocol, } if utilproxy.IsZeroCIDR(address) { // Empty IP address means all lp.IP = \"\" lps = append(lps, lp) // If we encounter a zero CIDR, then there is no point in processing the rest of the addresses. break } lps = append(lps, lp) //IP列表 } // 为node节点的ips打开端口并保存持有socket句柄 for _, lp := range lps { if proxier.portsMap[lp] != nil { klog.V(4).Infof(\"Port %s was open before and is still needed\", lp.String()) replacementPortsMap[lp] = proxier.portsMap[lp] } else if svcInfo.GetProtocol() != v1.ProtocolSCTP { // 打开和监听端口 socket, err := proxier.portMapper.OpenLocalPort(&lp) if err != nil { klog.Errorf(\"can't open %s, skipping this nodePort: %v\", lp.String(), err) continue } if lp.Protocol == \"udp\" { //清理udp conntrack记录 err := conntrack.ClearEntriesForPort(proxier.exec, lp.Port, isIPv6, v1.ProtocolUDP) if err != nil { klog.Errorf(\"Failed to clear udp conntrack for port %d, error: %v\", lp.Port, err) } } replacementPortsMap[lp] = socket //socket保存 } } //存在端点，写规则 if hasEndpoints { // -A KUBE-NODEPORTS -m comment --comment \"...\" -m $prot -p $prot --dport $nodePort args = append(args[:0], \"-A\", string(kubeNodePortsChain), \"-m\", \"comment\", \"--comment\", svcNameString, \"-m\", protocol, \"-p\", protocol, \"--dport\", strconv.Itoa(svcInfo.NodePort), ) if !svcInfo.OnlyNodeLocalEndpoints { //非本地nodeports则需SNAT规则添加, // -j KUBE-MARK-MASQ -j KUBE-XLB-XXXX16bitXXXX writeLine(proxier.natRules, append(args, \"-j\", string(KubeMarkMasqChain))...) // Jump to the service chain. writeLine(proxier.natRules, append(args, \"-j\", string(svcChain))...) } else { loopback := \"127.0.0.0/8\" if isIPv6 { loopback = \"::1/128\" } // 本地nodeports则规则添加, // -s $loopback -j KUBE-MARK-MASQ -j KUBE-XLB-XXXX16bitXXXX writeLine(proxier.natRules, append(args, \"-s\", loopback, \"-j\", string(KubeMarkMasqChain))...) writeLine(proxier.natRules, append(args, \"-j\", string(svcXlbChain))...) } } else { // 无hasEndpoints，添加-j reject规则 // -A KUBE-EXTERNAL-SERVICES -m comment --comment \"...\" -m addrtype \\ // --dst-type LOCAL -m $prot -p $prot --dport $nodePort -j REJECT writeLine(proxier.filterRules, \"-A\", string(kubeExternalServicesChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"%s has no endpoints\"`, svcNameString), \"-m\", \"addrtype\", \"--dst-type\", \"LOCAL\", \"-m\", protocol, \"-p\", protocol, \"--dport\", strconv.Itoa(svcInfo.NodePort), \"-j\", \"REJECT\", ) } } 基于服务名和协议，生成每个端点链 pkg/proxy/iptables/proxier.go:1087 for _, ep := range proxier.endpointsMap[svcName] { epInfo, ok := ep.(*endpointsInfo) if !ok { klog.Errorf(\"Failed to cast endpointsInfo %q\", ep.String()) continue } endpoints = append(endpoints, epInfo) //基于服务名和协议生成端点链名称 \"KUBE-SEP-XXXX16bitXXXX\" endpointChain = epInfo.endpointChain(svcNameString, protocol) endpointChains = append(endpointChains, endpointChain) // 创建端点链 if chain, ok := existingNATChains[utiliptables.Chain(endpointChain)]; ok { writeBytesLine(proxier.natChains, chain) } else { writeLine(proxier.natChains, utiliptables.MakeChainLine(endpointChain)) } activeNATChains[endpointChain] = true } 写SessionAffinity会话保持规则，实现在一段时间内保持session affinity，保持时间为180秒，通过添加“-m recent –rcheck –seconds 180 –reap”的iptables规则实现了会话保持。 pkg/proxy/iptables/proxier.go:1107 //SessionAffinityType设置为\"ClientIP\"，则写session保持规则 // -A KUBE-SVC-XXXX16bitXXXX -m recent -m comment –comment \"...\" \\ // --name KUBE-SEP-XXXX16bitXXXX --rcheck --seconds 180 --reap \\ // -j KUBE-SEP-XXXX16bitXXXX if svcInfo.SessionAffinityType == v1.ServiceAffinityClientIP { for _, endpointChain := range endpointChains { args = append(args[:0], \"-A\", string(svcChain), ) proxier.appendServiceCommentLocked(args, svcNameString) args = append(args, \"-m\", \"recent\", \"--name\", string(endpointChain), \"--rcheck\", \"--seconds\", strconv.Itoa(svcInfo.StickyMaxAgeSeconds), \"--reap\", \"-j\", string(endpointChain), ) writeLine(proxier.natRules, args...) } } 写负载均衡和DNAT规则，使用“-m statistic –-mode random -–probability ” iptables规则将后端POD组成一个基于概率访问的组合,实现服务访问的负载均衡功能效果。 针对服务的每个端点在nat表内该service对应的自定义链“KUBE-SVC-XXXX16bitXXXX”中加入iptables规则。如果该服务对应的endpoints大于等于2，则添加负载均衡规则。 针对选择非本地Node上的POD，需进行DNAT，将请求的目标地址设置成后选的POD的IP后进行路由。KUBE-MARK-MASQ将重设(伪装)源地址 -A KUBE-SVC-XXXX16bitXXXX -m comment –comment \"...\" -m statistic --mode random --probability $prob -j KUBE-SEP-XXXX16bitXXXX -A KUBE-SEP-XXXX16bitXXXX -m comment –comment \"...\" -s $epIp -j \"KUBE-MARK-MASQ\" -A KUBE-SVC-XXXX16bitXXXX -m comment –comment \"…\" -m prot -p $prot -j DNAT --to-destination X.X.X.X:xxx pkg/proxy/iptables/proxier.go:1123 // 写负载均衡和DNAT规则 n := len(endpointChains) for i, endpointChain := range endpointChains { epIP := endpoints[i].IP() if epIP == \"\" { // Error parsing this endpoint has been logged. Skip to next endpoint. continue } // 每个服务生成的负载均衡规则，后端POD组成一个基于概率访问的组合 // // -A KUBE-SVC-XXXX16bitXXXX -m comment –comment \"...\" // -m statistic --mode random --probability $prob // -j KUBE-SEP-XXXX16bitXXXX args = append(args[:0], \"-A\", string(svcChain)) proxier.appendServiceCommentLocked(args, svcNameString) if i 启用clusterCIDR (Kube-proxy中的--cluster-dir指定的是集群中pod使用的网段，而pod使用的网段和apiserver中指定的service的cluster ip或vip网段不是同一个网段) pkg/proxy/iptables/proxier.go:1179 // pod -> external VIP流量导向服务VIP(服务链) // -A KUBE-XLB-XXXX16bitXXXX -m comment --comment \"...\" -s $clusterCIDR // -j KUBE-SVC-XXXX16bitXXXXX if len(proxier.clusterCIDR) > 0 { args = append(args[:0], \"-A\", string(svcXlbChain), \"-m\", \"comment\", \"--comment\", `\"Redirect pods trying to reach external loadbalancer VIP to clusterIP\"`, \"-s\", proxier.clusterCIDR, \"-j\", string(svcChain), ) writeLine(proxier.natRules, args...) } 生成本地端点链规则，本地源IP保持(当只在本地选择POD服务请求时，则不存在SNAT规则，可保持源地址IP信息。在nodePort或XLB时，可定义\"externalTrafficPolicy\": \"Local\"控制向属于这个service的本地的POD转发请求，如果本地没有POD能服务这个请求，请求将被DROP掉，客户端会发现请求超时没有响应。 pkg/proxy/iptables/proxier.go:1190 numLocalEndpoints := len(localEndpointChains) if numLocalEndpoints == 0 { // 无本地端点，将流量Drop(流量黑洞处理) // -A KUBE-XLB-XXXX16bitXXXX -m comment --comment \"...\" -j KUBE-MARK-DROP args = append(args[:0], \"-A\", string(svcXlbChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"%s has no local endpoints\"`, svcNameString), \"-j\", string(KubeMarkDropChain), ) writeLine(proxier.natRules, args...) } else { // 本地端点会话保持开启 // -A KUBE-XLB-XXXX16bitXXXX -m comment --comment \"...\" -m recent \\ // --name KUBE-SEP-XXXX16bitXXXX \\ // --rcheck --seconds $StickyMaxAge --reap -j KUBE-SEP-XXXX16bitXXXX if svcInfo.SessionAffinityType == v1.ServiceAffinityClientIP { for _, endpointChain := range localEndpointChains { writeLine(proxier.natRules, \"-A\", string(svcXlbChain), \"-m\", \"comment\", \"--comment\", svcNameString, \"-m\", \"recent\", \"--name\", string(endpointChain), \"--rcheck\", \"--seconds\", strconv.Itoa(svcInfo.StickyMaxAgeSeconds), \"--reap\", \"-j\", string(endpointChain)) } } // 本地端点负载均衡处理\"-m statistic --mode random --probability\" // 后端POD组成一个基于概率访问的组合 for i, endpointChain := range localEndpointChains { // Balancing rules in the per-service chain. args = append(args[:0], \"-A\", string(svcXlbChain), \"-m\", \"comment\", \"--comment\", fmt.Sprintf(`\"Balancing rule %d for %s\"`, i, svcNameString), ) if i 5.5. 配置收尾规则数据 删除不再使用的服务自定义kube链\"KUBE-SVC-*\"/\"KUBE-SEP-*\"/\"KUBE-FW-*\"/\"KUBE-XLB-*\"。 pkg/proxy/iptables/proxier.go:1237 for chain := range existingNATChains { if !activeNATChains[chain] { chainString := string(chain) if !strings.HasPrefix(chainString, \"KUBE-SVC-\") && !strings.HasPrefix(chainString, \"KUBE-SEP-\") && !strings.HasPrefix(chainString, \"KUBE-FW-\") && !strings.HasPrefix(chainString, \"KUBE-XLB-\") { // Ignore chains that aren't ours. continue } // We must (as per iptables) write a chain-line for it, which has // the nice effect of flushing the chain. Then we can remove the // chain. writeBytesLine(proxier.natChains, existingNATChains[chain]) writeLine(proxier.natRules, \"-X\", chainString) } } 添加服务的nodeports规则(nat表-\"KUBE-SERVICES\"链) pkg/proxy/iptables/proxier.go:1254 // -A KUBE-SERVICES -m comment --comment \"...\" -m addrtype --dst-type LOCAL \\ // -j KUBE-NODEPORTS // // -A KUBE-SERVICES -m comment --comment \"...\" -d $NODEIP -j KUBE-NODEPORTS // addresses, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer) if err != nil { klog.Errorf(\"Failed to get node ip address matching nodeport cidr\") } else { isIPv6 := proxier.iptables.IsIpv6() for address := range addresses { // TODO(thockin, m1093782566): If/when we have dual-stack support we will want to distinguish v4 from v6 zero-CIDRs. if utilproxy.IsZeroCIDR(address) { args = append(args[:0], \"-A\", string(kubeServicesChain), \"-m\", \"comment\", \"--comment\", `\"kubernetes service nodeports; NOTE: this must be the last rule in this chain\"`, \"-m\", \"addrtype\", \"--dst-type\", \"LOCAL\", \"-j\", string(kubeNodePortsChain)) writeLine(proxier.natRules, args...) // Nothing else matters after the zero CIDR. break } // Ignore IP addresses with incorrect version if isIPv6 && !utilnet.IsIPv6String(address) || !isIPv6 && utilnet.IsIPv6String(address) { klog.Errorf(\"IP address %s has incorrect IP version\", address) continue } // create nodeport rules for each IP one by one args = append(args[:0], \"-A\", string(kubeServicesChain), \"-m\", \"comment\", \"--comment\", `\"kubernetes service nodeports; NOTE: this must be the last rule in this chain\"`, \"-d\", address, \"-j\", string(kubeNodePortsChain)) writeLine(proxier.natRules, args...) } } 添加forward规则(filter表-\"KUBE-FORWARD\"链) pkg/proxy/iptables/proxier.go:1289 //-A KUBE-FORWARD -m comment --comment \"...\" -m mark --mark 0xFFFF/0xFFFF -j ACCEPT writeLine(proxier.filterRules, \"-A\", string(kubeForwardChain), \"-m\", \"comment\", \"--comment\", `\"kubernetes forwarding rules\"`, \"-m\", \"mark\", \"--mark\", proxier.masqueradeMark, \"-j\", \"ACCEPT\", ) 添加带clusterCIDR配置(源/目标)规则(filter表-\"KUBE-FORWARD\"链) pkg/proxy/iptables/proxier.go:1297 //Kube-proxy中的cluster-dir指定的是集群中pod使用的网段 //pod使用的网段和service的cluster ip网段不是同一个网段 // // -A KUBE-FORWARD -s $clusterCIDR -m comment --comment \"...\" -m conntrack --ctstate \\ // RELATED,ESTABLISHED -j ACCEPT // -A KUBE-FORWARD -m comment --comment \"...\" -d $clusterCIDR -m conntrack --ctstate \\ // RELATED,ESTABLISHED -j ACCEPT // if len(proxier.clusterCIDR) != 0 { writeLine(proxier.filterRules, \"-A\", string(kubeForwardChain), \"-s\", proxier.clusterCIDR, //指定源 \"-m\", \"comment\", \"--comment\", `\"kubernetes forwarding conntrack pod source rule\"`, \"-m\", \"conntrack\", \"--ctstate\", \"RELATED,ESTABLISHED\", \"-j\", \"ACCEPT\", ) writeLine(proxier.filterRules, \"-A\", string(kubeForwardChain), \"-m\", \"comment\", \"--comment\", `\"kubernetes forwarding conntrack pod destination rule\"`, \"-d\", proxier.clusterCIDR, //指定目标 \"-m\", \"conntrack\", \"--ctstate\", \"RELATED,ESTABLISHED\", \"-j\", \"ACCEPT\", ) } 结尾标志写入 writeLine(proxier.filterRules, \"COMMIT\") writeLine(proxier.natRules, \"COMMIT\") 5.6. 汇集与加载 iptables 配置规则数据 pkg/proxy/iptables/proxier.go:1326 //汇集前面所处理的filter和nat表数据至iptablesData proxier.iptablesData.Reset() proxier.iptablesData.Write(proxier.filterChains.Bytes()) proxier.iptablesData.Write(proxier.filterRules.Bytes()) proxier.iptablesData.Write(proxier.natChains.Bytes()) proxier.iptablesData.Write(proxier.natRules.Bytes()) klog.V(5).Infof(\"Restoring iptables rules: %s\", proxier.iptablesData.Bytes()) // iptables-restore加载新配置(iptablesData) err = proxier.iptables.RestoreAll(proxier.iptablesData.Bytes(), utiliptables.NoFlushTables, utiliptables.RestoreCounters) if err != nil { klog.Errorf(\"Failed to execute iptables-restore: %v\", err) // Revert new local ports. klog.V(2).Infof(\"Closing local ports after iptables-restore failure\") utilproxy.RevertPorts(replacementPortsMap, proxier.portsMap) return } 6. IPtables 底层的 runner 实现 前面基本已看完整个proxy的执行流程，最后iptables proxier是如何使用系统层iptables命令进行底层的iptables规则CRUD操作（通俗的理解：iptables proxier实现都是在操作iptables命令生成相应的规则），下面我来看一下kuber-proxy组件底层iptables操作器的封装。 6.1. iptables 执行器 Interface**接口为运行iptables命令定义 pkg/util/iptables/iptables.go:45 //接口与接口方法定义 type Interface interface { GetVersion() (string, error) EnsureChain(table Table, chain Chain) (bool, error) FlushChain(table Table, chain Chain) error DeleteChain(table Table, chain Chain) error EnsureRule(position RulePosition, table Table, chain Chain, args ...string) (bool, error) DeleteRule(table Table, chain Chain, args ...string) error IsIpv6() bool SaveInto(table Table, buffer *bytes.Buffer) error Restore(table Table, data []byte, flush FlushFlag, counters RestoreCountersFlag) error RestoreAll(data []byte, flush FlushFlag, counters RestoreCountersFlag) error AddReloadFunc(reloadFunc func()) Destroy() } iptables Interface接实现类runner,完成对iptables命令执行器的定义 pkg/util/iptables/iptables.go:135 // 类结构定义 type runner struct { mu sync.Mutex //同步锁 exec utilexec.Interface //osExec命令执行 dbus utildbus.Interface //D-Bus操作API接口 protocol Protocol //协议IPv4/IPv6 hasCheck bool //\"-C\"检测命令flag hasListener bool //D-Bus信号监听是否开启(FirewallD start/restart) waitFlag []string //iptables命令\"wait\"flag,等待xtables锁 restoreWaitFlag []string //iptables-restore命令\"wait\"flag lockfilePath string //xtables锁文件位置 reloadFuncs []func() //定义reload处理func signal chan *godbus.Signal //dbus信号 } // Runner实现Iterface方法列表，后面将详细分析关键的方法实现代码逻辑 func (runner *runner) GetVersion() (string, error) func (runner *runner) EnsureChain(table Table, chain Chain) (bool, error) func (runner *runner) FlushChain(table Table, chain Chain) error func (runner *runner) DeleteChain(table Table, chain Chain) error func (runner *runner) EnsureRule(position RulePosition, table Table, chain Chain, args ...string) (bool, error) func (runner *runner) DeleteRule(table Table, chain Chain, args ...string) error func (runner *runner) IsIpv6() bool func (runner *runner) SaveInto(table Table, buffer *bytes.Buffer) error func (runner *runner) Restore(table Table, data []byte, flush FlushFlag, counters RestoreCountersFlag) error func (runner *runner) RestoreAll(data []byte, flush FlushFlag, counters RestoreCountersFlag) error func (runner *runner) AddReloadFunc(reloadFunc func()) func (runner *runner) Destroy() // Runner内部方法列表 func (runner *runner) connectToFirewallD() func (runner *runner) restoreInternal(args []string, data []byte, flush FlushFlag, counters RestoreCountersFlag) error func (runner *runner) run(op operation, args []string) ([]byte, error) func (runner *runner) runContext(ctx context.Context, op operation, args []string) ([]byte, error) func (runner *runner) checkRule(table Table, chain Chain, args ...string) (bool, error) func (runner *runner) checkRuleWithoutCheck(table Table, chain Chain, args ...string) (bool, error) func (runner *runner) checkRuleUsingCheck(args []string) (bool, error) func (runner *runner) dbusSignalHandler(bus utildbus.Connection) func (runner *runner) reload() iptables runner对象的构造New() -> newInternal(),返回runner{…}实例化对象(Interface接口类型) ，完成了创建一个iptables的命令执行器生成工作。 pkg/util/iptables/iptables.go:152 func newInternal(exec utilexec.Interface, dbus utildbus.Interface, protocol Protocol, lockfilePath string) Interface { vstring, err := getIPTablesVersionString(exec, protocol) //iptables版本获取 if err != nil { klog.Warningf(\"Error checking iptables version, assuming version at least %s: %v\", MinCheckVersion, err) vstring = MinCheckVersion } if lockfilePath == \"\" { lockfilePath = LockfilePath16x //默认锁文件位置\"/run/xtables.lock\" } runner := &runner{ exec: exec, //utilexec = osExec封装 dbus: dbus, //utildbus protocol: protocol, //IPv4 or IPv6 hasCheck: getIPTablesHasCheckCommand(vstring), //\"-C\" flag是否指定 hasListener: false, waitFlag: getIPTablesWaitFlag(vstring), //iptables -wait restoreWaitFlag: getIPTablesRestoreWaitFlag(exec, protocol), //iptables-restore -wait lockfilePath: lockfilePath, //xtables锁文件位置 } return runner } // 返回iptables exec命令执行器对象runner func New(exec utilexec.Interface, dbus utildbus.Interface, protocol Protocol) Interface { return newInternal(exec, dbus, protocol, \"\") } 6.2. iptables 执行器方法 runner.run() 这个是方法是runner最基础和公共调用的内部方法，也就是iptables命令执行os exec调用代码。run()有两个传参：1. 指定iptables操作command，2.参数列表。通过传参将组成一个完整的iptables命令进行exec调用执行。runContext()此方法内含有带context上下文和不带context两种执行方式。 pkg/util/iptables/iptables.go:218 func (runner *runner) run(op operation, args []string) ([]byte, error) { return runner.runContext(nil, op, args) } func (runner *runner) runContext(ctx context.Context, op operation, args []string) ([]byte, error) { iptablesCmd := iptablesCommand(runner.protocol) // \"iptabels or ip6tables\" fullArgs := append(runner.waitFlag, string(op)) fullArgs = append(fullArgs, args...) klog.V(5).Infof(\"running iptables %s %v\", string(op), args) // 根据是否传有Context上下文，调用不同的执行command/commandContext if ctx == nil { return runner.exec.Command(iptablesCmd, fullArgs...).CombinedOutput() } return runner.exec.CommandContext(ctx, iptablesCmd, fullArgs...).CombinedOutput() } //支持的iptables操作commands type operation string //runner.exec实现是osexec命令的执行 func New() Interface { return &executor{} } // Command is part of the Interface interface. func (executor *executor) Command(cmd string, args ...string) Cmd { return (*cmdWrapper)(osexec.Command(cmd, args...)) } // CommandContext is part of the Interface interface. func (executor *executor) CommandContext(ctx context.Context, cmd string, args ...string) Cmd { return (*cmdWrapper)(osexec.CommandContext(ctx, cmd, args...)) } runner.GetVersion() 获取系统安装的iptables版本信息，格式为 \"X.Y.Z\" pkg/util/iptables/iptables.go:218 func (runner *runner) GetVersion() (string, error) { return getIPTablesVersionString(runner.exec, runner.protocol) } func getIPTablesVersionString(exec utilexec.Interface, protocol Protocol) (string, error) { // 执行命令\"iptables or ip6tables --version\" iptablesCmd := iptablesCommand(protocol) bytes, err := exec.Command(iptablesCmd, \"--version\").CombinedOutput() if err != nil { return \"\", err } // 正则匹配，查找版本字符串，格式为 \"X.Y.Z\" versionMatcher := regexp.MustCompile(\"v([0-9]+(\\\\.[0-9]+)+)\") match := versionMatcher.FindStringSubmatch(string(bytes)) if match == nil { return \"\", fmt.Errorf(\"no iptables version found in string: %s\", bytes) } return match[1], nil } runner.EnsureChain() \"-N\" 检测指定的规则链是否存在，如果不存则创建此链，存在则返回true pkg/util/iptables/iptables.go:223 func (runner *runner) EnsureChain(table Table, chain Chain) (bool, error) { fullArgs := makeFullArgs(table, chain) runner.mu.Lock() defer runner.mu.Unlock() //执行\"iptables -t $tableName -N $chainName\" out, err := runner.run(opCreateChain, fullArgs) if err != nil { if ee, ok := err.(utilexec.ExitError); ok { if ee.Exited() && ee.ExitStatus() == 1 { return true, nil } } return false, fmt.Errorf(\"error creating chain %q: %v: %s\", chain, err, out) } return false, nil } runner.FlushChain() \"-F\" 清空指定链 pkg/util/iptables/iptables.go:242 func (runner *runner) FlushChain(table Table, chain Chain) error { fullArgs := makeFullArgs(table, chain) //... //执行\"iptables -t $tableName -F $chainName\" out, err := runner.run(opFlushChain, fullArgs) //... } runner.DeleteChain() \"-X\" 删除指定的链 pkg/util/iptables/iptables.go:256 func (runner *runner) DeleteChain(table Table, chain Chain) error { fullArgs := makeFullArgs(table, chain) //... //执行\"iptables -t $tableName -X $chainName\" out, err := runner.run(opDeleteChain, fullArgs) //... } runner.EnsureRule() 检测规则是否存在，不存在则指定的\"表内链上\", 指定position添加规则 pkg/util/iptables/iptables.go:271 func (runner *runner) EnsureRule(position RulePosition, table Table, chain Chain, args ...string) (bool, error) { fullArgs := makeFullArgs(table, chain, args...) runner.mu.Lock() defer runner.mu.Unlock() // 检测规则是否存在 exists, err := runner.checkRule(table, chain, args...) if err != nil { return false, err } if exists { return true, nil } // RulePosition \"-I\" \"-A\" // 指定链序插入规则,执行\"iptables -I $chainName -t $tableName ... \" // 链末添加规则,执行\"iptables -A $chainName -t $tableName ... \" out, err := runner.run(operation(position), fullArgs) if err != nil { return false, fmt.Errorf(\"error appending rule: %v: %s\", err, out) } return false, nil } //checkRule()先判断iptables是否支持\"-C\"flag,调用不同版本的检测rule的方法 func (runner *runner) checkRule(table Table, chain Chain, args ...string) (bool, error) { if runner.hasCheck { return runner.checkRuleUsingCheck(makeFullArgs(table, chain, args...)) } return runner.checkRuleWithoutCheck(table, chain, args...) } //支持\"-C\"flag func (runner *runner) checkRuleUsingCheck(args []string) (bool, error) { ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute) defer cancel() //... //执行\"iptables -wait -C $chainName -t $tableName ... \" out, err := runner.runContext(ctx, opCheckRule, args) //... } //不支持\"-C\"flag，为了兼容iptables版本 runner.DeleteRule() \"-D\" 指定的\"表中链上\"删除规则 pkg/util/iptables/iptables.go:292 func (runner *runner) DeleteRule(table Table, chain Chain, args ...string) error { fullArgs := makeFullArgs(table, chain, args...) //... //检测规则是否存在 exists, err := runner.checkRule(table, chain, args...) //... //执行\"iptables -D $chainName -t $tableName ...\" out, err := runner.run(opDeleteRule, fullArgs) //... } runner.SaveInto() 保存指定表的iptables规则集（buffer内） pkg/util/iptables/iptables.go:317 func (runner *runner) SaveInto(table Table, buffer *bytes.Buffer) error { //... // 执行 \"iptables-save -t $tableName\" iptablesSaveCmd := iptablesSaveCommand(runner.protocol) args := []string{\"-t\", string(table)} cmd := runner.exec.Command(iptablesSaveCmd, args...) cmd.SetStdout(buffer) cmd.SetStderr(buffer) return cmd.Run() } runner.Restore() 装载指定表由iptables-save保存的规则集(从标准输入接收输入) pkg/util/iptables/iptables.go:340 func (runner *runner) Restore(table Table, data []byte, flush FlushFlag, counters RestoreCountersFlag) error { // \"iptables-restore -T $tableName\" args := []string{\"-T\", string(table)} return runner.restoreInternal(args, data, flush, counters) //call and return } // restoreInternal()参数组装和iptables-restore命令恢复规则集data func (runner *runner) restoreInternal(args []string, data []byte, flush FlushFlag, counters RestoreCountersFlag) error { runner.mu.Lock() defer runner.mu.Unlock() trace := utiltrace.New(\"iptables restore\") defer trace.LogIfLong(2 * time.Second) //参数的组装 \"--noflush\" \"--counters\" \"--wait\" if !flush { args = append(args, \"--noflush\") } if counters { args = append(args, \"--counters\") } if len(runner.restoreWaitFlag) == 0 { locker, err := grabIptablesLocks(runner.lockfilePath) if err != nil { return err } trace.Step(\"Locks grabbed\") defer func(locker iptablesLocker) { if err := locker.Close(); err != nil { klog.Errorf(\"Failed to close iptables locks: %v\", err) } }(locker) } fullArgs := append(runner.restoreWaitFlag, args...) iptablesRestoreCmd := iptablesRestoreCommand(runner.protocol) klog.V(4).Infof(\"running %s %v\", iptablesRestoreCmd, fullArgs) // \"iptables-restore -T $tableName --wait --noflush --counters runner.RestoreAll() 如同上Restore()，调用命令iptables-restore装载所有备份规则集 pkg/util/iptables/iptables.go:347 func (runner *runner) RestoreAll(data []byte, flush FlushFlag, counters RestoreCountersFlag) error { args := make([]string, 0) //同上，无参数限制 return runner.restoreInternal(args, data, flush, counters) } runner.AddReloadFunc() 注册reload回调函数，实现iptables reload重新加载规则 pkg/util/iptables/iptables.go:679 func (runner *runner) AddReloadFunc(reloadFunc func()) { runner.mu.Lock() defer runner.mu.Unlock() //是否已启动监听 if !runner.hasListener { runner.connectToFirewallD() //启动D-bus监听 } runner.reloadFuncs = append(runner.reloadFuncs, reloadFunc) //注册信号触发回调func } //通过Linux内核D-bus机制实现对FirewallD进程的信号监听与处理（实现reload iptables规则） func (runner *runner) connectToFirewallD() { bus, err := runner.dbus.SystemBus() if err != nil { klog.V(1).Infof(\"Could not connect to D-Bus system bus: %s\", err) return } runner.hasListener = true //SystemBus对象添加匹配规则定义（firewalld） rule := fmt.Sprintf(\"type='signal',sender='%s',path='%s',interface='%s',member='Reloaded'\", firewalldName, firewalldPath, firewalldInterface) bus.BusObject().Call(\"org.freedesktop.DBus.AddMatch\", 0, rule) rule = fmt.Sprintf(\"type='signal',interface='org.freedesktop.DBus',member='NameOwnerChanged',path='/org/freedesktop/DBus',sender='org.freedesktop.DBus',arg0='%s'\", firewalldName) bus.BusObject().Call(\"org.freedesktop.DBus.AddMatch\", 0, rule) runner.signal = make(chan *godbus.Signal, 10) bus.Signal(runner.signal) go runner.dbusSignalHandler(bus) //D-Bus信号监听处理Handler } //goroutine监听D-Bus信号，监听FirewallD发生变化和reload信号则reload规则集 func (runner *runner) dbusSignalHandler(bus utildbus.Connection) { firewalld := bus.Object(firewalldName, firewalldPath) for s := range runner.signal { if s == nil { // 反注册dbus bus.Signal(runner.signal) return } switch s.Name { case \"org.freedesktop.DBus.NameOwnerChanged\": //信号：指定名称的拥有者发生了变化 name := s.Body[0].(string) new_owner := s.Body[2].(string) // 信号名称为\"org.fedoraproject.FirewallD1\" if name != firewalldName || len(new_owner) == 0 { continue } firewalld.Call(firewalldInterface+\".getDefaultZone\", 0) runner.reload() //重新加载与同步规则(遍历调用runner.reloadFuncs()) case firewalldInterface + \".Reloaded\": runner.reload() } } } runner.Destroy() D-bus监听注消 pkg/util/iptables/iptables.go:218 func (runner *runner) Destroy() { if runner.signal != nil { runner.signal 上面为kube-proxy第三层的iptables Proxier代码分析所有内容，对于另外两种模式ipvs、userspace模式的proxier实现代码分析可查询userspace-mode proxier和ipvs-mode proxier文章内容。 ~本文 END~ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 Iptables-mode Proxier1. 概述2. Proxier 数据结构与类定义3. Proxier对象生成与运行4. Proxier 服务与端点更新 Tracker5. syncProxyRule 同步配置与规则5.1. 更新 service 和 endpoints ;返回更新结果5.1.1. UpdateServiceMap() SVC 服务的更新实现5.1.2. UpdateEndpointsMap() 端点更新的实现5.2. 创建与联接 kube 链5.3. 创建 Iptables 基础数据5.4. 为每个 service 创建 rules5.5. 配置收尾规则数据5.6. 汇集与加载 iptables 配置规则数据6. IPtables 底层的 runner 实现6.1. iptables 执行器6.2. iptables 执行器方法"},"core/kube-proxy/ipvs.html":{"url":"core/kube-proxy/ipvs.html","title":"Ipvs-Mode Proxier","keywords":"","body":"Ipvs-mode proxier 概述 Ipvs-mode proxer 对象创建与初始化 Proxier 服务与端点更新机制 SyncProxyRules 同步 Proxy 规则 更新 service 与 endpoint变化信息 创建 kube 顶层链和连接信息 Dummy 接口和 ipset 默认集创建 每个服务生成 ipvs 规则 SyncIPSetEntries 同步 ipset 记录 创建 iptables 规则数据 刷新 iptables 规则 1. 概述 关于ipvs-mode proxier基础知识可参看官方文档(英文版、中文版)，其官方文档主要介绍以下几方面内容： ipvs技术简介和对比iptables-mode所带来的好处； ipvs-mode proxier按用户配置不同所生成的用户层iptables规则示例(masquerade-all/cluster-cidr/Load Balancer/NodePort/externalIPs)； 如何kube-proxy运行ipvs模式、运行必要条件、运行debug和排错操作。 本文分析将聚焦在代码层的实现解析 (如运行时必要条件检测的代码实现是怎么样的？ipvs为实现service层是如何实现的？iptables规则代码是怎样的？proxier完整的实现逻辑与方式是怎样？等等) 。 ipvs proxy模式主要依赖几个底层技术如 ipvs/ipset /iptables/netlink(用户空间与内核空间异步通信机制),有必要预先对其基础用途或技术细节进行扩展知识的熟悉，将有助于对整个ipvs-mode proxier的实现更深层次的理解。 Ipvs-mode proxier使用ipvs NAT模式实现，ipvs集群操作(如虚拟服务器、RealServer)是通过netlink内核通迅创建标准的协议格式通迅消息体进行交互实现。 Ipvs-mode proxier也同样使用了iptables固定模板规则结合ipset集来进行动态管理变化更新。 Ipvs-mode proxier整个代码机制逻辑与iptables-mode一致(参看iptable-mode代码逻辑示意图)。同样是通过同步apiserver事件及更新信息，生成相应的路由规则。但ipvs-mode服务规则不同于iptables-mode,不仅使用了ipset扩展的方式简化iptables规则条目和优化性能，而且还使用ipvs技术实现更丰富的集群负载策略管理。其规则生成操作须对ipset集、iptables规则、ipvs集群进行同步更新操作，关键逻辑代码在syncProxyRules()内。 2. Ipvs-mode proxer 对象创建与初始化 ProxyServer实例化时初始化了proxier模式，如果代理模式指定为Ipvs,则创建proxier对象，且指定其service与endpoints的事件处理器。 cmd/kube-proxy/app/server_others.go:59 func newProxyServer(...) (*ProxyServer, error) { //... else if proxyMode == proxyModeIPVS { //当proxy模式指定为IPVS模式(命令参数或配置文件) klog.V(0).Info(\"Using ipvs Proxier.\") proxierIPVS, err := ipvs.NewProxier( //创建ipvs-mode proxier对象 //... ) // porxyServer的proxier对象与事件处理器的指定 proxier = proxierIPVS serviceEventHandler = proxierIPVS endpointsEventHandler = proxierIPVS //... } ipvs-mode proxier对象实例化NewProxier()，对ipvs环境进行初始化。 相关内核参数调整说明： net/ipv4/conf/all/route_localnet 是否允许外部访问localhost； net/bridge/bridge-nf-call-iptables 为二层的网桥在转发包时也会被iptables的FORWARD规则所过滤，这样就会出现L3层的iptables rules去过滤L2的帧的问题； net/ipv4/vs/conntrack 开启NFCT(Netfilter connection tracking连接与状态跟踪)； net/ipv4/vs/conn_reuse_mode 网络连接复用模式的选择； net/ipv4/vs/expire_nodest_conn 值为0，当LVS转发数据包，发现目的RS无效（删除）时，会丢弃该数据包，但不删除相应连接。值为1时，则马上释放相应连接； net/ipv4/vs/expire_quiescent_template 值为0，当RS的weight值=0（如，健康检测失败，应用程序将RS weight置0）时，会话保持的新建连接 还会继续调度到该RS上；值为1，则马上将会话保持的连接模板置为无效，重新调度新的RS。如果有会话保持的业务，建议该值配置为1； net/ipv4/ip_forward 是否打开ipv4的IP转发模式; net/ipv4/conf/all/arp_ignore 定义对目标地址为本地IP的ARP询问不同的应答模式(0~8),模式1表示：只回答目标IP地址是来访网络接口本地地址的ARP查询请求； net/ipv4/conf/all/arp_announce 对网络接口上，本地IP地址的发出的，ARP回应，作出相应级别的限制；值为2表示：对查询目标使用最适当的本地地址； pkg/proxy/ipvs/proxier.go:280 func NewProxier(...) (*Proxier, error) { // sysctl配置项 \"net/ipv4/conf/all/route_localnet\" 值为1 if val, _ := sysctl.GetSysctl(sysctlRouteLocalnet); val != 1 { if err := sysctl.SetSysctl(sysctlRouteLocalnet, 1); err != nil { return nil, fmt.Errorf(\"can't set sysctl %s: %v\", sysctlRouteLocalnet, err) } } //... // sysctl配置项 \"net/bridge/bridge-nf-call-iptables\" 值为1 sysctl.GetSysctl(sysctlBridgeCallIPTables) // sysctl配置项 \"net/ipv4/vs/conntrack\" 值为1 sysctl.SetSysctl(sysctlVSConnTrack, 1) // sysctl配置项 \"net/ipv4/vs/conn_reuse_mode\" 值为0 sysctl.SetSysctl(sysctlConnReuse, 0) // sysctl配置项 \"net/ipv4/vs/expire_nodest_conn\" 值为1 sysctl.SetSysctl(sysctlExpireNoDestConn, 1) // sysctl配置项 \"net/ipv4/vs/expire_quiescent_template\" 值为1 sysctl.SetSysctl(sysctlExpireQuiescentTemplate, 1) // sysctl配置项 \"net/ipv4/ip_forward\" 值为1 sysctl.SetSysctl(sysctlForward, 1) // sysctl配置项 \"net/ipv4/conf/all/arp_ignore\" 值为1 sysctl.SetSysctl(sysctlArpIgnore, 1) // sysctl配置项 \"net/ipv4/conf/all/arp_announce\" 值为2 sysctl.SetSysctl(sysctlArpAnnounce, 2) //... // 生成masquerade标志用于SNAT规则 masqueradeValue := 1 3. Proxier 服务与端点更新机制 ipvs模式和iptables模式的service和endpoints更新变化信息同步机制是一致的(更详细说明可参考iptables-mode proxier文章)，但为了本文的完整性和相对独立性，这里我们也简单的过一下部分代码。 在构建ipvs-mode proxier对象时指定同步运行器async.NewBoundedFrequencyRunner，同步proxy的规则处理则是syncProxyRules()。同样ipvs-proxier类对象有两个属性对象：serviceChanges(ServiceChangeTracker)和endpointsChanges(EndpointChangeTracker)是就是用来跟踪并记录service和endpoints的变化信息更新至相应的两个属性Items map(serviceChange和endpointsChange)。 pkg/proxy/ipvs/proxier.go:429 proxier.syncRunner = async.NewBoundedFrequencyRunner(\"sync-runner\", proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) 在框架层第二层proxy server的运行时最后的调用就是\"s.Proxier.SyncLoop()\" pkg/proxy/ipvs/proxier.go:631 func (proxier *Proxier) SyncLoop() { // Update healthz timestamp at beginning in case Sync() never succeeds. // ... proxier.syncRunner.Loop(wait.NeverStop) //执行NewBoundedFrequencyRunner对象Loop } pkg/util/async/bounded_frequency_runner.go:169 func (bfr *BoundedFrequencyRunner) Loop(stop BoundedFrequencyRunner.tryRun() 按指定频率执行回调函数func \"bfr.fn()\" pkg/util/async/bounded_frequency_runner.go:211 func (bfr *BoundedFrequencyRunner) tryRun() { bfr.mu.Lock() defer bfr.mu.Unlock() //限制条件允许运行func if bfr.limiter.TryAccept() { bfr.fn() // 重点执行部分，调用func，上下文来看此处就是 // 对syncProxyRules()的调用 bfr.lastRun = bfr.timer.Now() // 记录运行时间 bfr.timer.Stop() bfr.timer.Reset(bfr.maxInterval) // 重设下次运行时间 klog.V(3).Infof(\"%s: ran, next possible in %v, periodic in %v\", bfr.name, bfr.minInterval, bfr.maxInterval) return } //限制条件不允许运行，计算下次运行时间 elapsed := bfr.timer.Since(bfr.lastRun) // elapsed:上次运行时间到现在已过多久 nextPossible := bfr.minInterval - elapsed // nextPossible:下次运行至少差多久（最小周期） nextScheduled := bfr.maxInterval - elapsed // nextScheduled:下次运行最迟差多久(最大周期) klog.V(4).Infof(\"%s: %v since last run, possible in %v, scheduled in %v\", bfr.name, elapsed, nextPossible, nextScheduled) if nextPossible 4. SyncProxyRules 同步 Proxy 规则 syncProxyRules()为proxier的核心逻辑，类似于iptables proxier实现了对apiserver同步的service、endpoints信息的同步与监听，同时在其生成初始和变化时同步ipvs规则（iptables、ipvs虚拟主机、ipset集规则），最终实现kubernetes的\"service\"机制。 syncProxyRules()代码部分过长，下面将分开对重点部分一一进行分析。 ipvs-mode proxier的同步ipvs规则主要完成以下几个主要步骤操作： 同步与新更service和endpoints； 初始化链和ipset集； 每个服务构建ipvs规则(iptables/ipvs/ipset),服务类型不同生成的规则也相应不同； 清理过旧规则及信息 。 4.1. 更新 service 与 endpoint变化信息 ipvs-mode proxier的service和endpoint变化更新的机制与iptables-mode的完全一致，详细可以参考iptables-mode的\"syncProxyRule 同步配置与规则\"内的相关内容，这里就不再详细赘述。 Proxier类对象有两个属性：serviceChanges和endpointsChanges是就是用来跟踪Service和Endpoint的更新信息，以及两个Tracker及方法：ServiceChangeTracker服务信息变更Tracker，EndpointChangeTracker 端点信息变更Tracker，实时监听apiserver的变更事件。 UpdateServiceMap() svc 服务的更新实现，将serviceChanges的服务项与proxier serviceMap进行更新(合并、删除废弃项)返回，UpdateEndpointsMap() 端点更新的实现，将endpointsChanges的端点项与proxier endpointMap进行更新(合并、删除废弃项)并返回已更新信息。 pkg/proxy/ipvs/proxier.go:730 serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxy.UpdateEndpointsMap(proxier.endpointsMap, proxier.endpointsChanges) 4.2. 创建 kube 顶层链和连接信息 pkg/proxy/ipvs/proxier.go:748 proxier.natChains.Reset() //nat链 proxier.natRules.Reset() //nat规则 proxier.filterChains.Reset() //filter链 proxier.filterRules.Reset() //filter规则 //写表头 writeLine(proxier.filterChains, \"*filter\") writeLine(proxier.natChains, \"*nat\") proxier.createAndLinkeKubeChain() //创建kubernetes的表连接链数据 pkg/proxy/ipvs/proxier.go:1418 func (proxier *Proxier) createAndLinkeKubeChain() { //通过iptables-save获取现有的filter和NAT表存在的链数据 existingFilterChains := proxier.getExistingChains(proxier.filterChainsData, utiliptables.TableFilter) existingNATChains := proxier.getExistingChains(proxier.iptablesData, utiliptables.TableNAT) // 顶层链数据的构建 // NAT表链： KUBE-SERVICES / KUBE-POSTROUTING / KUBE-FIREWALL // KUBE-NODE-PORT / KUBE-LOAD-BALANCER / KUBE-MARK-MASQ // Filter表链： KUBE-FORWARD for _, ch := range iptablesChains { //不存在则创建链，创建顶层链 if _, err := proxier.iptables.EnsureChain(ch.table, ch.chain); err != nil { klog.Errorf(\"Failed to ensure that %s chain %s exists: %v\", ch.table, ch.chain, err) return } //nat表写链 if ch.table == utiliptables.TableNAT { if chain, ok := existingNATChains[ch.chain]; ok { writeBytesLine(proxier.natChains, chain) //现存在的链 } else { // \"KUBE-POSTROUTING\" writeLine(proxier.natChains, utiliptables.MakeChainLine(kubePostroutingChain)) } } else { // filter表写链 if chain, ok := existingFilterChains[KubeForwardChain]; ok { writeBytesLine(proxier.filterChains, chain) //现存在的链 } else { // \"KUBE-FORWARD\" writeLine(proxier.filterChains, utiliptables.MakeChainLine(KubeForwardChain)) } } } // 默认链下创建kubernete服务专用跳转规则 // iptables -I OUTPUT -t nat --comment \"kubernetes service portals\" -j KUBE-SERVICES // iptables -I PREROUTING -t nat --comment \"kubernetes service portals\" -j KUBE-SERVICES // iptables -I POSTROUTING -t nat --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING // iptables -I FORWARD -t filter --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD for _, jc := range iptablesJumpChain { args := []string{\"-m\", \"comment\", \"--comment\", jc.comment, \"-j\", string(jc.to)} if _, err := proxier.iptables.EnsureRule(utiliptables.Prepend, jc.table, jc.from, args...); err != nil { klog.Errorf(\"Failed to ensure that %s chain %s jumps to %s: %v\", jc.table, jc.from, jc.to, err) } } // 写kubernetes专用的POSTROUTING nat规则 // -A KUBE-POSTROUTING -m comment --comment \"...\" -m mark --mark $masqueradeMark -j MASQUERADE writeLine(proxier.natRules, []string{ \"-A\", string(kubePostroutingChain), \"-m\", \"comment\", \"--comment\", `\"kubernetes service traffic requiring SNAT\"`, \"-m\", \"mark\", \"--mark\", proxier.masqueradeMark, \"-j\", \"MASQUERADE\", }...) // 写kubernetes专用的masquerade伪装地址标记规则 // -A KUBE-MARK-MASQ -j MARK --set-xmark $masqueradeMark writeLine(proxier.natRules, []string{ \"-A\", string(KubeMarkMasqChain), \"-j\", \"MARK\", \"--set-xmark\", proxier.masqueradeMark, }...) } 4.3. Dummy 接口和 ipset 默认集创建 pkg/proxy/ipvs/proxier.go:760 //为服务地址的绑定，确保已创建虚拟接口kube-ipvs0 _, err := proxier.netlinkHandle.EnsureDummyDevice(DefaultDummyDevice) if err != nil { klog.Errorf(\"Failed to create dummy interface: %s, error: %v\", DefaultDummyDevice, err) return } // 确保kubernetes专用的ipset集已创建 for _, set := range proxier.ipsetList { if err := ensureIPSet(set); err != nil { return } set.resetEntries() } proxier.ipsetList的定义信息,在proxier对象创建时初始化了ipsetList列表 pkg/proxy/ipvs/proxier.go:113 var ipsetInfo = []struct { name string //ipset set名称 setType utilipset.Type //set类型{HashIPPortIP|HashIPPort|HashIPPortNet|BitmapPort} comment string //comment描述信息 }{ {kubeLoopBackIPSet, utilipset.HashIPPortIP, kubeLoopBackIPSetComment}, //... } ipset集名称 类型 描述 KUBE-LOOP-BACK hash:ip,port,ip Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose KUBE-CLUSTER-IP hash:ip,port Kubernetes service cluster ip + port for masquerade purpose KUBE-EXTERNAL-IP hash:ip,port Kubernetes service external ip + port for masquerade and filter purpose KUBE-LOAD-BALANCER hash:ip,port Kubernetes service lb portal KUBE-LOAD-BALANCER-FW hash:ip,port Kubernetes service load balancer ip + port for load balancer with sourceRange KUBE-LOAD-BALANCER-LOCAL hash:ip,port Kubernetes service load balancer ip + port with externalTrafficPolicy=local KUBE-LOAD-BALANCER-SOURCE-IP hash:ip,port,ip Kubernetes service load balancer ip + port + source IP for packet filter purpose KUBE-LOAD-BALANCER-SOURCE-CIDR hash:ip,port,net Kubernetes service load balancer ip + port + source cidr for packet filter purpose KUBE-NODE-PORT-TCP BitmapPort Kubernetes nodeport TCP port for masquerade purpose KUBE-NODE-PORT-LOCAL-TCP BitmapPort BitmapPort,Kubernetes nodeport TCP port with externalTrafficPolicy=local KUBE-NODE-PORT-UDP BitmapPort Kubernetes nodeport UDP port for masquerade purpose KUBE-NODE-PORT-LOCAL-UDP BitmapPort Kubernetes nodeport UDP port with externalTrafficPolicy=local KUBE-NODE-PORT-SCTP BitmapPort Kubernetes nodeport SCTP port for masquerade purpose KUBE-NODE-PORT-LOCAL-SCTP BitmapPort Kubernetes nodeport SCTP port with externalTrafficPolicy=local 4.4. 每个服务生成 ipvs 规则 代码逻辑包含在一个for循环内，对serviceMap内的每个服务进行遍历处理，对不同的服务类型(clusterip/nodePort/externalIPs/load-balancer)进行不同的处理(ipset集/ipvs虚拟主机/ipvs后端服务器)。 ipvs模式，通过svc创建的集群都绑定在默认dummy(kube-ipvs0)虚拟网卡，创建ipvs集群IP执行以下几项操作： 节点中存在虚拟接口为 kube-ipvs0,且服务 IP 地址绑定到虚拟接口 分别为每个kube服务 IP 地址创建 IPVS 虚拟服务器 为每个 IPVS 虚拟服务器创建RealServers (kube服务 endpoints) pkg/proxy/ipvs/proxier.go:784 for svcName, svc := range proxier.serviceMap { //...... 后面详细分析 } 基于此服务的有效endpoint列表，更新KUBE-LOOP-BACK的ipset集，以备后面生成相应iptables规则(SNAT伪装地址)。 pkg/proxy/ipvs/proxier.go:796 for _, e := range proxier.endpointsMap[svcName] { ep, ok := e.(*proxy.BaseEndpointInfo) if !ok { klog.Errorf(\"Failed to cast BaseEndpointInfo %q\", e.String()) continue } if !ep.IsLocal { //非本地 continue } epIP := ep.IP() //端点IP epPort, err := ep.Port() //端点Port if epIP == \"\" || err != nil { //有效IP和端口正常 continue } // 构造ipset集的entry记录项 entry := &utilipset.Entry{ IP: epIP, Port: epPort, Protocol: protocol, IP2: epIP, SetType: utilipset.HashIPPortIP, } // 类型校验KUBE-LOOP-BACK集合entry记录项 if valid := proxier.ipsetList[kubeLoopBackIPSet].validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoopBackIPSet].Name)) continue } // 插入此entry记录至active记录队列 proxier.ipsetList[kubeLoopBackIPSet].activeEntries.Insert(entry.String()) } clusterIP服务类型流量的承接(clusterIP为默认方式，仅资源集群内可访问),ipset集KUBE-CLUSTER-IP更新,以备后面生成相应iptables规则。 pkg/proxy/ipvs/proxier.go:827 //构建ipset entry entry := &utilipset.Entry{ IP: svcInfo.ClusterIP.String(), Port: svcInfo.Port, Protocol: protocol, SetType: utilipset.HashIPPort, } // 类型校验ipset entry if valid := proxier.ipsetList[kubeClusterIPSet].validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeClusterIPSet].Name)) continue } // 名为KUBE-CLUSTER-IP的ipset集插入entry,以备后面统一生成IPtables规则 proxier.ipsetList[kubeClusterIPSet].activeEntries.Insert(entry.String()) // 构建ipvs虚拟服务器VS服务对象 serv := &utilipvs.VirtualServer{ Address: svcInfo.ClusterIP, Port: uint16(svcInfo.Port), Protocol: string(svcInfo.Protocol), Scheduler: proxier.ipvsScheduler, } // 设置IPVS服务的会话保持标志和超时时间 if svcInfo.SessionAffinityType == v1.ServiceAffinityClientIP { serv.Flags |= utilipvs.FlagPersistent serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds) } // 将clusterIP绑定至dummy虚拟接口上，syncService()处理中需置bindAddr地址为True。 // ipvs为服务创建VS(虚拟主机) if err := proxier.syncService(svcNameString, serv, true); err == nil { activeIPVSServices[serv.String()] = true activeBindAddrs[serv.Address.String()] = true // 为虚拟主机/服务(vip)同步endpoints信息。 // IPVS为VS更新RS(realServer后端服务器) if err := proxier.syncEndpoint(svcName, false, serv); err != nil { klog.Errorf(\"Failed to sync endpoint for service: %v, err: %v\", serv, err) } } else { klog.Errorf(\"Failed to sync service: %v, err: %v\", serv, err) } syncService() 更新和同步ipvs服务信息及服务IP与dummy接口的绑定 pkg/proxy/ipvs/proxier.go:1498 func (proxier *Proxier) syncService(svcName string, vs *utilipvs.VirtualServer, bindAddr bool) error { //获取IPVS虚拟主机服务信息 appliedVirtualServer, _ := proxier.ipvs.GetVirtualServer(vs) //无此虚拟主机服务或此服务信息变更 if appliedVirtualServer == nil || !appliedVirtualServer.Equal(vs) { if appliedVirtualServer == nil { // 服务未找到，则创建新的服务 klog.V(3).Infof(\"Adding new service %q %s:%d/%s\", svcName, vs.Address, vs.Port, vs.Protocol) if err := proxier.ipvs.AddVirtualServer(vs); err != nil { klog.Errorf(\"Failed to add IPVS service %q: %v\", svcName, err) return err } } else { // 服务信息改变，则更新存在服务信息，在更新期间服务VIP不会关闭 klog.V(3).Infof(\"IPVS service %s was changed\", svcName) if err := proxier.ipvs.UpdateVirtualServer(vs); err != nil { klog.Errorf(\"Failed to update IPVS service, err:%v\", err) return err } } // 将服务IP绑定到dummy接口上 if bindAddr { klog.V(4).Infof(\"Bind addr %s\", vs.Address.String()) _, err := proxier.netlinkHandle.EnsureAddressBind(vs.Address.String(), DefaultDummyDevice) //netlinkHandle处理的实现在文章最后的netlink工具介绍部分详细说明 if err != nil { klog.Errorf(\"Failed to bind service address to dummy device %q: %v\", svcName, err) return err } } return nil } syncEndpoint() 为虚拟主机/服务(clusterip)同步endpoints信息，实现ipvs为VS更新RS(realServer后端服务器)。 pkg/proxy/ipvs/proxier.go:1532 func (proxier *Proxier) syncEndpoint(svcPortName proxy.ServicePortName, onlyNodeLocalEndpoints bool, vs *utilipvs.VirtualServer) error { appliedVirtualServer, err := proxier.ipvs.GetVirtualServer(vs) if err != nil || appliedVirtualServer == nil { klog.Errorf(\"Failed to get IPVS service, error: %v\", err) return err } // curEndpoints表示当前系统IPVS目标列表 curEndpoints := sets.NewString() // newEndpoints表示从apiServer监听到的Endpoints newEndpoints := sets.NewString() // 依据虚拟服务器获取RS(realservers)列表 curDests, err := proxier.ipvs.GetRealServers(appliedVirtualServer) if err != nil { klog.Errorf(\"Failed to list IPVS destinations, error: %v\", err) return err } for _, des := range curDests { curEndpoints.Insert(des.String()) // 写入curEndpoints } //迭代endpointsMaps信息，将非本地的enpoints写入newEndpoints for _, epInfo := range proxier.endpointsMap[svcPortName] { if onlyNodeLocalEndpoints && !epInfo.GetIsLocal() { continue } newEndpoints.Insert(epInfo.String()) } // 创建新的endpoints for _, ep := range newEndpoints.List() { ip, port, err := net.SplitHostPort(ep) if err != nil { klog.Errorf(\"Failed to parse endpoint: %v, error: %v\", ep, err) continue } portNum, err := strconv.Atoi(port) if err != nil { klog.Errorf(\"Failed to parse endpoint port %s, error: %v\", port, err) continue } newDest := &utilipvs.RealServer{ Address: net.ParseIP(ip), Port: uint16(portNum), Weight: 1, } //判断当前系统ipvs列表是否存在 if curEndpoints.Has(ep) { //检测是否在gracefulDelete列表，如果是则此处立即删除 uniqueRS := GetUniqueRSName(vs, newDest) if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) { continue } klog.V(5).Infof(\"new ep %q is in graceful delete list\", uniqueRS) err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS) if err != nil { klog.Errorf(\"Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v\", ep, err) continue } } // 不存在则新增RealServer项(对应目标endpoint) err = proxier.ipvs.AddRealServer(appliedVirtualServer, newDest) if err != nil { klog.Errorf(\"Failed to add destination: %v, error: %v\", newDest, err) continue } } // 删除过旧的endpoints for _, ep := range curEndpoints.Difference(newEndpoints).UnsortedList() { // 如果curEndpoint在gracefulDelete内，跳过 uniqueRS := vs.String() + \"/\" + ep if proxier.gracefuldeleteManager.InTerminationList(uniqueRS) { continue } ip, port, err := net.SplitHostPort(ep) if err != nil { klog.Errorf(\"Failed to parse endpoint: %v, error: %v\", ep, err) continue } portNum, err := strconv.Atoi(port) if err != nil { klog.Errorf(\"Failed to parse endpoint port %s, error: %v\", port, err) continue } delDest := &utilipvs.RealServer{ Address: net.ParseIP(ip), Port: uint16(portNum), } klog.V(5).Infof(\"Using graceful delete to delete: %v\", uniqueRS) // 删除RS err = proxier.gracefuldeleteManager.GracefulDeleteRS(appliedVirtualServer, delDest) if err != nil { klog.Errorf(\"Failed to delete destination: %v, error: %v\", uniqueRS, err) continue } } return nil } externalIPs服务类型流量的承接，服务是否启用ExternalIPs,在指定的Node上开启监听端口(代码逻辑判断是否为本地ip),而非像nodeport所有节点监听。ipset集KUBE-EXTERNAL-IP更新，以备后面生成相应iptables规则。 pkg/proxy/ipvs/proxier.go:866 for _, externalIP := range svcInfo.ExternalIPs { if local, err := utilproxy.IsLocalIP(externalIP); err != nil { klog.Errorf(\"can't determine if IP is local, assuming not: %v\", err) // 如果指定的externealIP为本地地址且协议不为SCTP } else if local && (svcInfo.GetProtocol() != v1.ProtocolSCTP) { lp := utilproxy.LocalPort{ Description: \"externalIP for \" + svcNameString, IP: externalIP, Port: svcInfo.Port, Protocol: protocol, } if proxier.portsMap[lp] != nil { //端口已存在 klog.V(4).Infof(\"Port %s was open before and is still needed\", lp.String()) replacementPortsMap[lp] = proxier.portsMap[lp] } else { socket, err := proxier.portMapper.OpenLocalPort(&lp) //打开本地端口socket if err != nil { msg := fmt.Sprintf(\"can't open %s, skipping this externalIP: %v\", lp.String(), err) proxier.recorder.Eventf( //通知事件 &v1.ObjectReference{ Kind: \"Node\", Name: proxier.hostname, UID: types.UID(proxier.hostname), Namespace: \"\", }, v1.EventTypeWarning, err.Error(), msg) klog.Error(msg) continue } replacementPortsMap[lp] = socket //存放端口信息 } } // 创建ipset entry entry := &utilipset.Entry{ IP: externalIP, Port: svcInfo.Port, Protocol: protocol, SetType: utilipset.HashIPPort, } // We have to SNAT packets to external IPs. if valid := proxier.ipsetList[kubeExternalIPSet].validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeExternalIPSet].Name)) continue } // 名为KUBE-EXTERNAL-IP的ipset集插入entry,以备后面统一生成IPtables规则 proxier.ipsetList[kubeExternalIPSet].activeEntries.Insert(entry.String()) // 为服务定义ipvs虚拟主机信息 serv := &utilipvs.VirtualServer{ Address: net.ParseIP(externalIP), Port: uint16(svcInfo.Port), Protocol: string(svcInfo.Protocol), Scheduler: proxier.ipvsScheduler, } if svcInfo.SessionAffinityType == v1.ServiceAffinityClientIP { serv.Flags |= utilipvs.FlagPersistent serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds) } // 将clusterIP绑定至dummy虚拟接口上，syncService()处理中需置bindAddr地址为True。 // ipvs为服务创建VS(虚拟主机) // 为虚拟主机/服务同步endpoints信息。 // IPVS为VS更新RS(realServer后端服务器) //...(同clusterip) } load-balancer服务类型流量的承接,服务的LoadBalancerSourceRanges和externalTrafficPolicy=local被指定时将对KUBE-LOAD-BALANCER-LOCAL、KUBE-LOAD-BALANCER-FW、KUBE-LOAD-BALANCER-SOURCE-CIDR、KUBE-LOAD-BALANCER-SOURCE-IP ipset集更新,以备后面生成相应iptables规则。 pkg/proxy/ipvs/proxier.go:937 for _, ingress := range svcInfo.LoadBalancerStatus.Ingress { if ingress.IP != \"\" { // 构建ipset entry entry = &utilipset.Entry{ IP: ingress.IP, Port: svcInfo.Port, Protocol: protocol, SetType: utilipset.HashIPPort, } // 增加SLB(service load balancer)ingressIP:Port与kube服务IP集对应 // KUBE-LOAD-BALANCER ipset集更新 if valid := proxier.ipsetList[kubeLoadBalancerSet].validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSet].Name)) continue } proxier.ipsetList[kubeLoadBalancerSet].activeEntries.Insert(entry.String()) // 服务指定externalTrafficPolicy=local时,KUBE-LOAD-BALANCER-LOCAL ipset集更新 if svcInfo.OnlyNodeLocalEndpoints { if valid := proxier.ipsetList[kubeLoadBalancerLocalSet].validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerLocalSet].Name)) continue } proxier.ipsetList[kubeLoadBalancerLocalSet].activeEntries.Insert(entry.String()) } // 服务的LoadBalancerSourceRanges被指定时，基于源IP保护的防火墙策略开启，KUBE-LOAD-BALANCER-FW ipset集更新 if len(svcInfo.LoadBalancerSourceRanges) != 0 { if valid := proxier.ipsetList[kubeLoadbalancerFWSet].validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadbalancerFWSet].Name)) continue } proxier.ipsetList[kubeLoadbalancerFWSet].activeEntries.Insert(entry.String()) allowFromNode := false for _, src := range svcInfo.LoadBalancerSourceRanges { // 构建ipset entry entry = &utilipset.Entry{ IP: ingress.IP, Port: svcInfo.Port, Protocol: protocol, Net: src, SetType: utilipset.HashIPPortNet, } // 枚举所有源CIDR白名单列表，KUBE-LOAD-BALANCER-SOURCE-CIDR ipset集更新 if valid := proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].Name)) continue } proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].activeEntries.Insert(entry.String()) // ignore error because it has been validated _, cidr, _ := net.ParseCIDR(src) if cidr.Contains(proxier.nodeIP) { allowFromNode = true } } // 允许来自Node流量（LB对应后端hosts之间交互） if allowFromNode { entry = &utilipset.Entry{ IP: ingress.IP, Port: svcInfo.Port, Protocol: protocol, IP2: ingress.IP, SetType: utilipset.HashIPPortIP, } // 枚举所有白名单源IP列表，KUBE-LOAD-BALANCER-SOURCE-IP ipset集更新 if valid := proxier.ipsetList[kubeLoadBalancerSourceIPSet].validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSourceIPSet].Name)) continue } proxier.ipsetList[kubeLoadBalancerSourceIPSet].activeEntries.Insert(entry.String()) } } // 构建ipvs 虚拟主机对象 serv := &utilipvs.VirtualServer{ Address: net.ParseIP(ingress.IP), // SLB ip Port: uint16(svcInfo.Port), // SLB 端口 Protocol: string(svcInfo.Protocol), // 协议 Scheduler: proxier.ipvsScheduler, // RR } if svcInfo.SessionAffinityType == v1.ServiceAffinityClientIP { serv.Flags |= utilipvs.FlagPersistent serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds) } // ipvs为服务创建VS(虚拟主机)，LB ingressIP绑定dummy接口 // ipvs为VS更新RS(realServer后端服务器) //...(同clusterip) } } NodePort服务类型流量的承接,服务将在每个节点上都将开启指定的nodeport端口,并更新相应的ipset集。 pkg/proxy/ipvs/proxier.go:1040 if svcInfo.NodePort != 0 { addresses, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer) // 获取node addresses if err != nil { klog.Errorf(\"Failed to get node ip address matching nodeport cidr: %v\", err) continue } var lps []utilproxy.LocalPort for address := range addresses { lp := utilproxy.LocalPort{ Description: \"nodePort for \" + svcNameString, IP: address, Port: svcInfo.NodePort, Protocol: protocol, } if utilproxy.IsZeroCIDR(address) { // Empty IP address means all lp.IP = \"\" lps = append(lps, lp) break } lps = append(lps, lp) //整理与格式化后的lps列表 } // 为node节点的IPs打开端口并保存持有socket句柄 for _, lp := range lps { if proxier.portsMap[lp] != nil { klog.V(4).Infof(\"Port %s was open before and is still needed\", lp.String()) replacementPortsMap[lp] = proxier.portsMap[lp] } else if svcInfo.GetProtocol() != v1.ProtocolSCTP { // 打开和监听端口(非SCTP协议) socket, err := proxier.portMapper.OpenLocalPort(&lp) if err != nil { klog.Errorf(\"can't open %s, skipping this nodePort: %v\", lp.String(), err) continue } if lp.Protocol == \"udp\" { // UDP协议，清理udp conntrack记录 isIPv6 := utilnet.IsIPv6(svcInfo.ClusterIP) conntrack.ClearEntriesForPort(proxier.exec, lp.Port, isIPv6, v1.ProtocolUDP) } replacementPortsMap[lp] = socket } //socket保存 } // Nodeports无论是否为本地都需要SNAT // 构建ipset entry entry = &utilipset.Entry{ // No need to provide ip info Port: svcInfo.NodePort, Protocol: protocol, SetType: utilipset.BitmapPort, } var nodePortSet *IPSet //基于协议类型选择ipset集 switch protocol { case \"tcp\": // KUBE-NODE-PORT-TCP nodePortSet = proxier.ipsetList[kubeNodePortSetTCP] case \"udp\": // KUBE-NODE-PORT-UDP nodePortSet = proxier.ipsetList[kubeNodePortSetUDP] case \"sctp\": // KUBE-NODE-PORT-SCTP nodePortSet = proxier.ipsetList[kubeNodePortSetSCTP] default: klog.Errorf(\"Unsupported protocol type: %s\", protocol) } if nodePortSet != nil { if valid := nodePortSet.validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, nodePortSet.Name)) continue } // 更新ipset集 nodePortSet.activeEntries.Insert(entry.String()) } // 服务externaltrafficpolicy=local指定时，基于协议类型更新ipset集entry if svcInfo.OnlyNodeLocalEndpoints { var nodePortLocalSet *IPSet switch protocol { case \"tcp\": //KUBE-NODE-PORT-LOCAL-TCP nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetTCP] case \"udp\": //KUBE-NODE-PORT-LOCAL-UDP nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetUDP] case \"sctp\": //KUBE-NODE-PORT-LOCAL-SCTP nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetSCTP] default: klog.Errorf(\"Unsupported protocol type: %s\", protocol) } if nodePortLocalSet != nil { if valid := nodePortLocalSet.validateEntry(entry); !valid { klog.Errorf(\"%s\", fmt.Sprintf(EntryInvalidErr, entry, nodePortLocalSet.Name)) continue } // 更新ipset集 nodePortLocalSet.activeEntries.Insert(entry.String()) } } // 为Node每个ip address创建ipvs路由项(VS/RS) var nodeIPs []net.IP for address := range addresses { if !utilproxy.IsZeroCIDR(address) { nodeIPs = append(nodeIPs, net.ParseIP(address)) continue } // zero cidr nodeIPs, err = proxier.ipGetter.NodeIPs() if err != nil { klog.Errorf(\"Failed to list all node IPs from host, err: %v\", err) } } for _, nodeIP := range nodeIPs { // 构建ipvs VS对象 serv := &utilipvs.VirtualServer{ Address: nodeIP, //node ip地址 Port: uint16(svcInfo.NodePort), //node端口 Protocol: string(svcInfo.Protocol), //协议 Scheduler: proxier.ipvsScheduler, //RR } if svcInfo.SessionAffinityType == v1.ServiceAffinityClientIP { serv.Flags |= utilipvs.FlagPersistent serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds) } // 这里不需要将Node IP绑定到dummy接口，参数值为false // ipvs为服务创建VS(虚拟主机) // ipvs为VS更新RS(realServer后端服务器) //...(同clusterip) } 4.5. SyncIPSetEntries 同步 ipset 记录 pkg/proxy/ipvs/proxier.go:1176 for _, set := range proxier.ipsetList { set.syncIPSetEntries() } pkg/proxy/ipvs/ipset.go:125 func (set *IPSet) syncIPSetEntries() { appliedEntries, err := set.handle.ListEntries(set.Name) if err != nil { klog.Errorf(\"Failed to list ip set entries, error: %v\", err) return } // currentIPSetEntries代表从apiServer上一直监听着的endpoints列表 currentIPSetEntries := sets.NewString() for _, appliedEntry := range appliedEntries { currentIPSetEntries.Insert(appliedEntry) } // 求差集 // s1 = {a1, a2, a3} // s2 = {a1, a2, a4, a5} // s1.Difference(s2) = {a3} // s2.Difference(s1) = {a4,a5} if !set.activeEntries.Equal(currentIPSetEntries) { // 清理过旧记录（取currentIPSetEntries在activeEntries中没有的entries） for _, entry := range currentIPSetEntries.Difference(set.activeEntries).List() { if err := set.handle.DelEntry(entry, set.Name); err != nil { if !utilipset.IsNotFoundError(err) { klog.Errorf(\"Failed to delete ip set entry: %s from ip set: %s, error: %v\", entry, set.Name, err) } } else { klog.V(3).Infof(\"Successfully delete legacy ip set entry: %s from ip set: %s\", entry, set.Name) } } // 新增记录（取activeEntries在currentIPSetEntries中没有的entries） for _, entry := range set.activeEntries.Difference(currentIPSetEntries).List() { if err := set.handle.AddEntry(entry, &set.IPSet, true); err != nil { klog.Errorf(\"Failed to add entry: %v to ip set: %s, error: %v\", entry, set.Name, err) } else { klog.V(3).Infof(\"Successfully add entry: %v to ip set: %s\", entry, set.Name) } } } } 4.6. 创建 iptables 规则数据 pkg/proxy/ipvs/proxier.go:1182 proxier.writeIptablesRules() 基于ipset定义创建iptables NAT表的kubernetes初始固定链规则数据。 pkg/proxy/ipvs/proxier.go:1269 for _, set := range ipsetWithIptablesChain { if _, find := proxier.ipsetList[set.name]; find && !proxier.ipsetList[set.name].isEmpty() { args = append(args[:0], \"-A\", set.from) if set.protocolMatch != \"\" { args = append(args, \"-p\", set.protocolMatch) } args = append(args, \"-m\", \"comment\", \"--comment\", proxier.ipsetList[set.name].getComment(), \"-m\", \"set\", \"--match-set\", set.name, set.matchType, ) // -A $setFrom -p $prot -m comment --comment $commentStr // -m set --match-set $setName $setType -j $setTo writeLine(proxier.natRules, append(args, \"-j\", set.to)...) } } 依据ipsetWithIptablesChain定义生成以下创建固定链规则数据 KUBE-POSTROUTING匹配KUBE-LOOP-BACK ipset表则伪装地址 -A KUBE-POSTROUTING -m comment --comment \"Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\" -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE LoadBalancer服务类型相关规则 -A KUBE-SERVICES -m comment --comment \"Kubernetes service lb portal\" -m set --match-set KUBE-LOAD-BALANCER dst,dst -j KUBE-LOAD-BALANCER -A KUBE-LOAD-BALANCER -m comment --comment \"Kubernetes service load balancer ip + port for load balancer with sourceRange\" -m set --match-set KUBE-LOAD-BALANCER-FW dst,dst -j KUBE-FIREWALL -A KUBE-FIREWALL -m comment --comment \"Kubernetes service load balancer ip + port + source cidr for packet filter\" -m set --match-set KUBE-LOAD-BALANCER-SOURCE-CIDR dst,dst,src -j RETURN -A KUBE-FIREWALL -m comment --comment \"Kubernetes service load balancer ip + port + source IP for packet filter purpose\" -m set --match-set KUBE-LOAD-BALANCER-SOURCE-IP dst,dst,src -j RETURN -A KUBE-LOAD-BALANCER -m comment --comment \"Kubernetes service load balancer ip + port with externalTrafficPolicy=local\" -m set --match-set KUBE-LOAD-BALANCER-LOCAL dst,dst -j RETURN Nodeport服务类型相关规则 -A KUBE-NODE-PORT -p tcp -m comment --comment \"Kubernetes service load balancer ip + port with externalTrafficPolicy=local\" -m set --match-set KUBE-NODE-PORT-LOCAL-TCP dst -j RETURN -A KUBE-NODE-PORT -p tcp -m comment --comment \"Kubernetes nodeport TCP port for masquerade purpose\" -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ -A KUBE-NODE-PORT -p udp -m comment --comment \"Kubernetes nodeport UDP port with externalTrafficPolicy=local\" -m set --match-set KUBE-NODE-PORT-LOCAL-UDP dst -j RETURN -A KUBE-NODE-PORT -p udp -m comment --comment \"Kubernetes nodeport UDP port for masquerade purpose\" -m set --match-set KUBE-NODE-PORT-UDP dst -j KUBE-MARK-MASQ -A KUBE-SERVICES -p sctp -m comment --comment \"Kubernetes nodeport SCTP port for masquerade purpose\" -m set --match-set KUBE-NODE-PORT-SCTP dst -j KUBE-NODE-PORT -A KUBE-NODE-PORT -p sctp -m comment --comment \"Kubernetes nodeport SCTP port with externalTrafficPolicy=local\" -m set --match-set KUBE-NODE-PORT-LOCAL-SCTP dst -j RETURN kube-proxy启动参数\"--masquerade-all=true\"， 针对类型为clusterip服务生成相应的NAT表KUBE-SERVICES链规则数据，masquerade-all实现访问service ip流量伪装。 pkg/proxy/ipvs/proxier.go:1284 //ipset名称为\"KUBE-CLUSTER-IP\"不为空，即clusterip类型服务 if !proxier.ipsetList[kubeClusterIPSet].isEmpty() { args = append(args[:0], \"-A\", string(kubeServicesChain), \"-m\", \"comment\", \"--comment\", proxier.ipsetList[kubeClusterIPSet].getComment(), \"-m\", \"set\", \"--match-set\", kubeClusterIPSet, ) //当proxy配置为masqueradeAll=true if proxier.masqueradeAll { //nat表：-A KUBE-SERVICES -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ writeLine(proxier.natRules, append(args, \"dst,dst\", \"-j\", string(KubeMarkMasqChain))...) } else if len(proxier.clusterCIDR) > 0 { //当指定了clusterCIDR，针对非集群到服务VIP的流量masquerades规则 （dst,dst 目标ip:目标端口） // nat表：-A KUBE-SERVICES -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP dst,dst ! -s $clusterCIDR -j KUBE-MARK-MASQ writeLine(proxier.natRules, append(args, \"dst,dst\", \"! -s\", proxier.clusterCIDR, \"-j\", string(KubeMarkMasqChain))...) } else { // 所有来自服务VIP出流量masquerades规则 （src,dst 源ip:目标端口） // 如：VIP: to VIP: // nat表：-A KUBE-SERVICES -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP src,dst -j KUBE-MARK-MASQ writeLine(proxier.natRules, append(args, \"src,dst\", \"-j\", string(KubeMarkMasqChain))...) } } 为服务externalIPs专用ipset集(存在配置externalIPs的服务)生成相应的iptables NAT表规则数据。 pkg/proxy/ipvs/proxier.go:1311 if !proxier.ipsetList[kubeExternalIPSet].isEmpty() { // 为external IPs添加masquerade规则 args = append(args[:0], \"-A\", string(kubeServicesChain), \"-m\", \"comment\", \"--comment\", proxier.ipsetList[kubeExternalIPSet].getComment(), \"-m\", \"set\", \"--match-set\", kubeExternalIPSet, \"dst,dst\", ) // -A KUBE-SERVICES -m comment --comment \"Kubernetes service external ip + port for masquerade and filter purpose\" -m set --match-set KUBE-EXTERNAL-IP dst,dst -j KUBE-MARK-MASQ writeLine(proxier.natRules, append(args, \"-j\", string(KubeMarkMasqChain))...) // 允许external ips流量,而非来自本地网桥流量(如来自一个容器流量或本地处理的forward至服务流量) externalTrafficOnlyArgs := append(args, \"-m\", \"physdev\", \"!\", \"--physdev-is-in\", \"-m\", \"addrtype\", \"!\", \"--src-type\", \"LOCAL\") // -m set match-set KUBE-EXTERNAL-IP dst,dst -m PHYSDEV ! --physdev-is-in -m addrtype ! --src-type LOCAL -j ACCEPT writeLine(proxier.natRules, append(externalTrafficOnlyArgs, \"-j\", \"ACCEPT\")...) dstLocalOnlyArgs := append(args, \"-m\", \"addrtype\", \"--dst-type\", \"LOCAL\") // 识别与允许本地流量 // -m set match-set KUBE-EXTERNAL-IP dst,dst -m addrtype --dst-type LOCAL -j ACCEPT writeLine(proxier.natRules, append(dstLocalOnlyArgs, \"-j\", \"ACCEPT\")...) } acceptIPVSTraffic 在NAT表的KUBE-SERVICE链最后添加对所有目地址为ipvs虚拟服务的流量ACCEPT规则（此规则应放置于KUBE-SERVICE的最底部）。默认服务类型clusterip则生成规则-A KUBE-SERVICE -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT，如果有服务类型为LoadBalancer则生成规则-A KUBE-SERVICE -m set --match-set KUBE-LOAD-BALANCER dst,dst -j ACCEPT。 pkg/proxy/ipvs/proxier.go:1397 proxier.acceptIPVSTraffic() // -A KUBE-SERVICE -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT // -A KUBE-SERVICE -m set --match-set KUBE-LOAD-BALANCER dst,dst -j ACCEPT func (proxier *Proxier) acceptIPVSTraffic() { sets := []string{kubeClusterIPSet, kubeLoadBalancerSet} for _, set := range sets { var matchType string if !proxier.ipsetList[set].isEmpty() { switch proxier.ipsetList[set].SetType { case utilipset.BitmapPort: matchType = \"dst\" default: matchType = \"dst,dst\" //目标ip，目标端口 } writeLine(proxier.natRules, []string{ \"-A\", string(kubeServicesChain), \"-m\", \"set\", \"--match-set\", set, matchType, \"-j\", \"ACCEPT\", }...) } } } 增加masqueradeMark，允许NodePort流量转发(即使默认FORWARD规则策略不允许)。 pkg/proxy/ipvs/proxier.go:1361 // -A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000 -j ACCEPT writeLine(proxier.filterRules, \"-A\", string(KubeForwardChain), \"-m\", \"comment\", \"--comment\", `\"kubernetes forwarding rules\"`, \"-m\", \"mark\", \"--mark\", proxier.masqueradeMark, \"-j\", \"ACCEPT\", ) clusterCIDR被指定时生成两条filter表KUBE-FORWARD链规则数据，接受源或目标来自一个pod流量。(注：kube-proxy组件配置--cluster-dir参数指定集群中pod使用的网段) pkg/proxy/ipvs/proxier.go:1369 if len(proxier.clusterCIDR) != 0 { // 两条规则确保kubernetes forward规则定义的初始包被接受（clusterCIDR所指定的源或目标流量） // -A KUBE-FORWARD -s $clusterCIDR -m -comment --comment \"kubernetes forwarding conntrack pod source rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT writeLine(proxier.filterRules, \"-A\", string(KubeForwardChain), \"-s\", proxier.clusterCIDR, \"-m\", \"comment\", \"--comment\", `\"kubernetes forwarding conntrack pod source rule\"`, \"-m\", \"conntrack\", \"--ctstate\", \"RELATED,ESTABLISHED\", \"-j\", \"ACCEPT\", ) // -A KUBE-FORWARD -m -comment --comment \"kubernetes forwarding conntrack pod source rule\" -d $clusterCIDR -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT writeLine(proxier.filterRules, \"-A\", string(KubeForwardChain), \"-m\", \"comment\", \"--comment\", `\"kubernetes forwarding conntrack pod destination rule\"`, \"-d\", proxier.clusterCIDR, \"-m\", \"conntrack\", \"--ctstate\", \"RELATED,ESTABLISHED\", \"-j\", \"ACCEPT\", ) } 4.7. 刷新 iptables 规则 pkg/proxy/ipvs/proxier.go:1186 // 合并iptables规则 proxier.iptablesData.Reset() proxier.iptablesData.Write(proxier.natChains.Bytes()) proxier.iptablesData.Write(proxier.natRules.Bytes()) proxier.iptablesData.Write(proxier.filterChains.Bytes()) proxier.iptablesData.Write(proxier.filterRules.Bytes()) klog.V(5).Infof(\"Restoring iptables rules: %s\", proxier.iptablesData.Bytes()) // 基于iptables格式化规则数据，使用iptables-restore刷新iptables规则 err = proxier.iptables.RestoreAll(proxier.iptablesData.Bytes(), utiliptables.NoFlushTables, utiliptables.RestoreCounters) if err != nil { klog.Errorf(\"Failed to execute iptables-restore: %v\\nRules:\\n%s\", err, proxier.iptablesData.Bytes()) // Revert new local ports. utilproxy.RevertPorts(replacementPortsMap, proxier.portsMap) return } ipvs-mode Proxier整个逻辑实现已分析完，其关键逻辑即syncProxyRules(){…}内代码，其中还有一些细节技术未展开叙述,如几个关键的依赖底层技术ipset的实现runner、ipvs路由(VS/RS)操作基于netlink机制通迅机制的实现等，因篇幅过长，后续再看具体情况补充。 ~本文 END~ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 Ipvs-mode proxier1. 概述2. Ipvs-mode proxer 对象创建与初始化3. Proxier 服务与端点更新机制4. SyncProxyRules 同步 Proxy 规则4.1. 更新 service 与 endpoint变化信息4.2. 创建 kube 顶层链和连接信息4.3. Dummy 接口和 ipset 默认集创建4.4. 每个服务生成 ipvs 规则4.5. SyncIPSetEntries 同步 ipset 记录4.6. 创建 iptables 规则数据4.7. 刷新 iptables 规则"},"core/kube-proxy/userspace.html":{"url":"core/kube-proxy/userspace.html","title":"Userspace-mode Proxier","keywords":"","body":"Userspace-mode Proxier Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 "},"core/kubelet/":{"url":"core/kubelet/","title":"kubelet","keywords":"","body":"kubelet Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"around/":{"url":"around/","title":"概述","keywords":"","body":"周边项目源码分析 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-17 20:22:48 "},"around/client-go/":{"url":"around/client-go/","title":"client-go","keywords":"","body":"client-go client-go 部分我不打算从头到尾一点点讲。在核心组件源码的分析过程中有用到一些 client-go 中的关键特性时咱以专题形式逐渐添加进来。 1. 本章规划 Custom Controller 之 Informer (一) Custom Controller 之 Informer (二) Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 client-go1. 本章规划"},"around/client-go/informer.html":{"url":"around/client-go/informer.html","title":"Informer (一)","keywords":"","body":"Custom Controller 之 Informer (一) 概述 架构概览 client-go 相关模块 自定义控制器相关模块 第一步：reflector - List & Watch API Server Reflector 对象定义 ListAndWatch 第二步：watchHandler - add obj to delta fifo 第三、四、五步：Informer - pop obj from delta fifo、Add obj to store Controller processLoop clientState 第六步：Dispatch Event Handler functions sharedIndexInformer sharedProcessor processorListener processorListener.run() processorListener.pop() processorListener.add() sharedProcessor.distribute() sharedIndexInformer.HandleDeltas() sharedIndexInformer.Run() sharedProcessor.run() 1. 概述 本节标题写的是 Informer，不过我们的内容不局限于狭义的 Informer 部分，只是 Informer 最有代表性，其他的 Reflector 等也不好独立开来讲。 Informer 在很多组件的源码中可以看到，尤其是 kube-controller-manager (写这篇文章时我已经基本写完 kube-scheduler 的源码分析，着手写 kube-controller-manager 了，鉴于 controlelr 和 client-go 关联比较大，跳过来先讲讲典型的控制器工作流程中涉及到的 client-go 部分). Informer 是 client-go 中一个比较核心的工具，通过 Informer(实际我们用到的都不是单纯的 informer，而是组合了各种工具的 sharedInformerFactory) 我们可以轻松 List/Get 某个资源对象，可以监听资源对象的各种事件(比如创建和删除)然后触发回调函数，让我们能够在各种事件发生的时候能够作出相应的逻辑处理。举个例字，当 pod 数量变化的时候 deployment 是不是需要判断自己名下的 pod 数量是否还和预期的一样？如果少了是不是要考虑创建？ 2. 架构概览 自定义控制器的工作流程基本如下图所示，我们今天要分析图中上半部分的逻辑。(图片来自https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md) 我们开发自定义控制器的时候用到的“机制”主要定义在 client-go 的 tool/cache下： 先关注一下第一幅图中涉及到的一些 components： 2.1. client-go 相关模块 Reflector: Reflector 类型定义在 cache 包中(tools/cache/reflector.go:47)，它的作用是向 apiserver watch 特定的资源类型。这个功能通过其绑定的 ListAndWatch 方法实现。Watch 的资源可以是 in-build 的资源也可以是 custom 的资源。当 Reflector 通过 watch API 接收到存在新的资源对象实例的通知后，它使用相应的 list API 获取新创建的资源对象，然后 put 进 Delta Fifo 队列。这个步骤在 watchHandler 函数(tools/cache/reflector.go:268)中完成。 Informer: 一个定义在 cache 包中的基础 controller(tools/cache/controller.go:75) (一个 informer) 从 Delta Fifo 队列中 pop 出来资源对象实例(这个功能在 processLoop 中实现(tools/cache/controller.go:148))。这个 base controller 做的工作是保存这个对象用于后续检索处理用的，然后触发我们自己的控制器来处理这个对象。 Indexer: Indexer 提供的是 objects 之上的检索能力。Indexer 也定义在 cache 包中(tools/cache/index.go:27). 一个典型的检索使用方式是基于一个对象的 labels 创建索引。Indexer 可以基于各种索引函数维护索引。Indexer 使用一个线程安全的 store 来存储对象和其对应的 key. 还有一个默认函数 MetaNamespaceKeyFunc(tools/cache/store.go:76) 可以生成对象的 key，类似 / 格式来关联对应的对象。 2.2. 自定义控制器相关模块 Informer reference: 这是一个知道如何处理自定义资源对象的 Informer 实例的引用。自定义控制器需要创建合适的 Informer. Indexer reference: 这是一个知道如何处理自定义资源对象的 Indexer 实例的引用. 自定义控制器代码需要创建这个引用对象，然后用于检索资源对象用于后续的处理。 Base controller 提供了 NewIndexerInformer(tools/cache/controller.go:345) 函数来创建 Informer 和 Indexer. 在代码里我们可以直接调用这个函数或者使用工厂方法来创建 informer. Resource Event Handlers: 这是一个回调函数，在 Informer 想要分发一个对象给控制器的时候会调用这个函数。典型的用法是写一个函数来获取分发过来的对象的 key，将 key 放入队列中等待进一步的处理。 Work queue: 这个队列是在自己的控制器代码中创建的，用来解耦一个对象的分发和处理过程。Resource event handler 函数会被写成提取分发来的对象的 key，然后将这个 key 添加到 work queue 里面。 Process Item 这是我们在自己代码中实现的用来处理 work queue 中拿到的 items 的函数。这里可以有一个或多个函数来处理具体的过程，这个函数的典型用法是使用 Indexer 索引或者一个 Listing wrapper 来根据相应的 key 检索对象。 下面我们根据图中这几个步骤来跟源码。 3. 第一步：reflector - List & Watch API Server Reflector 会监视特定的资源，将变化写入给定的存储中，也就是 Delta FIFO queue. 3.1. Reflector 对象定义 Reflector 的中文含义是反射器，我们先看一下类型定义： tools/cache/reflector.go:47 type Reflector struct { name string metrics *reflectorMetrics expectedType reflect.Type store Store listerWatcher ListerWatcher period time.Duration resyncPeriod time.Duration ShouldResync func() bool clock clock.Clock lastSyncResourceVersion string lastSyncResourceVersionMutex sync.RWMutex } reflector.go中主要就 Reflector 这个 struct 和相关的一些函数： 3.2. ListAndWatch ListAndWatch 首先 list 所有 items，获取当前的资源版本信息，然后使用这个版本信息来 watch(也就是从这个版本开始的所有资源变化会被关注)。我们看一下这里的 ListAndWatch 方法主要逻辑： tools/cache/reflector.go:168 func (r *Reflector) ListAndWatch(stopCh 4. 第二步：watchHandler - add obj to delta fifo 前面讲到 ListAndWatch 函数的最后一步逻辑是 watchHandler，在 ListAndWatch 中先是更新了 Delta FIFO 中的 item，然后 watch 资源对象，最后交给 watchHandler 处理，所以 watchHandler 基本可以猜到是将有变化的资源添加到 Delta FIFO 中，我们具体来看。 tools/cache/reflector.go:287 func (r *Reflector) watchHandler(w watch.Interface, resourceVersion *string, errc chan error, stopCh 5. 第三、四、五步：Informer - pop obj from delta fifo、Add obj to store 先看 Controller 是什么 5.1. Controller Informer 会实现 Controller 接口，这个接口长这样： tools/cache/controller.go:82 type Controller interface { Run(stopCh 和这个 Controller 对应的有一个基础 controller 实现： tools/cache/controller.go:75 type controller struct { config Config reflector *Reflector reflectorMutex sync.RWMutex clock clock.Clock } controller 类型结构如下： 可以看到主要对外暴露的逻辑是 Run() 方法，还有一个重点 processLoop() 其实也在 Run() 里面被调用，我们看一下 Run() 中的逻辑： tools/cache/controller.go:100 func (c *controller) Run(stopCh 5.2. processLoop tools/cache/controller.go:148 func (c *controller) processLoop() { for { // 主要逻辑 obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) // 异常处理 } } 这里的 Queue 就是 Delta FIFO，Pop 是个阻塞方法，内部实现时会逐个 pop 队列中的数据，交给 PopProcessFunc 处理。我们先不看 Pop 的实现，关注一下 PopProcessFunc 是如何处理 Pop 中从队列拿出来的 item 的。 PopProcessFunc 是一个类型，如下： type PopProcessFunc func(interface{}) error 所以这里只是一个类型转换，我们关注c.config.Process就行： tools/cache/controller.go:367 Process: func(obj interface{}) error { for _, d := range obj.(Deltas) { switch d.Type { // 更新、添加、同步、删除等操作 case Sync, Added, Updated: if old, exists, err := clientState.Get(d.Object); err == nil && exists { if err := clientState.Update(d.Object); err != nil { return err } h.OnUpdate(old, d.Object) } else { if err := clientState.Add(d.Object); err != nil { return err } h.OnAdd(d.Object) } case Deleted: if err := clientState.Delete(d.Object); err != nil { return err } h.OnDelete(d.Object) } } return nil }, 这里涉及到2个点： clientState ResourceEventHandler (h) 我们后面会一一分析到。 5.3. clientState 前面说到 clientState，这个变量的初始化是clientState := NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers) NewIndexer 代码如下： tools/cache/store.go:239 func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer { return &cache{ cacheStorage: NewThreadSafeStore(indexers, Indices{}), keyFunc: keyFunc, } } tools/cache/index.go:27 type Indexer interface { Store Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(indexName string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers AddIndexers(newIndexers Indexers) error } 顺带看一下 NewThreadSafeStore() tools/cache/thread_safe_store.go:298 func NewThreadSafeStore(indexers Indexers, indices Indices) ThreadSafeStore { return &threadSafeMap{ items: map[string]interface{}{}, indexers: indexers, indices: indices, } } 然后关注一下 Process 中的err := clientState.Add(d.Object)的 Add() 方法： tools/cache/store.go:123 func (c *cache) Add(obj interface{}) error { // 计算key；一般是namespace/name key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } // Add c.cacheStorage.Add(key, obj) return nil } cacheStorage 是一个 ThreadSafeStore 实例，这个 Add() 代码如下： tools/cache/thread_safe_store.go:68 func (c *threadSafeMap) Add(key string, obj interface{}) { c.lock.Lock() defer c.lock.Unlock() // 拿出 old obj oldObject := c.items[key] // 写入 new obj c.items[key] = obj // 更新索引，有一堆逻辑 c.updateIndices(oldObject, obj, key) } 这块逻辑先分析到这里，后面关注 threadSafeMap 实现的时候再继续深入。 6. 第六步：Dispatch Event Handler functions 我们先看一个接口 SharedInformer 6.1. sharedIndexInformer tools/cache/shared_informer.go:43 type SharedInformer interface { AddEventHandler(handler ResourceEventHandler) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) GetStore() Store GetController() Controller Run(stopCh SharedInformer 有一个共享的 data cache，能够分发 changes 通知到缓存，到通过 AddEventHandler 注册了的 listerners. 当你接收到一个通知，缓存的内容能够保证至少和通知中的一样新。 再看一下 SharedIndexInformer 接口： tools/cache/shared_informer.go:66 type SharedIndexInformer interface { SharedInformer // AddIndexers add indexers to the informer before it starts. AddIndexers(indexers Indexers) error GetIndexer() Indexer } 相比 SharedInformer 增加了一个 Indexer. 然后看具体的实现 sharedIndexInformer 吧： tools/cache/shared_informer.go:127 type sharedIndexInformer struct { indexer Indexer controller Controller processor *sharedProcessor cacheMutationDetector CacheMutationDetector listerWatcher ListerWatcher objectType runtime.Object resyncCheckPeriod time.Duration defaultEventHandlerResyncPeriod time.Duration clock clock.Clock started, stopped bool startedLock sync.Mutex blockDeltas sync.Mutex } 这个类型内包了很多我们前面看到过的对象，indexer、controller、listeratcher 都不陌生，我们看这里的 processor 是做什么的： 6.2. sharedProcessor 类型定义如下： tools/cache/shared_informer.go:375 type sharedProcessor struct { listenersStarted bool listenersLock sync.RWMutex listeners []*processorListener syncingListeners []*processorListener clock clock.Clock wg wait.Group } 这里的重点明显是 listeners 属性了，我们继续看 listeners 的类型中的 processorListener： 6.2.1. processorListener tools/cache/shared_informer.go:466 type processorListener struct { nextCh chan interface{} addCh chan interface{} handler ResourceEventHandler // 一个 ring buffer，保存未分发的通知 pendingNotifications buffer.RingGrowing // …… } processorListener 主要有2个方法： run() pop() 6.2.2. processorListener.run() 先看一下这个 run 做了什么： tools/cache/shared_informer.go:540 func (p *processorListener) run() { stopCh := make(chan struct{}) wait.Until(func() { // 一分钟执行一次这个 func() // 一分钟内的又有几次重试 err := wait.ExponentialBackoff(retry.DefaultRetry, func() (bool, error) { // 等待信号 nextCh for next := range p.nextCh { // notification 是 next 的实际类型 switch notification := next.(type) { // update case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) // add case addNotification: p.handler.OnAdd(notification.newObj) // delete case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fmt.Errorf(\"unrecognized notification: %#v\", next)) } } return true, nil }) if err == nil { close(stopCh) } }, 1*time.Minute, stopCh) } 这个 run 过程不复杂，等待信号然后调用 handler 的增删改方法做对应的处理逻辑。case 里的 Notification 再看一眼： tools/cache/shared_informer.go:176 type updateNotification struct { oldObj interface{} newObj interface{} } type addNotification struct { newObj interface{} } type deleteNotification struct { oldObj interface{} } 另外注意到for next := range p.nextCh是下面的 case 执行的前提，也就是说触发点是 p.nextCh，我们接着看 pop 过程(这里的逻辑不简单，可能得多花点精力) 6.2.3. processorListener.pop() tools/cache/shared_informer.go:510 func (p *processorListener) pop() { defer utilruntime.HandleCrash() defer close(p.nextCh) // Tell .run() to stop // 这个 chan 是没有初始化的 var nextCh chan 这里的 pop 逻辑的入口是，我们继续向上找一下这个 addCh 的来源： 6.2.4. processorListener.add() tools/cache/shared_informer.go:506 func (p *processorListener) add(notification interface{}) { p.addCh 这个 add() 方法又在哪里被调用呢？ 6.2.5. sharedProcessor.distribute() tools/cache/shared_informer.go:400 func (p *sharedProcessor) distribute(obj interface{}, sync bool) { p.listenersLock.RLock() defer p.listenersLock.RUnlock() if sync { for _, listener := range p.syncingListeners { listener.add(obj) } } else { for _, listener := range p.listeners { listener.add(obj) } } } 这个方法逻辑比较简洁，分发对象。我们继续看哪里进入的 distribute： 6.3. sharedIndexInformer.HandleDeltas() tools/cache/shared_informer.go:344 func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { // 根据 DeltaType 选择 case case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil && exists { // indexer 更新的是本地 store if err := s.indexer.Update(d.Object); err != nil { return err } // 前面分析的 distribute；update s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } // 前面分析的 distribute；add s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } // 前面分析的 distribute；delete s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 继续往前看代码逻辑。 6.4. sharedIndexInformer.Run() tools/cache/shared_informer.go:189 func (s *sharedIndexInformer) Run(stopCh 看到这里已经挺和谐了，在 sharedIndexInformer 的 Run() 方法中先是创建一个 DeltaFIFO，然后和 lw 一起初始化 cfg，利用 cfg 创建 controller，最后 Run 这个 controller，也就是最基础的 informer. 在这段代码里我们还注意到有一步是s.processor.run，我们看一下这个 run 的逻辑。 6.4.1. sharedProcessor.run() tools/cache/shared_informer.go:415 func (p *sharedProcessor) run(stopCh 撇开细节，可以看到这里调用了内部所有 listener 的 run() 和 pop() 方法，和前面的分析呼应上了。 到这里，我们基本讲完了自定义 controller 的时候 client-go 里相关的逻辑，也就是图中的上半部分： Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 Custom Controller 之 Informer (一)1. 概述2. 架构概览2.1. client-go 相关模块2.2. 自定义控制器相关模块3. 第一步：reflector - List & Watch API Server3.1. Reflector 对象定义3.2. ListAndWatch4. 第二步：watchHandler - add obj to delta fifo5. 第三、四、五步：Informer - pop obj from delta fifo、Add obj to store5.1. Controller5.2. processLoop5.3. clientState6. 第六步：Dispatch Event Handler functions6.1. sharedIndexInformer6.2. sharedProcessor6.2.1. processorListener6.2.2. processorListener.run()6.2.3. processorListener.pop()6.2.4. processorListener.add()6.2.5. sharedProcessor.distribute()6.3. sharedIndexInformer.HandleDeltas()6.4. sharedIndexInformer.Run()6.4.1. sharedProcessor.run()"},"around/client-go/informer2.html":{"url":"around/client-go/informer2.html","title":"Informer(二)","keywords":"","body":"Custom Controller 之 Informer(二) In Edit 1. 概述 2. GenericInformer informers/generic.go:58 type GenericInformer interface { Informer() cache.SharedIndexInformer Lister() cache.GenericLister } type genericInformer struct { informer cache.SharedIndexInformer resource schema.GroupResource } 2.1. SharedIndexInformer 2.2. GenericLister 3. SharedInformerFactory informers/factory.go:185 type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh 3.1. apps.Interface 从 apps.Interface 一路到 DeploymentInformer informers/apps/interface.go:29 type Interface interface { // V1 provides access to shared informers for resources in V1. V1() v1.Interface // V1beta1 provides access to shared informers for resources in V1beta1. V1beta1() v1beta1.Interface // V1beta2 provides access to shared informers for resources in V1beta2. V1beta2() v1beta2.Interface } 3.1.1. v1.Interface informers/apps/v1/interface.go:26 type Interface interface { // ControllerRevisions returns a ControllerRevisionInformer. ControllerRevisions() ControllerRevisionInformer // DaemonSets returns a DaemonSetInformer. DaemonSets() DaemonSetInformer // Deployments returns a DeploymentInformer. Deployments() DeploymentInformer // ReplicaSets returns a ReplicaSetInformer. ReplicaSets() ReplicaSetInformer // StatefulSets returns a StatefulSetInformer. StatefulSets() StatefulSetInformer } 3.1.2. DeploymentInformer informers/apps/v1/deployment.go:36 type DeploymentInformer interface { Informer() cache.SharedIndexInformer Lister() v1.DeploymentLister } deploymentInformer informers/apps/v1/deployment.go:41 type deploymentInformer struct { factory internalinterfaces.SharedInformerFactory tweakListOptions internalinterfaces.TweakListOptionsFunc namespace string } informers/apps/v1/deployment.go:79 func (f *deploymentInformer) defaultInformer(client kubernetes.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer { return NewFilteredDeploymentInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions) } func (f *deploymentInformer) Informer() cache.SharedIndexInformer { return f.factory.InformerFor(&appsv1.Deployment{}, f.defaultInformer) } func (f *deploymentInformer) Lister() v1.DeploymentLister { return v1.NewDeploymentLister(f.Informer().GetIndexer()) } 3.2. sharedInformerFactory informers/factory.go:53 type sharedInformerFactory struct { client kubernetes.Interface namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc lock sync.Mutex defaultResync time.Duration customResync map[reflect.Type]time.Duration informers map[reflect.Type]cache.SharedIndexInformer // startedInformers is used for tracking which informers have been started. // This allows Start() to be called multiple times safely. startedInformers map[reflect.Type]bool } 3.2.1. kubernetes.Interface 3.2.2. Clientset type Clientset struct { *discovery.DiscoveryClient admissionregistrationV1alpha1 *admissionregistrationv1alpha1.AdmissionregistrationV1alpha1Client admissionregistrationV1beta1 *admissionregistrationv1beta1.AdmissionregistrationV1beta1Client appsV1beta1 *appsv1beta1.AppsV1beta1Client appsV1beta2 *appsv1beta2.AppsV1beta2Client appsV1 *appsv1.AppsV1Client // …… coreV1 *corev1.CoreV1Client eventsV1beta1 *eventsv1beta1.EventsV1beta1Client extensionsV1beta1 *extensionsv1beta1.ExtensionsV1beta1Client // …… } 3.2.3. appsv1.AppsV1Client kubernetes/typed/apps/v1/apps_client.go:38 type AppsV1Client struct { restClient rest.Interface } 3.2.4. rest.Interface type Interface interface { GetRateLimiter() flowcontrol.RateLimiter Verb(verb string) *Request Post() *Request Put() *Request Patch(pt types.PatchType) *Request Get() *Request Delete() *Request APIVersion() schema.GroupVersion } 3.2.5. RESTClient rest/client.go:61 type RESTClient struct { // base is the root URL for all invocations of the client base *url.URL versionedAPIPath string contentConfig ContentConfig serializers Serializers createBackoffMgr func() BackoffManager Throttle flowcontrol.RateLimiter Client *http.Client } Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-20 14:58:28 Custom Controller 之 Informer(二)1. 概述2. GenericInformer2.1. SharedIndexInformer2.2. GenericLister3. SharedInformerFactory3.1. apps.Interface3.1.1. v1.Interface3.1.2. DeploymentInformer3.2. sharedInformerFactory3.2.1. kubernetes.Interface3.2.2. Clientset3.2.3. appsv1.AppsV1Client3.2.4. rest.Interface3.2.5. RESTClient"}}