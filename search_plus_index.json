{"./":{"url":"./","title":"前言","keywords":"","body":" 1、关于本书 本书将系统讲解kubernetes的核心组件源码，附带介绍相关的周边项目，比如client-go等。 建议通过公众号CloudGeek接收更新消息，通过github pages阅读本书。 2、内容更新 本项目会不定期更新，一般会在一周内更新一节；更新内容将同步发到公众号CloudGeek、博客园CloudGeek等。细微的更新，比如错别字修改等不会同步到其他所有平台。 3、版本说明 本书基于：v1.13版本源码讲解。 4、协议 本书使用Apache License 2.0协议，但是保留出版图书的权利。 5、贡献 欢迎参与本书编写！如果你对开源项目源码分析感兴趣，可以和我联系，如果你擅长某个项目，可以主导一个章节的编写。 但是得提前告诉你我是一个有洁癖的人，包括代码洁癖，文字洁癖等，所以接受不了太随意的风格哦～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-22 18:18:20 "},"prepare/":{"url":"prepare/","title":"k8s源码分析准备工作","keywords":"","body":"k8s源码分析准备工作 1. 概述 准备工作分为2部分： 源码准备 测试环境搭建 源码准备阶段主要介绍k8s源码的获取与本地golang编译环境配置等；调试环境搭建是介绍如何准备一个k8s环境，用于后续组件的代码调试。调试环境可以在大体学习完一个组件源码后进行，用于验证自己在看源码过程中的一些想法和疑惑。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-24 18:16:02 k8s源码分析准备工作1. 概述"},"prepare/get-code.html":{"url":"prepare/get-code.html","title":"源码准备","keywords":"","body":"源码准备 环境准备 源码下载 源码编译 IDE 1. 环境准备 操作系统：我们使用Linux作为k8s源码分析和调试环境，fedora、centos、ubuntu都行，我这里使用fedora； golang相关： GOROOT=/usr/local/lib/golang GOPATH=/root/go go version go1.10.3 linux/amd64 2. 源码下载 mkdir -p /root/go/src/k8s.io cd /root/go/src/k8s.io/ git clone https://github.com/kubernetes/kubernetes.git 下载后本地目录： 3. 源码编译 我们先看一下几个主要的目录： 目录名 用途 cmd 每个组件代码入口（main函数） pkg 各个组件的具体功能实现 staging 已经分库的项目 vendor 依赖 考虑到国内网络环境等因素，我们不使用容器化方式构建。我们尝试在kubernetes项目cmd目录下构建一个组件（执行路径：/root/go/src/k8s.io/kubernetes/cmd/kube-scheduler）： 这里需要注意一下，如果报依赖错误，找不到k8s.io下的某些项目，就到vendor下看一下软链接是不是都还在，如下： 注意到k8s是使用这种方式解决k8s.io下的依赖问题的，如果我们在windows下下载的代码，然后copy到linux下，就很容易遇到这些软链接丢失的情况，导致go找不到依赖，编译失败。 4. IDE 我们使用Goland看代码： 最后，别忘了在正式研读源码前切换到release-1.13分支～ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-18 16:27:26 源码准备1. 环境准备2. 源码下载3. 源码编译4. IDE"},"prepare/debug-environment.html":{"url":"prepare/debug-environment.html","title":"测试环境搭建","keywords":"","body":"测试环境搭建 （k8s-1.13版本单节点环境搭建） 1. 概述 大家注意哦，不一定要先搭建好环境再看源码，大可以先看一个组件，感觉差不多理解了，想要run一把，想要改几行试试的时候回过头来搭建k8s环境。 当然，大家开始看源码的时候，我相信各位都是搭建过不少次k8s集群，敲过N多次kubectl命令了，所以下面我不会解释太基础的命令是做什么的。 今天我们要做的是搭建一个单机版的k8s环境，用于后面的学习。虽然k8s的环境搭建没有openstack来的复杂，但是由于网络等乱七八糟的问题，在国内手动搭建一个五脏俱全的k8s也不算太容易，一个不小心就会被乱七八遭的障碍磨灭兴趣。今天看看我能不能让大家觉得这个过程无痛吧～ 2. kubeadm简介 选择一个好的工具很重要！大家在刚开始学习k8s的时候应该都用二进制文件一步一步搭建过集群吧，那个过程是不是很酸爽？手动一步一步搭建环境对于初学者来说确实大有裨益，可以了解各个组件的细节。我已经懒掉了，我决定从众多k8s自动化安装方案中选择一个来搭建这次的k8s环境。 kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，这不是一个单独的项目哦，我们在kubernetes源码里可以看到这个组件（kubernetes/cmd/kubeadm/）： kubeadm这个工具可以通过简单的kubeadm init和kubeadm join命令来创建一个kubernetes集群，kubeadm提供的其他命令都比较通俗易懂： kubeadm init 启动一个master节点； kubeadm join 启动一个node节点，加入master； kubeadm upgrade 更新集群版本； kubeadm config 从1.8.0版本开始已经用处不大，可以用来view一下配置； kubeadm token 管理kubeadm join的token； kubeadm reset 把kubeadm init或kubeadm join做的更改恢复原状； kubeadm version打印版本信息； kubeadm alpha预览一些alpha特性的命令。 关于kubeadm的成熟度官方有一个表格： Area Maturity Level Command line UX GA Implementation GA Config file API beta CoreDNS GA kubeadm alpha subcommands alpha High availability alpha DynamicKubeletConfig alpha Self-hosting alpha 主要特性其实都已经GA了，虽然还有一些小特性仍处于活跃开发中，但是整体已经接近准生产级别了。对于我们的场景来说用起来已经绰绰有余！ 3. 操作系统准备 我们先使用一个机子来装，后面需要拓展可以增加节点，使用kubeadm join可以很轻松扩展集群。 3.1. 系统信息 内存：2G CPU：2 磁盘：20G 系统版本和内核版本如下所示，大家不需要严格和我保持一致，不要使用太旧的就行了。 # cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) # uname -r 3.10.0-862.9.1.el7.x86_64 3.2. 配置selinux和firewalld # Set SELinux in permissive mode setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config # Stop and disable firewalld systemctl enable firewalld --now 3.3. 系统参数与内核模块 # 修改内核参数 cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 加载内核模块 modprobe br_netfilter lsmod | grep br_netfilter 3.4. 配置yum源 # base repo cd /etc/yum.repos.d mv CentOS-Base.repo CentOS-Base.repo.bak curl -o CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/gpgcheck=1/gpgcheck=0/g' /etc/yum.repos.d/CentOS-Base.repo # docker repo curl -o docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # k8s repo cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # update cache yum clean all yum makecache yum repolist 最终我们可以看到这些repo： 3.5. 禁用swap swapoff -a echo \"vm.swappiness = 0\">> /etc/sysctl.conf sysctl -p 4. 安装docker 先看一下有哪些可用版本：yum list docker-ce --showduplicates | sort -r 我们选择一个版本安装： yum install docker-ce- 这里我选择18.03.1，所以我用的命令是： yum install docker-ce-18.06.3.ce 可以用rpm命令看一下docker-ce这个rpm包带来了哪些文件： 启动docker： systemctl enable docker --now 查看服务状态： systemctl status docker 5. 安装kubeadm、kubelet和kubectl kubeadm不管kubelet和kubectl，所以我们需要手动安装kubelet和kubectl： yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 如果你看到这个教程的时候yum源里已经有了1.14版本的kubeadm，那么要么你装新版本，要么通过上面装docker同样的方式指定1.13版本安装，我这里使用的是1.13.3。 最后启动kubelet： systemctl enable --now kubelet 6. 镜像准备 为了解决国内普遍访问不到k8s.gcr.io的问题，我们从mirrorgooglecontainers下载image，然后打个tag来绕过网络限制： docker pull docker.io/mirrorgooglecontainers/kube-apiserver-amd64:v1.13.3 docker tag docker.io/mirrorgooglecontainers/kube-apiserver-amd64:v1.13.3 k8s.gcr.io/kube-apiserver:v1.13.3 docker pull docker.io/mirrorgooglecontainers/kube-controller-manager-amd64:v1.13.3 docker tag docker.io/mirrorgooglecontainers/kube-controller-manager-amd64:v1.13.3 k8s.gcr.io/kube-controller-manager:v1.13.3 docker pull docker.io/mirrorgooglecontainers/kube-scheduler-amd64:v1.13.3 docker tag docker.io/mirrorgooglecontainers/kube-scheduler-amd64:v1.13.3 k8s.gcr.io/kube-scheduler:v1.13.3 docker pull docker.io/mirrorgooglecontainers/kube-proxy-amd64:v1.13.3 docker tag docker.io/mirrorgooglecontainers/kube-proxy-amd64:v1.13.3 k8s.gcr.io/kube-proxy:v1.13.3 docker pull docker.io/mirrorgooglecontainers/pause-amd64:3.1 docker tag docker.io/mirrorgooglecontainers/pause-amd64:3.1 k8s.gcr.io/pause:3.1 docker pull docker.io/mirrorgooglecontainers/etcd-amd64:3.2.24 docker tag docker.io/mirrorgooglecontainers/etcd-amd64:3.2.24 k8s.gcr.io/etcd:3.2.24 docker pull docker.io/coredns/coredns:1.2.6 docker tag docker.io/coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6 7. 安装k8s master kubeadm init --pod-network-cidr=10.100.0.0/16 如上，跑kubeadm init命令后等几分钟。 如果遇到报错，对着错误信息修正一下。比如没有关闭swap会遇到error，系统cpu不够会遇到error，网络不通等等都会出错，仔细看一下错误信息一般都好解决～ 跑完上面的init命令后，会看到类似如下的输出： Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.19.100:6443 --token i472cq.tr9a81qxnyqc5zj2 --discovery-token-ca-cert-hash sha256:acba957db29e0efbffe2cf4e484521b3b7e0f9d5c2ab7f9db68a5e31565d0d66 上面输出告诉我们还需要做一些工作： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml 稍等一会，应该可以看到node状态变成ready： # kubectl get node NAME STATUS ROLES AGE VERSION kube-master Ready master 23m v1.13.3 如果你的环境迟迟都是NotReady状态，可以kubectl get pod -n kube-system看一下pod状态，一般可以发现问题，比如flannel的镜像下载失败啦～ 当node Ready的时候，我们可以看到pod也全部ready了： 再看一下核心组件状态： 最后注意到kube-master这个node上有一个Taint： # kubectl describe node kube-master | grep Taints Taints: node-role.kubernetes.io/master:NoSchedule 默认master节点是不跑业务pod的，我们暂时只有一个node，所以先去掉这个Taint： # kubectl taint node kube-master node-role.kubernetes.io/master- node/kube-master untainted # kubectl describe node kube-master | grep Taints Taints: 8. 环境验证 我们来跑一个pod，证明环境正常可用了： 写一个yaml： apiVersion: apps/v1 kind: Deployment metadata: name: mytomcat spec: replicas: 1 selector: matchLabels: app: mytomcat template: metadata: name: mytomcat labels: app: mytomcat spec: containers: - name: mytomcat image: tomcat:8 ports: - containerPort: 8080 如上内容保存为tomcat-deploy.yaml，执行kubectl create -f tomcat-deploy.yaml，然后看pod状态： 确认了，是熟悉的Running，哈哈，基本算大功告成了！最后我们看一下tomcat服务能不能访问到： 很完美，如果加个svc配置，就能够通过浏览器访问到汤姆猫界面了！ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-25 19:03:24 测试环境搭建1. 概述2. kubeadm简介3. 操作系统准备3.1. 系统信息3.2. 配置selinux和firewalld3.3. 系统参数与内核模块3.4. 配置yum源3.5. 禁用swap4. 安装docker5. 安装kubeadm、kubelet和kubectl6. 镜像准备7. 安装k8s master8. 环境验证"},"core/":{"url":"core/","title":"概述","keywords":"","body":"核心组件源码分析 1. 概述 核心组件的源码分析主要包括： scheduler apiserver proxy kubelet controller-manager 在分析第一个组件的时候会穿插一些整体性的介绍，比如源码组织啊、使用的一些三方库啊……；后面有些组件比较依赖其他较大的项目的，比如一个核心组件依赖于对client-go项目的理解，那就会先介绍client-go，当然client-go的介绍不会混在核心组件分析的章节中，我会单独分一个大类“周边项目源码分析”中。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:28:56 核心组件源码分析1. 概述"},"core/scheduler/":{"url":"core/scheduler/","title":"scheduler","keywords":"","body":"scheduler Scheduler部分我们先从设计原理上介绍，然后分析源码，再准备环境调试，最后修改一下源码，实现一个自己的调度器。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-21 17:10:40 "},"core/scheduler/desigh.html":{"url":"core/scheduler/desigh.html","title":"调度器设计","keywords":"","body":"调度器设计 概述 源码层级 调度算法 Predicates 和 priorities 策略 Scheduler 的拓展性 调度策略的修改 1. 概述 我们先整体了解一下Scheduler的设计原理，然后再看这些过程是如何用代码实现的。关于调度器的设计在官网有介绍，我下面结合官网给的说明，简化掉不影响理解的复杂部分，和大家介绍一下Scheduler的工作过程。 英文还可以的小伙伴们可以看一下官网的介绍先：scheduler.md 官网有一段描述如下： The Kubernetes scheduler runs as a process alongside the other master components such as the API server. Its interface to the API server is to watch for Pods with an empty PodSpec.NodeName, and for each Pod, it posts a binding indicating where the Pod should be scheduled. 简单翻译一下，也就是说Scheduler是一个跑在其他组件边上的独立程序，对接Apiserver寻找PodSpec.NodeName为空的Pod，然后用post的方式发送一个api调用，指定这些pod应该跑在哪个node上。 通俗地说，就是scheduler是相对独立的一个组件，主动访问api server，寻找等待调度的pod，然后通过一系列调度算法寻找哪个node适合跑这个pod，然后将这个pod和node的绑定关系发给api server，从而完成了调度的过程。 2. 源码层级 从高level看，scheduler的源码可以分为3层： cmd/kube-scheduler/scheduler.go: main() 函数入口位置，在scheduler过程开始被调用前的一系列初始化工作。 pkg/scheduler/scheduler.go: 调度框架的整体逻辑，在具体的调度算法之上的框架性的代码。 pkg/scheduler/core/generic_scheduler.go: 具体的计算哪些node适合跑哪些pod的算法。 3. 调度算法 调度过程整体如下图所示（官文里这个图没对齐，逼疯强迫症了！！！当然由于中文显示的问题，下图有中文的行也没法完全对齐，这个地方让我很抓狂。。。）： 对于一个给定的pod +---------------------------------------------+ | 可用于调度的nodes如下： | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 初步过滤: node 3 资源不足 +----------------------+----------------------+ | v +----------------------+----------------------+ | 剩下的nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | +----------------------+----------------------+ | v +----------------------+----------------------+ 优先级算法计算结果: node 1: 分数=2 node 2: 分数=5 +----------------------+----------------------+ | v 选择分值最高的节点 = node 2 Scheduler为每个pod寻找一个适合其运行的node，大体分成三步： 通过一系列的“predicates”过滤掉不能运行pod的node，比如一个pod需要500M的内存，有些节点剩余内存只有100M了，就会被剔除； 通过一系列的“priority functions”给剩下的node排一个等级，分出三六九等，寻找能够运行pod的若干node中最合适的一个node； 得分最高的一个node，也就是被“priority functions”选中的node胜出了，获得了跑对应pod的资格。 4. Predicates 和 priorities 策略 Predicates是一些用于过滤不合适node的策略 . Priorities是一些用于区分node排名（分数）的策略（作用在通过predicates过滤的node上）. K8s默认内建了一些predicates 和 priorities 策略，官方文档介绍地址： scheduler_algorithm.md. Predicates 和 priorities 的代码分别在： pkg/scheduler/algorithm/predicates/predicates.go pkg/scheduler/algorithm/priorities. 5. Scheduler 的拓展性 我们可以选择哪些预置策略生效，也可以添加自己的策略。几个月前我司有个奇葩调度需求，当时我就是通过增加一个priorities策略，然后重新编译了一个Scheduler来实现的需求。 6. 调度策略的修改 默认调度策略是通过defaultPredicates() 和 defaultPriorities()函数定义的，源码在 pkg/scheduler/algorithmprovider/defaults/defaults.go，我们可以通过命令行flag --policy-config-file来覆盖默认行为。所以我们可以通过配置文件的方式或者修改pkg/scheduler/algorithm/predicates/predicates.go /pkg/scheduler/algorithm/priorities，然后注册到defaultPredicates()/defaultPriorities()来实现。配置文件类似下面这个样子： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\" : [ {\"name\" : \"PodFitsHostPorts\"}, {\"name\" : \"PodFitsResources\"}, {\"name\" : \"NoDiskConflict\"}, {\"name\" : \"NoVolumeZoneConflict\"}, {\"name\" : \"MatchNodeSelector\"}, {\"name\" : \"HostName\"} ], \"priorities\" : [ {\"name\" : \"LeastRequestedPriority\", \"weight\" : 1}, {\"name\" : \"BalancedResourceAllocation\", \"weight\" : 1}, {\"name\" : \"ServiceSpreadingPriority\", \"weight\" : 1}, {\"name\" : \"EqualPriority\", \"weight\" : 1} ], \"hardPodAffinitySymmetricWeight\" : 10, \"alwaysCheckAllPredicates\" : false } ok，看到这里大伙应该在流程上对Scheduler的原理有个感性的认识了，下一节我们就开始看一下Scheduler源码是怎么写的。 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-22 10:14:07 调度器设计1. 概述2. 源码层级3. 调度算法4. Predicates 和 priorities 策略5. Scheduler 的拓展性6. 调度策略的修改"},"core/scheduler/before-scheduler-run.html":{"url":"core/scheduler/before-scheduler-run.html","title":"调度程序启动前逻辑","keywords":"","body":"调度程序启动前逻辑 概述 cobra和main cobra是啥 使用cobra Scheduler的main 1. 概述 前面提到过scheduler程序可以分为三层，第一层是调度器启动前的逻辑，包括命令行参数解析、参数校验、调度器初始化等一系列逻辑。这个部分我不会太详细地介绍，因为这些代码位于调度框架之前，相对比较枯燥无趣，讲多了磨灭大伙对源码的兴趣～ 2. cobra和main 剧透一下先，如果你之前没有用过cobra，那么在第一次见到cobra之后，很可能以后你自己写的程序，开发的小工具会全部变成cobra风格。我最近半年写的命令行程序就全部是基于cobra+pflag的。cobra有多优雅呢，且听我慢慢道来～ 2.1. cobra是啥 从github上我们可以找到这个项目，截至今天已经有上万个star，一百多个contributors，可见来头不小！Cobra官方描述是： Cobra is both a library for creating powerful modern CLI applications as well as a program to generate applications and command files. 也就是这个意思：Cobra既是一个创建强大的现代化命令行程序的库，又是一个用于生成应用和命令行文件的程序。有很多流行的Go项目用了Cobra，其中当然包括我们最最熟知的k8s和docker，大致列出来有这些： Kubernetes Hugo rkt etcd Moby (former Docker) Docker (distribution) OpenShift Delve GopherJS CockroachDB Bleve ProjectAtomic (enterprise) Giant Swarm's gsctl Nanobox/Nanopack rclone nehm Pouch 如果你是云计算方向的攻城狮，上面半数项目应该都耳熟能详～ 2.2. 使用cobra 下面我们实践一下cobra，先下载这个项目编译一下： # 如果你的网络很给力，那么下面这个命令就够了； go get -u github.com/spf13/cobra/cobra # 如果你的网络不给力，那就下载cobra的zip包，丢到GOPATH下对应目录，然后解决依赖，再build 于是我们得到了这样一个可执行文件及项目源码： 我们试一下这个命令：cobra init ${project-name} [root@farmer-hutao src]# cobra init myapp Your Cobra application is ready at /root/go/src/myapp Give it a try by going there and running `go run main.go`. Add commands to it by running `cobra add [cmdname]`. [root@farmer-hutao src]# ls myapp/ cmd LICENSE main.go [root@farmer-hutao src]# pwd /root/go/src 如上，本地可以看到一个main.go和一个cmd目录，这个cmd和k8s源码里的cmd是不是很像～ main.go里面的代码很精简，如下： main.go package main import \"myapp/cmd\" func main() { cmd.Execute() } 这里注意到调用了一个cmd的Execute()方法，我们继续看cmd是什么： 如上图，在main.go里面import了myapp/cmd，也就是这个root.go文件。所以Execute()函数就很好找了。在Execute里面调用了rootCmd.Execute()方法，这个rootCmd是*cobra.Command类型的。我们关注一下这个类型。 下面我们继续使用cobra命令给myapp添加一个子命令： 如上，我们的程序可以使用version子命令了！我们看一下源码发生了什么变化： 多了一个version.go，在这个源文件的init()函数里面调用了一个rootCmd.AddCommand(versionCmd)，这里可以猜到是根命令下添加一个子命令的意思，根命令表示的就是我们直接执行这个可执行文件，子命令就是version，放在一起的感觉就类似大家使用kubectl version的感觉。 另外注意到这里的Run属性是一个匿名函数，这个函数中输出了“version called”字样，也就是说我们执行version子命令的时候其实是调用到了这里的Run. 最后我们实践一下多级子命令： 套路也就这样，通过serverCmd.AddCommand(createCmd)调用后就能够把*cobra.Command类型的createCmd变成serverCmd的子命令了，这个时候我们玩起来就像kubectl get pods. 行，看到这里我们回头看一下scheduler的源码就能找到main的逻辑了。 3. Scheduler的main 我们打开文件：cmd/kube-scheduler/scheduler.go可以找到scheduler的main()函数，很简短，去掉枝干后如下： cmd/kube-scheduler/scheduler.go:34 func main() { command := app.NewSchedulerCommand() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } 看到这里猜都能猜到kube-scheduler这个二进制文件在运行的时候是调用了command.Execute()函数背后的那个Run，那个Run躲在command := app.NewSchedulerCommand()这行代码调用的NewSchedulerCommand()方法里，这个方法一定返回了一个*cobra.Command类型的对象。我们跟进去这个函数，看一下是不是这个样子： cmd/kube-scheduler/app/server.go:70 / NewSchedulerCommand creates a *cobra.Command object with default parameters func NewSchedulerCommand() *cobra.Command { cmd := &cobra.Command{ Use: \"kube-scheduler\", Long: `The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity. The scheduler needs to take into account individual and collective resource requirements, quality of service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, deadlines, and so on. Workload-specific requirements will be exposed through the API as necessary.`, Run: func(cmd *cobra.Command, args []string) { if err := runCommand(cmd, args, opts); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } return cmd } 如上，同样我先删掉了一些枝干代码，剩下的可以很清楚地看到，schduler启动时调用了runCommand(cmd, args, opts)，这个函数在哪里呢，继续跟一下： cmd/kube-scheduler/app/server.go:117 // runCommand runs the scheduler. func runCommand(cmd *cobra.Command, args []string, opts *options.Options) error { c, err := opts.Config() stopCh := make(chan struct{}) // Get the completed config cc := c.Complete() return Run(cc, stopCh) } 如上，可以看到这里是处理配置问题后调用了一个Run()函数，Run()的作用是基于给定的配置启动scheduler，它只会在出错时或者channel stopCh被关闭时才退出，代码主要部分如下： cmd/kube-scheduler/app/server.go:167 // Run executes the scheduler based on the given configuration. It only return on error or when stopCh is closed. func Run(cc schedulerserverconfig.CompletedConfig, stopCh 可以看到这里最终是要跑sched.Run()这个方法来启动scheduler，sched.Run()方法已经在pkg下，具体位置是pkg/scheduler/scheduler.go:276，也就是scheduler框架真正运行的逻辑了。于是我们已经从main出发，找到了scheduler主框架的入口，具体的scheduler逻辑我们下一讲再来仔细分析。 最后我们来看一下sched的定义，在linux里我们经常会看到一些软件叫做什么什么d，d也就是daemon，守护进程的意思，也就是一直跑在后台的一个程序。这里的sched也就是“scheduler daemon”的意思。sched的其实是*Scheduler类型，定义在： pkg/scheduler/scheduler.go:58 // Scheduler watches for new unscheduled pods. It attempts to find // nodes that they fit on and writes bindings back to the api server. type Scheduler struct { config *factory.Config } 如上，注释也很清晰，说Scheduler watch新创建的未被调度的pods，然后尝试寻找合适的node，回写一个绑定关系到api server.这里也可以体会到daemon的感觉，我们平时搭建的k8s集群中运行着一个daemon进程叫做kube-scheduler，这个一直跑着的进程做的就是上面注释里说的事情，在程序里面也就对应这样一个对象：Scheduler. Scheduler结构体中的Config对象我们再简单看一下： pkg/scheduler/factory/factory.go:96 // Config is an implementation of the Scheduler's configured input data. type Config struct { // It is expected that changes made via SchedulerCache will be observed // by NodeLister and Algorithm. SchedulerCache schedulerinternalcache.Cache // Ecache is used for optimistically invalid affected cache items after // successfully binding a pod Ecache *equivalence.Cache NodeLister algorithm.NodeLister Algorithm algorithm.ScheduleAlgorithm GetBinder func(pod *v1.Pod) Binder // PodConditionUpdater is used only in case of scheduling errors. If we succeed // with scheduling, PodScheduled condition will be updated in apiserver in /bind // handler so that binding and setting PodCondition it is atomic. PodConditionUpdater PodConditionUpdater // PodPreemptor is used to evict pods and update pod annotations. PodPreemptor PodPreemptor // NextPod should be a function that blocks until the next pod // is available. We don't use a channel for this, because scheduling // a pod may take some amount of time and we don't want pods to get // stale while they sit in a channel. NextPod func() *v1.Pod // SchedulingQueue holds pods to be scheduled SchedulingQueue internalqueue.SchedulingQueue } 如上，同样我只保留了一些好理解的字段，我们随便扫一下可以看到譬如：SchedulingQueue、NextPod、NodeLister这些很容易从字面上理解的字段，也就是Scheduler对象在工作（完成调度这件事）中需要用到的一些对象。 ok，下一讲我们开始聊Scheduler的工作过程！ Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-22 15:04:12 调度程序启动前逻辑1. 概述2. cobra和main2.1. cobra是啥2.2. 使用cobra3. Scheduler的main"},"core/apiserver/":{"url":"core/apiserver/","title":"apiserver","keywords":"","body":"apiserver Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/proxy/":{"url":"core/proxy/","title":"proxy","keywords":"","body":"proxy Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/kubelet/":{"url":"core/kubelet/","title":"kubelet","keywords":"","body":"kubelet Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"core/controller-manager/":{"url":"core/controller-manager/","title":"controller-manager","keywords":"","body":"controller-manager Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-15 12:29:29 "},"around/":{"url":"around/","title":"概述","keywords":"","body":"周边项目源码分析 Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:30:13 "},"around/client-go/":{"url":"around/client-go/","title":"client-go","keywords":"","body":"client-go Copyright © farmer.hutao@outlook.com 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-02-19 12:30:13 "}}